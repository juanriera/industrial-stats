[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "",
    "text": "Prefacio\nEste libro trata sobre la enseñanza de algunos métodos básicos de la estadística y la moderna ciencia de datos y su aplicación al entorno industrial. Está concebido de forma práctica con multitud de ejemplos, no sólo industriales, con el objetivo de mostrar los métodos cuantitativos de análisis, y también el razonamiento necesario para dar sentido a los resultados presentados por las herramientas de análisis de datos.\nEl objetivo del libro es acercar a los estudiantes de la Formación Profesional al uso de las herramientas de análisis de datos industriales. El entorno de la industria actual produce un enorme y constante flujo de datos como resultado tanto de la implantación de sistemas de captura automáticos como del aumento de la tecnificación de los puestos de trabajo; se requiere por parte de los profesionales industriales que sean capaces de analizar esta enorme cantidad de datos para transformarlos en información para la decisión. En la empresa industrial actual, son los ingenieros y técnicos de planta, y no estadísticos o ingenieros informáticos, quienes participan diariamente en la presentacion y discusion de los datos y en la toma de decisiones operativas, tanto en los equipos de trabajo como ante la Dirección. Por esta razón, considero necesario proporcionar a los estudiantes de la Formación Profesional un conocimiento básico de los conceptos, herramientas y métodos del análisis de datos, así como de algunas técnicas de presentación y comunicación de la información.\nLa enseñanza de los conceptos estadísticos ha estado, casi siempre, a cargo de profesores con una gran formación en matemáticas. Estos profesores suelen identificar la comprensión de los conceptos estadísticos con su comprensión matemática. Sin embargo, cuando enseñamos estadística industrial, debemos hacer énfasis tanto en las ideas y la comprensión de los conceptos como en su utilización práctica, y reconocer que el razonamiento matemático no es el único camino para la comprensión conceptual.\nLa práctica de la estadística requiere buen juicio y sentido común. Dado que el buen juicio se desarrolla con la experiencia, un curso de iniciación debe presentar unas guías claras de aplicación de los métodos, y no dar por supuestas unas exigencias excesivamente altas sobre la capacidad de juicio analítico de los estudiantes; no sería un planteamiento razonable. Con el fin de desarrollar esta capacidad de juicio analítico he introducido explicaciones detalladas en la mayor parte de los ejemplos. En todos los casos, los ejercicios requerirán del estudiante no sólo una resolución numérica, sino el uso del juicio analítico y la explicación verbal (o escrita) de las decisiones tomadas y conclusiones realizadas. Creo que este planteamiento será mucho más beneficioso a largo plazo que limitarse a una simple resolución numérica.\nMi experiencia industrial me ha mostrado que en las situaciones reales, sobre el terreno, la comprensión práctica de los conceptos es más importante que su rigurosa formulación matemática. Por esta razón, en el desarrollo del contenido del libro he insistido más en la forma de aplicar las herramientas y entender los análisis que en el conocimiento formal de las fórmulas estadísticas y su deducción matemática. He hecho especial hincapié en la utilización de herramientas sencillas, sobre todo gráficas, que casi siempre son una ayuda para comprender la información contenida en un conjunto de datos. El objetivo es proporcionar al estudiante las bases de la metodología del análisis de datos y del análisis estadístico, y cómo puede aplicarse a la resolución de problemas técnicos concretos, más que el conocimiento de la teoría matemática de la estadística.\nHe evitado las explicaciones formales sobre temas estadísticos cuando no son indispensables para su aplicación práctica. Así, por ejemplo, al explicar la media de un conjunto de datos considero más importante entender el concepto físico de “centro de gravedad” que los conceptos estadísticos de esperanza matemática, que no se tocan en este texto. En este sentido, he intentado que el alumno aprenda a diferenciar “en qué consiste” un estadístico, de “cómo se calcula”. Comprender la diferencia entre el concepto y su fórmula de cálculo es fundamental para entender en qué situación debe usarse uno u otro estadístico.\nUn curso de introducción a la estadística y análisis de datos industriales debe ser, ante todo, práctico y orientado a su aplicación en el entorno industrial real. Los principales temas de trabajo estadístico en la industria tienen que ver con la captura de datos, su almacenamiento y su depuración, su descripción utilizando gráficos, la inferencia (intervalos de confianza y tests), la construcción de modelos explicativos, el diseño de los experimentos industriales, el control estadístico de la calidad y la exposición y presentación de resultados. Dado el alcance limitado de este libro, algunos de estos temas se tratarán de forma muy ligera, y necesitarán de un estudio posterior si el alumno tiene interés en profundizar en ellos. A pesar de que los temas más especializados puedan ser importantes en algunas aplicaciones específicas, no preparan al estudiante para lo que se va a encontrar en el terreno en la mayor parte de las ocasiones. En cambio, la resolución de problemas en equipo en un entorno de aprendizaje dinámico enfrentándose a problemas exigentes, y el desarrollo de las habilidades de análisis, de síntesis y de comunicación, tendrán un impacto mucho más positivo.\nHe intentado mostrar la necesidad de que los estudiantes comprendan y apliquen el método científico en el entorno industrial, y no sólo apliquen un recetario de procedimientos de manera automática. Son mucho mas importantes la comprensión y la utilización adecuada del método científico y de las herramientas y gráficos básicos, antes que la aplicación rutinaria y mecánica de determinadas fórmulas matemáticas o métodos sofisticados y complejos que el alumno puede no comprender en toda su profundidad.\nLas industrias líderes destacan por la aplicación intensiva de métodos tales como Six Sigma, Lean Manufacturing, diseño robusto de productos, y otros que hacen un uso intensivo de los datos, tanto de los obtenidos en producción como de los obtenidos en la realización de experimentos bien diseñados. Pero la mejora de la competitividad en estas empresas no se debe tanto a la aplicación de unos u otros métodos, como al desarrollo del juicio analítico de sus equipos humanos y a la aplicación de lo aprendido a la mejora continua de los procesos industriales. Veremos que la experiencia y el conocimiento tecnológico de estos procesos son fundamentales para el desarrollo del buen juicio analítico, y, en consecuencia, para la buena interpretación de los resultados que se obtienen con las herramientas estadísticas y de análisis.\nTratándose de un libro para el uso en la Formación Profesional, considero prioritario que su estudio se oriente al desarrollo de habilidades que sean de aplicación práctica directa en el puesto de trabajo y además faciliten la empleabilidad del estudiante, y no a la obtención de conocimiento abstracto. El Informe sobre el futuro del empleo, publicado por el Foro Económico Mundial en mayo de 2023 (World Economic Forum 2023), considera que las principales habilidades clave en el trabajo del futuro serán en primer lugar el desarrollo del pensamiento analítico y, en segundo, del pensamiento crítico; unidas al desarrollo de la curiosidad y la voluntad de aprendizaje a lo largo de la vida, la eficacia, la confianza en el propio trabajo y la atención al detalle. La voluntad de este libro es proporcionar conocimientos que ayuden al estudiante a desarrollarse en esta dirección.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#a-quién-va-dirigido-este-libro",
    "href": "index.html#a-quién-va-dirigido-este-libro",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "A quién va dirigido este libro",
    "text": "A quién va dirigido este libro\nEl libro está orientado a completar la formación técnica de los estudiantes de Formación Profesional, en las especialidades relacionadas con la actividad productiva industrial. También creo que será de utilidad para los técnicos industriales en activo que no han tenido la oportunidad de recibir una adecuada formación en estas metodologías, y que han encontrado dificultad para lanzarse a su aprendizaje mientras desarrollan so actividad profesional. Espero, también, que los profesores de la Formación Profesional en estos ámbitos de competencia encuentren en este documento los elementos de apoyo que les permitan integrar estas enseñanzas en sus respectivos ciclos formativos.\nEn todos los casos, el aprendizaje requerirá de un esfuerzo que quizás será mayor en los estudiantes que no tengan una base mínima en álgebra y cálculo. En estos casos, el trabajo en equipo y la discusión abierta entre compañeros y con los profesores ayudará a la comprensión de los conceptos.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#organización-del-libro",
    "href": "index.html#organización-del-libro",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Organización del libro",
    "text": "Organización del libro\nEl libro se estructura en varias partes:\nUna introducción, con dos capítulos:\n\nEl capítulo 1 hace una introducción general al pensamiento estadístico y su aplicación industrial, y presenta algunos conceptos básicos, tales como población, muestra y variable.\nEl capítulo 2 presenta las dos herramientas básicas que se usarán en el libro, la hoja de cálculo y el programa estadístico de código abierto R. Se describen los aspectos básicos del software estadístico R y sus principales aplicaciones en la manipulación, limpieza y análisis de conjuntos de datos. Se introduce también el concepto actual de reproducibilidad, y se explica su importancia y el impacto de utilizar una hoja de cálculo o un análisis basado en un script programado.\n\nLa primera parte introduce las técnicas básicas:\n\nEn el capítulo 3 se hace una introducción al concepto de flujo de trabajo y al concepto de datos ordenados o tidy data, que resulta fundamental para las fases posteriores de análisis. Se explica también cómo mover datos desde excel a R y viceversa.\nEl capítulo 4 habla de la exploración de los datos como primer paso en el análisis. Se introducen las distribuciones de frecuencia y los principales gráficos que se utilizarán, y se explica cómo hacerlos con Excel y con R.\nEn el capítulo 5 se presentan los principales estadísticos que describen una población, mediante un valor central y una medida de dispersión, tanto paramétricos (media y varianza) como no paramétricos (mediana y rango intercuartil), de forma sencilla y sin recurrir a explicaciones matemáticas o estadísticas avanzadas.\nEl capítulo 6 explica la importancia de comprender la forma de los datos, es decir, su distribución. Se ponen en práctica las herramientas estudiadas en los dos capítulos anteriores para estudiar algunos casos prácticos.\nEn el capítulo 7 se presentan los métodos para detectar la relación entre dos variables (correlación y regresión lineal), haciendo énfasis en los métodos gráficos, y se discuten las diferencias entre correlación y causalidad.\nEl capítulo 8 se hace una breve introducción a las técnicas de comunicación, tanto a la presentación de datos como a la preparación de informes. Se hace hincapié en las nuevas herramientas como Quarto, que facilitan la elaboración de informes automatizados.\n\nLa segunda parte del libro trata algunas conceptos avanzados, pero desarrollados de forma básica; he intentado que estos conceptos puedan comprenderse sin necesidad de desarrollos teóricos o formulaciones complejas.\n\nEl capítulo 9 introduce el concepto de probabilidad, así como las distribuciones de probabilidad, necesarias para la construcción de los tests de hipótesis y, en general, de la estadística inferencial. Este contenido se presenta de forma breve y, sobre todo, práctica.\nEl capítulo 10 introduce de manera sencilla el análisis de la varianza, necesario para métodos importantes en la industria como el control de la precisión analítica, que se trata en un capítulo posterior.\nEn el capítulo 11 se hace una introducción al diseño de experimentos. La utilidad de esta técnica es primordial para el industrial, sobre todo para el área de I+D y el diseño de productos. Dado que esta técnica puede ser muy compleja en su aplicación real, se facilitan enlaces a otros recursos, como cursos, que serán útiles a los que quieran profundizar más.\n\nLa tercera y última parte del libro desarrolla algunas aplicaciones prácticas al entorno industrial de lo estudiado en los capítulos anteriores.\n\nEl capítulo 12 presenta una de las aplicaciones más importantes de la estadística en el entorno industrial, el control estadístico de procesos. Dada la importancia de este capítulo, se refuerza su contenido con numerosos ejemplos y casos prácticos, y se incluye un caso extenso para su análisis.\nEl capítulo 13 trata del análisis del sistema de medición, la calidad de las medidas y la medida de la precisión analítica. Resulta sorprendente la cantidad de laboratorios que dan soporte analítico a procesos productivos de gran impacto económico en la vida de la empresa, sin realizar nunca un autocontrol sobre el nivel de precisión de sus análisis. En este capítulo se hace una presentación básica del tema con el objetivo de que resulte útil y práctica.\nFinalmente, el capítulo 14 explica la aplicación estructurada de los conceptos y técnicas estudiadas en el libro a la mejora de los procesos y la calidad industrial mediante el método Six Sigma.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#cómo-usar-el-libro",
    "href": "index.html#cómo-usar-el-libro",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Cómo usar el libro",
    "text": "Cómo usar el libro\nHe intentado que cada capítulo sea lo más autocontenido posible de forma que se facilite la organización pedagógica por temas. No obstante, a veces puede ser necesario conocer los contenidos de los capítulos anteriores, por lo que se sugiere estudiarlo en el orden presentado.\nEl libro es eminentemente práctico, con numerosos ejercicios; su resolución puede ser individual o en equipo.\nAlgunos recuadros utilizan códigos de color para indicar el objetivo de la información que contienen. Básicamente, los colores utilizados son:\n\n\n\n\n\n\nProblema o cuestión a resolver\n\n\n\nEl recuadro azul se utilizará para proponer problemas sencillos cuya respuesta se encuentra más adelante en el texto. El objetivo de estos problemas es estimular la reflexión, aunque puede ser necesario recurrir a cálculos sencillos ayudados por las herramientas disponibles.\n\n\n\n\n\n\n\n\nRespuesta al problema o explicación del código R\n\n\n\n\n\nEl recuadro verde se utilizará para dos cosas\n\nproponer una respuesta al problema planteado; respuesta que no tiene por qué ser la única posible, o bien\npara explicar las líneas de código R que se mostrarán a continuación\n\nNormalmente se presentará de forma desplegable para no entorpecer la lectura del texto.\n\n\n\nAdemás se incluyen diferentes tipos de avisos cada vez que se introduce algún concepto que es necesario resaltar.\n\n\n\n\n\n\nImportante\n\n\n\nEn este formato se indican cuestiones importantes\n\n\n\n\n\n\n\n\n¡Atención!\n\n\n\nEn este formato se indican cuestiones a las que hay que prestar especial atención o que pueden inducir a error",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#uso-del-ordenador-y-el-software-estadístico",
    "href": "index.html#uso-del-ordenador-y-el-software-estadístico",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Uso del ordenador y el software estadístico",
    "text": "Uso del ordenador y el software estadístico\nEn la práctica diaria, los técnicos industriales usan los ordenadores para almacenar y visualizar los datos de producción, para solucionar problemas mediante análisis estadísticos, y para presentar sus resultados de forma gráfica. De la misma forma que en el entorno industrial, en este libro se utilizarán también los ordenadores de forma habitual, y por esta razón es imprescindible que los estudiantes tengan acceso individual a un ordenador en el que esté instalado el software recomendado, y que se acostumbren a utilizarlo para resolver los problemas y casos planteados como ejercicios prácticos, individualmente y en grupo.\nEl estudiante que se incorpora a una empresa, sea en un laboratorio o en una planta de producción, se va a encontrar muy pronto delante de una hoja de cálculo, y debe saber cómo utilizarla correctamente. Actualmente, lo más probable es que esa hoja de cálculo sea Microsoft Excel, aunque hay otras alternativas posibles, como Google Sheets, Apple Numbers, OpenOffice Calc y algunas más. La gran dominancia en el mercado de Microsoft Excel ha hecho que todas estas herramientas sean totalmente compatibles o tengan modos de compatibilidad con Excel. Por esta razón, este libro se basa en la utilización de Excel como hoja de cálculo y herramienta principal para el almacenamiento de datos.\nA lo largo del libro se presentarán informes y gráficos obtenidos con Microsoft Excel, y también con el software estadístico R. Prácticamente todos ellos pueden ser exportados a otras herramientas, como Google Sheets, OpenOffice, Minitab o Matlab, o analizarse con otros lenguajes de programación, como Python o Julia. En realidad, el método de análisis y cómo obtener un resultado correcto son aspectos más importantes que la herramienta que se utilice para ello, por lo que queda en manos del instructor la decisión final sobre qué usar y cómo. Para facilitar este trabajo de conversión, en su caso, todo el material del libro y los datos de ejemplo estarán disponibles en un repositorio de GitHub.\nAlgunos ejercicios tienen que ver con la interpretación y presentación de los resultados. Es importante que estos trabajos se realicen en grupo y se haga énfasis en la comprensión del problema y en su correcta exposición; en los equipos industriales de hoy, la discusión de problemas y la exposición de resultados, en reuniones de trabajo o en paneles informativos a pie de planta, forma parte del trabajo diario. Estas habilidades de comunicación deben ser desarrolladas en los estudiantes de forma prioritaria.\nLa ventaja de R sobre Excel es que el código R, si está bien documentado, muestra cada paso realizado, y esto permite que otras personas puedan verificar el resultado y reproducirlo a partir de los datos originales, e incluso reutilizar los procedimientos. Utilizar código en vez de clicks de ratón es esencial para asegurar la reproducibilidad de los análisis de datos1. Por esta razón, recomiendo el uso del lenguaje R como complemento o alternativa a la hoja de cálculo, tanto para analizar como para visualizar datos. Sin embargo, como la realidad del mundo de la empresa es que los lenguajes como R están todavía poco introducidos, es inevitable mantener el uso de la hoja de cálculo; en el libro se explicarán algunas mejores prácticas, que permitirán el uso simultáneo de ambas herramientas de forma óptima.\nRespecto a la programación informática, en el libro no se hace énfasis en la programación R más que como sucesión de órdenes individuales en scripts sencillos. No se busca la eficiencia computacional ni la rapidez en el cálculo, sino la comprensión de la metodología de resolución de problemas y cómo ésta se apoya en las herramientas presentadas. De la misma manera, tampoco se hace ningún uso de la programación en Excel, ya sea con macros o con Visual Basic; estos temas quedan fuera del perímetro de este libro.\nUn paso en la dirección de la implantación de flujos de trabajo reproducibles es la elaboración de informes automatizados. Estos informes incluyen el código R, los comentarios del autor en forma de texto formateado en markdown, y los resultados del código. Herramientas como Quarto, o Google Colaboratory, que usa la interface Jupyter, son nuevas formas de elaborar y presentar los informes y resultados estadísticos. En el entorno docente, estas herramientas abren posibilidades muy interesantes en la presentación de un ejercicio o un exámen escrito, ya que el alumno puede detallar perfectamente todos los pasos hasta llegar al resultado final, y facilita la revisión por sus compañeros o por el profesor a cargo de la asignatura.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#sec-aprendizaje",
    "href": "index.html#sec-aprendizaje",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Recursos adicionales y cómo usarlos",
    "text": "Recursos adicionales y cómo usarlos\nEn este libro se hace una introducción muy general a R y a Excel; se presupone que el alumno tiene un conocimiento básico de ambas herramientas. Si no tiene ninguna formación sobre el lenguaje R y el entorno RStudio, recomiendo hacer alguna formación previa sencilla que introduzca los conceptos básicos. Datacamp tiene cursos gratuitos de introducción a R; también hay cursos de formación tanto de R como de Excel en otras plataformas web como edX, Udemy y Coursera, muchos de ellos gratuitos. El Gobierno de España, dentro de una de sus iniciativas de transformación digital, la iniciativa de datos abiertos, incluye también una amplia referencia a cursos de formación sobre R.\nTodos los datos presentados en los ejemplos se incluyen en hojas de cálculo que están disponibles en GitHub. También se incluyen fuentes de datos adicionales que pueden permitir plantear nuevos ejercicios.\nAl final del libro se incluye una bibliografía completa.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#sobre-el-libro",
    "href": "index.html#sobre-el-libro",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Sobre el libro",
    "text": "Sobre el libro\nEl libro ha sido editado en Quarto. Está disponible en PDF.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#agradecimientos",
    "href": "index.html#agradecimientos",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "Agradecimientos",
    "text": "Agradecimientos\n\n\n\n\n\nWorld Economic Forum. 2023. «Future of Jobs Report, 2023». 91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World Economic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2023/.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introducción al análisis de datos industriales utilizando Microsoft Excel y R",
    "section": "",
    "text": "El concepto de reproducibilidad, cada vez más importante, se desarrolla en el capítulo 1↩︎",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "010-intro.html",
    "href": "010-intro.html",
    "title": "1  La estadística en el entorno industrial.",
    "section": "",
    "text": "1.1 Introducción\n¿Alguna vez te has preguntado cómo podemos resolver problemas en el trabajo de forma ordenada? La estadística y el método científico nos ayudan a hacerlo, y son más sencillos de lo que parece.\nEl método científico es como una receta que seguimos para resolver problemas: primero observamos qué está pasando, luego pensamos qué puede estar causando el problema, después hacemos pruebas para comprobarlo, y finalmente sacamos conclusiones y tomamos acciones orientadas a eliminar el problema o conseguir una mejora, aunque sea parcial, y repetimos el ciclo. La estadística nos ayuda a entender si nuestras pruebas funcionaron o no, usando números y datos reales.\nPensar de forma estadística significa entender que todo lo que hacemos en el trabajo está conectado, y que es normal que las cosas cambien o varíen un poco. Lo importante es saber cuándo estos cambios son normales y cuándo indican un problema real. Es como cuando cocinamos: sabemos que cada plato puede salir un poco diferente, pero reconocemos cuando algo ha salido realmente mal.\nEn las fábricas y talleres, estas herramientas son muy útiles. Por ejemplo, si una máquina empieza a producir piezas defectuosas, podemos:\nLa experimentación es muy importante en la industria. En vez de cambiar las cosas al azar, hacemos pruebas organizadas para ver qué funciona mejor. Es como cuando ajustas la temperatura del horno y el tiempo de cocción para que un pastel salga perfecto: pruebas diferentes combinaciones y anotas los resultados.\nLo mejor de todo es que cada vez que hacemos estos experimentos, aprendemos algo nuevo. Incluso si algo no funciona como esperábamos, esa información nos ayuda a mejorar la próxima vez. Es un proceso continuo de aprendizaje y mejora.\nEsta forma de trabajar nos ayuda a:\nRecuerda: no necesitas ser un genio de las matemáticas para usar estas herramientas. Lo importante es ser organizado, observar bien lo que pasa y anotar los resultados de lo que hacemos. Así, poco a poco, podemos mejorar nuestro trabajo y resolver problemas de forma más eficiente.\nLos métodos estadísticos nos ayudan a describir y comprender la variabilidad. Cuando hablamos de variabilidad queremos decir que sucesivas observaciones de un mismo proceso o sistema no dan exactamente los mismos resultados. Por ejemplo, el consumo de gasolina de un coche no es siempre igual, sino que varía de manera considerable. Esta variación depende de muchos factores, como la forma de conducir, el tipo de carretera, la situación del propio vehículo (presión de neumáticos, compresión del motor, …), la marca de la gasolina, el octanaje, o incluso las condiciones meteorológicas. Todos estos factores son causas de variabilidad en el consumo de gasolina. La estadística nos permite analizar estos factores y determinar cuáles son los más importantes o tienen mayor impacto en el consumo; una vez conocidos, podemos actuar sobre ellos.\nEn este libro aprenderemos a utilizar herramientas diversas, tanto estadísticas como de la ciencia de datos, para realizar nuestro análisis. Para aprender de los datos necesitamos más que los simples números; para interpretarlos necesitaremos siempre el conocimiento del proceso industrial que estamos analizando.En un análisis de la producción de un producto lácteo, por ejemplo, los números significan poco sin un conocimiento del proceso; los valores de pH, temperatura o concentración de lactosa influyen en el resultado del proceso de forma diferente. Los datos son números dentro de un contexto, y necesitamos conocer este contexto para dar sentido a los números.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "010-intro.html#introducción",
    "href": "010-intro.html#introducción",
    "title": "1  La estadística en el entorno industrial.",
    "section": "",
    "text": "Observar qué está pasando\nPensar en posibles causas\nHacer pruebas controladas para encontrar el problema\nUsar números y datos para confirmar si lo hemos solucionado\n\n\n\n\n\nResolver problemas de forma ordenada\nTomar mejores decisiones basadas en datos reales\nMejorar la calidad de nuestro trabajo\nAhorrar tiempo y dinero evitando errores\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEl objetivo principal de la mejora industrial es la reducción de la variabilidad.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "010-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "href": "010-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "title": "1  La estadística en el entorno industrial.",
    "section": "1.2 El pensamiento estadístico y el método científico.",
    "text": "1.2 El pensamiento estadístico y el método científico.\nLa estadística y el método científico mantienen una relación fundamental que impulsa el avance del conocimiento y la innovación. El método científico, con sus pasos de observación, hipótesis, experimentación y conclusiones, encuentra en la estadística las herramientas necesarias para validar o refutar hipótesis de manera objetiva y cuantificable.\nEl pensamiento estadístico es una filosofía de aprendizaje y acción basada en tres principios fundamentales:\n\nque todo proceso industrial está compuesto por un subsistema de procesos interconectados,\nque la variabilidad existe y es inherente a estos procesos, y\nque entender y reducir la variación es clave para la mejora y el éxito\n\nEn el entorno industrial, esta integración entre estadística y método científico ha revolucionado los procesos de mejora continua. Por ejemplo, cuando una línea de producción enfrenta problemas de calidad, el método científico guía la investigación sistemática: se observa el proceso, se formulan hipótesis sobre las causas del problema, se diseñan experimentos controlados, y se analizan los resultados estadísticamente para determinar la validez de las soluciones propuestas.\nLa experimentación industrial, particularmente a través del diseño de experimentos (DOE), se ha convertido en una herramienta fundamental para la mejora continua. En lugar de modificar procesos basándose en intuiciones o experiencias pasadas, las empresas pueden realizar experimentos controlados que:\n\nOptimizan múltiples variables simultáneamente\nIdentifican interacciones entre factores que afectan el proceso\nReducen el tiempo y costo de las mejoras\nProporcionan conclusiones respaldadas por evidencia estadística\n\nAl realizar experimentos controlados, las empresas pueden probar hipótesis que permitan evaluar el impacto de diferentes variables en los procesos y productos y determinar los valores óptimos de los parámetros de los procesos para maximizar la calidad y la eficiencia. Esto genera conocimiento valioso sobre los procesos y productos, permite tomar decisiones más informadas, y ayuda a crear y mantener una ventaja competitiva.\nLa experimentación, combinada con el análisis estadístico, permite a las empresas implementar ciclos de mejora continua, donde los resultados de cada experimento se utilizan para refinar los procesos y productos.\nAl integrar el pensamiento estadístico con el método científico en el entorno industrial, las organizaciones pueden desarrollar una cultura de mejora continua basada en la evidencia de los datos y no en suposiciones o intuiciones. En una cultura industrial basada en el método científico, cada problema es una oportunidad para experimentar, aprender y optimizar. Esta aproximación sistemática no solo mejora la calidad y eficiencia de los procesos, sino que también fomenta la innovación y el aprendizaje organizacional continuo.\nLa clave del éxito radica en entender que la experimentación no es un evento aislado, sino un proceso continuo de aprendizaje y mejora. Cada experimento, exitoso o no, aporta información valiosa que, analizada correctamente mediante métodos estadísticos, contribuye al conocimiento colectivo de la organización y sienta las bases para futuras mejoras. La estadística y el método científico no solo son pilares fundamentales para la investigación académica, sino que también desempeñan un papel crucial en el entorno industrial. La aplicación sistemática del método científico, respaldada por herramientas estadísticas, permite a las empresas optimizar procesos, mejorar la calidad de sus productos y servicios, y fomentar la innovación.\n\n\n\n\n\n\nEl pensamiento estadístico en la toma de decisiones industriales\n\n\n\nEn el entorno industrial, el pensamiento estadístico es esencial para:\n\nTomar decisiones basadas en datos, evitando decisiones basadas en intuiciones o suposiciones.\nEvaluar riesgos, cuantificando la incertidumbre y permitiendo tomar decisiones que minimicen los riesgos.\nMejorar el conocimiento de los procesos para una toma de decisiones más eficaz.\n\nComunicar resultados, presentando los resultados de los análisis de manera clara y concisa.\n\nLa integración del método científico y el pensamiento estadístico en la cultura empresarial impulsa la innovación, la eficiencia y la competitividad.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "010-intro.html#los-datos-industriales",
    "href": "010-intro.html#los-datos-industriales",
    "title": "1  La estadística en el entorno industrial.",
    "section": "1.3 Los datos industriales",
    "text": "1.3 Los datos industriales\nEn el entorno industrial, podemos organizar los datos en varias categorías, considerando tanto su naturaleza como la forma en que se obtienen:\nSegún su estructura:\n\nDatos estructurados:\n\nSon datos organizados en un formato definido, como tablas de bases de datos, hojas de cálculo o archivos CSV. Su estructura facilita el análisis y la consulta.\nEjemplos: lecturas de sensores, registros de producción, datos de control de calidad, información de inventario.\n\nDatos no estructurados:\n\nSon datos que no tienen un formato predefinido, como texto, imágenes, audio o video.Su análisis requiere técnicas más avanzadas, como el procesamiento de lenguaje natural o la visión artificial.\nEjemplos: registros de mantenimiento, informes de incidentes, imágenes de inspección visual, grabaciones de audio de maquinaria.\n\nDatos semiestructurados:\n\nSon datos que tienen cierta estructura, pero no tan rígida como los datos estructurados, y permiten una mayor flexibilidad en el almacenamiento y el intercambio de información.\nEjemplos: archivos XML o JSON, registros de eventos.\n\n\nSegún su origen y método de obtención:\n\nDatos históricos (Estudios retrospectivos):\n\nSon datos recopilados en el pasado, que se utilizan para analizar tendencias, identificar patrones y predecir el comportamiento futuro. Permiten comprender la evolución de los procesos.\nEjemplos: registros de producción de años anteriores, datos de fallos de maquinaria, históricos de ventas.\n\nDatos observacionales (Estudios observacionales):\n\nSon datos recopilados mediante la observación de procesos o sistemas, sin intervenir en ellos. Permiten identificar relaciones entre variables y comprender el comportamiento de los procesos en condiciones reales.\nEjemplos: mediciones de temperatura, vibración o presión de maquinaria, registros de tiempo de ciclo de producción.\n\nDatos experimentales (Experimentos diseñados):\n\nSon datos recopilados mediante la realización de experimentos controlados, en los que se manipulan variables para evaluar su efecto. Permiten establecer relaciones de causa y efecto, estudiar las interacciones entre las variables y optimizar el rendimiento de los procesos.\nEjemplos: datos de pruebas de rendimiento de nuevos materiales, resultados de experimentos de optimización de procesos.\n\nDatos de monitorización o control de procesos en tiempo real:\n\nson datos que se recaban de manera instantanea, y permiten actuar de manera casi inmediata en los procesos; permiten implementar el mantenimiento predictivo, y evitar perdidas de produccion.\nEjemplos: datos de sensores que detectan fallos en maquinaria, alarmas de procesos fuera de control.\n\n\nEn el contexto de la Industria 4.0, la cantidad y variedad de datos industriales está aumentando exponencialmente.\n\nEstudios retrospectivos o históricos\nUn estudio retrospectivo o histórico es el que utiliza una muestra o todos los datos históricos de un proceso, recogidos en el pasado durante un período determinado de tiempo. El objetivo de un estudio de este tipo puede ser la investigación sobre la relación entre algunas variables, o explorar la calidad de la información disponible, o construir un modelo que permita explicar el proceso tal como es actualmente, o saber si se ha desviado. Estos modelos del proceso se denominan modelos empíricos, porque están basados en los propios datos del proceso y no en una formulación teórica sobre el mismo.\nUn estudio retrospectivo tiene la ventaja de tener a su disposición un gran número de datos que ya han sido recogidos, minimizando el esfuerzo de obtenerlos. Sin embargo, tiene varios problemas potenciales:\n\nSi no disponemos de detalles suficientes, es posible que no podamos determinar si las condiciones de variación de los valores obtenidos responden a las mismas causas que en la situación actual.\nEs posible que nos falte algún valor clave que no haya sido recogido o que lo haya sido de manera defectuosa\nAlgunas veces, la fiabilidad y validez de los datos de proceso históricos son dudosas, o al menos, cuestionables.\nLos datos históricos no siempre se han recogido con la perspectiva actual del proceso, y es posible que no nos proporciones explicaciones adecuadas del proceso en su situación actual.\nA veces queremos utilizar los datos históricos de proceso para fines que no estaban previstos cuando se recogieron\nLas notas sobre los valores del proceso, incluyendo los valores anormales, pueden ser insuficientes o inexistentes, y no tenemos ninguna explicación sobre los posibles valores anómalos que detectamos en el análisis.\n\nUsar datos históricos siempre tiene el riesgo de que, por la razón que sea, no se hayan recogido datos importantes, o que estos datos se hayan perdido, o se hayan transcrito de forma inadecuada o incorrecta. Es decir, los datos históricos pueden tener problemas de calidad de datos.\nEl hecho de que algunos datos se hayan recogido históricamente no siempre quiere decir que estos datos sean relevantes o útiles. Cuando el grado de conocimiento del proceso no es suficiente, o no se basa en un análisis metódico y riguroso de los datos, es posible que no se hayan recogido algunos datos que pueden ser importantes para el proceso, a veces simplemente porque son complejos o difíciles de analizar. Los datos históricos no pueden proporcionar la información que buscamos si la información de las variables clave nunca se ha recogido o se ha hecho sin una buena base experimental.\nEl propósito del análisis de los datos industriales es aislar las causas que están detrás de los sucesos que afectan e influyen en los procesos. En los datos históricos, estos sucesos pueden haber ocurrido semanas, meses o incluso años antes, sin que haya registros ni notas que hayan intentado explicar estas causas, y los recuerdos de las personas que han participado en ellos se pierden con el tiempo, o se alteran involuntariamente, proporcionando explicaciones supuestamente válidas pero que en realidad son incorrectas. Por eso, con frecuencia, el análisis de los datos históricos puede poner de manifiesto hechos interesantes, pero sus causas quedan sin explicar.\nLos estudios históricos pueden requerir una fase previa de preparación y depuración de datos que puede llegar a ser muy larga y tediosa. Se estima que en muchos estudios de ciencia de datos, el tiempo de preparación de los datos puede llegar al \\(60\\%\\) del tiempo total empleado en el estudio. Las herramientas de análisis de datos son de gran ayuda en esta fase del proceso, aunque en muchas ocasiones será necesario un trabajo manual de recolección de datos en papel, hojas de cálculo diversas y otras fuentes. Esta fase es muy útil no sólo para la preparación de datos para el estudio, sino para mejorar el conocimiento de los datos, cómo se originan y cómo se almacenan. Este conocimiento siempre es de gran utilidad para mejorar los procedimientos actuales de captura de datos, facilitando la fiabilidad de los análisis futuros.\n\n\nEstudios observacionales\nComo su nombre indica, un estudio observacional simplemente observa un proceso durante un tiempo de operación en rutina. Normalmente, el ingeniero o técnico interfiere lo mínimo posible en el proceso; sólo lo suficiente para recoger la información que necesita, si piensa que esa información puede ser relevante. En muchas ocasiones, el estudio no forma parte de los controles de rutina, y representa un trabajo adicional.\nSi se planifican adecuadamente, los estudios observacionales proporcionan datos fiables, precisos y completos para documentar un proceso. Por otra parte, estos estudios proporcionan una información limitada sobre las relaciones entre las variables del proceso, porque es posible que durante el tiempo limitado de observación, el rango de variación de las variables no recoja todas las situaciones posibles; por ejemplo, las situaciones extraordinarias.\n\n\nExperimentos diseñados\nLa tercera forma de recoger información de un proceso son los experimentos diseñados. En un experimento de este tipo, el ingeniero o técnico hace un cambio deliberado en las variables que controla (llamadas factores), observa el resultado, y toma una decisión respecto a qué variable o variables son responsables de los cambios que observa en el proceso.\nUna diferencia importante respecto a los estudios históricos y los observacionales es que las diferentes combinaciones de factores se aplican al azar sobre un conjunto de unidades experimentales. Esto permite establecer con precisión las relaciones causa-efecto, cosa que no suele ser posible ni en los estudios históricos ni en los observacionales.\n\n\n\n\n\n\nExperimentos diseñados\n\n\n\nEn el capítulo 11 se hará una introducción básica al diseño de experimentos",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "010-intro.html#algunas-definiciones-importantes",
    "href": "010-intro.html#algunas-definiciones-importantes",
    "title": "1  La estadística en el entorno industrial.",
    "section": "1.4 Algunas definiciones importantes",
    "text": "1.4 Algunas definiciones importantes\n\nPoblación y muestra\nUna población es un conjunto de de personas, cosas o, en general, objetos en estudio. A veces, una población es demasiado grande para que podamos abarcarla completa; para poder estudiarla, obtenemos una muestra, que consiste en un subconjunto de la población que hemos seleccionado para su estudio. El proceso de obtener una muestra se llama muestreo, y se realiza de acuerdo con normas y procedimientos específicos.\nEn muchas ocasiones, cuando se recogen los datos como resultado de una experimentación, definimos la población como todos los resultados que podríamos haber obtenido. Llamamos a este conjunto de posibles resultados una población conceptual. Por ejemplo, cuando medimos el \\(pH\\) de varias muestras de leche, la población es el conjunto de todos los resultados posibles que podríamos haber tenido. Muchos problemas de ingeniería y tecnología se refieren a poblaciones conceptuales.\n\n\n\n\n\n\nRecuerda\n\n\n\nEn la mayoría de las ocasiones, nuestros datos provienen de una muestra obtenida de una población,\n\n\nCuando tomamos una muestra, debemos estar seguros de que contiene las propiedades que queremos estudiar en la población. En ese caso, decimos que la muestra es representativa: los individuos de la muestra son representativos de la población. Para que la muestra sea representativa, debe ser obtenida mediante un muestreo aleatorio. Una muestra aleatoria simple de tamaño \\(n\\) consiste en \\(n\\) individuos de una población, elegidos de forma que cada conjunto posible de \\(n\\) individuos tiene la misma probabilidad de ser elegido.\n\n\n\n\n\n\n¿Qué es la probabilidad?\n\n\n\nEl concepto de probabilidad se explica en el capítulo 6\n\n\n\n\nParámetro y estadístico\nUn parámetro es una característica de una población. Podemos estimar su valor mediante la extracción de una muestra, que utilizaremos para calcular un estadístico muestral. Llamamos estadístico a un número que representa una propiedad o característica de la muestra, y constituye una estimación del valor de un parámetro de la población que estamos estudiando.\n\n\nVariables y casos\nA los objetos descritos en un conjunto de datos los llamamos casos, de forma genérica. A veces, estos casos pueden corresponder a personas; en ese caso podemos llamarlos individuos. Cuando los objetos que estudiamos no son personas, como es lo habitual en el entorno industrial, utilizamos la nomenclatura genérica.\nUn atributo es una característica que define una propiedad de un objeto, persona o cosa. Por ejemplo, edad, peso, altura, sexo, color de ojos, son atributos de una persona. Llamamos variable a una característica cualquiera de un individuo que puede ser medida. Una variable puede tomar diferentes valores en diferentes individuos o casos.\nSegún estas definiciones que acabamos de ver, una muestra está formada por un conjunto de casos, y cada caso contiene un determinado número de variables, que contienen los valores que hemos analizado o medido.\n\n\n\n\n\n\nEjemplo 1: Muestreando una cámara de maduración de queso\n\n\n\nImagínate que tienes que analizar el extracto seco de una producción de queso que está en fase de maduración en una cámara. Como la cámara está muy llena, es difícil acceder al interior, y decides coger tu muestra de los quesos que están más a tu alcance, justo al lado de la puerta y a la altura de la vista.¿Crees que es una buena idea? ¿Podrías definir la población en este caso?.\n\n\n\n\n\n\n\n\nRespuesta al ejemplo 1: Muestreando una cámara de maduración de queso\n\n\n\n\n\nNo es una buena idea porque no tenemos garantía de que las condiciones de humedad,temperatura y circulación de aire sean las mismas en toda la cámara. Para asegurar que nuestra muestra es representativa, debemos tomar una muestra aleatoria de la población, que en este caso es el total de quesos en la cámara.\n\n\n\n\n\nTipos de variables\nAlgunas variables, como el color, sirven para clasificar los individuos en categorías. Otras, como la altura o el peso de un individuo, pueden tomar valores numéricos con los que podemos hacer cálculos. Por ejemplo, podemos sumar la altura de varias personas, pero no tiene sentido sumar los colores del arco-iris (aunque sí podemos contarlos, y hacer cálculos con estos recuentos). También podemos categorizar variables continuas: podemos clasificar nuestro grupo de personas en altas o bajas, y podemos contar cuántas personas entran en cada categoría.\n\n\n\n\n\n\n\n\n\nVariables cualitativas  o categóricas\n\nVariables cuantitativas  o métricas\n\n\n\n\n\nNominales\nOrdinales\nDiscretas\nContinuas\n\n\nValores en categorías arbitrarias\nValores en categorías ordenadas\nValores enteros en escala numérica\nValores continuos en escala numérica\n\n\n(sin unidades)\n(sin unidades)\nUnidades contadas\nUnidades medidas\n\n\n\nUna variable categórica coloca a un individuo en uno o más grupos o categorías\nUna variable métrica toma valores numéricos con los que tiene sentido realizar cálculos aritméticos como sumar, restar, etc.\nLas variables categóricas se conocen también como variables cualitativas porque indican cualidades.\nLas variables métricas se conocen también como variables cuantitativas porque indican cantidades.\n\n\n\n\n\n\nComentario: ¿Cualitativo quiere decir “que tiene calidad”?\n\n\n\nA veces se utiliza la palabra cualitativo de forma incorrecta para indicar calidad, por ejemplo cuando alguien dice: “Este envase es muy cualitativo”. Deberíamos decir “Este envase tiene gran calidad”. Cualitativo no se deriva de calidad, sino de cualidad.\n\n\n\n\n\n\n\n\nPara resolver\n\n\n\nEjemplo 1. Tiramos un dado al aire. Describe a qué corresponde la variable y el caso.\nEjemplo 2. Durante un proceso de envasado de un producto que dura una hora, controlamos el peso de cada envase cada minuto. Describe la variable y el caso. ¿Puede haber más de una variable?\n\n\n\n\n\n\n\n\nRespuestas: Para resolver\n\n\n\n\n\nEjemplo 1: La variable es el resultado que obtenemos cada vez; podríamos denominarla, por ejemplo, \\(resultado\\_obtenido\\). Colocaríamos este nombre en el encabezado de una columna en una hoja de cálculo. Cada tirada que hacemos es un caso; iríamos colocando el resultado que obtenemos cada vez en una nueva fila de nuestra hoja de cálculo.\nEjemplo 2. En este caso, la variable es el \\(peso\\_obtenido\\), y cada pesada constituye un caso. Si registrásemos, además, la hora y el minuto en el que que hemos hecho cada control de peso, podríamos definir una nueva variable, que podríamos llamar \\(hora\\), y que colocaríamos en una columna al lado del \\(peso\\_obtenido\\). Incluso podríamos definir otra variable adicional, el \\(numero\\_de\\_pesada\\), que sería un número secuencial empezando en \\(1\\) y que se incrementaría en cada pesada, de forma que al final esta variable nos daría el número de pesadas realizadas, y nos indicaría además el orden en el que las hemos realizado. Puesto que hemos realizado una pesada cada minuto, tendríamos tres variables y 61 líneas (un encabezado y 60 líneas correspondientes una a cada minuto)\n\n\n\n\n\nReglas básicas para establecer los nombres de las variables.\nSegún hemos visto, existen diferentes tipos de variables, cualitativas (categóricas) y cuantitativas (métricas). Normalmente, los valores de las variables categóricas se describen mediante textos del tipo “color blanco”, “hombre”, “mujer”, “alto”, “bajo”, etc.  Suelen corresponder con características descriptivas, y por lo tanto, no puede hacerse cálculos directamente con ellos, a menos que se hayan resumido, por ejemplo, mediante un conteo. Las variables métricas consisten en valores numéricos, que pueden ser enteros o continuos, y pueden utilizarse directamente para hacer cálculos tales como sumas, etc.A partir de aquí utilizaremos una nomenclatura compatible con las hojas de cálculo en formato europeo para escribir los números; usaremos la coma para la separación decimal y el punto y coma para la separación de los números cuando los escribamos de forma seriada.\nUna variable está descrita siempre por un nombre, que designa la variable, y un valor o conjunto de valores, que corresponden a los casos. Este conjunto de valores, como acabamos de ver, pueden ser textos o números.\nExisten también otros tipos de variables que veremos más tarde, como variables lógicas o fechas, según el tipo de dato que almacenemos en esa variable.\nEjemplos de valores de texto: “Carlos”, “fruta”, “Lluvia fuerte”, “muy ácido”, “sabor a fresa”\nEjemplos de valores numéricos: \\(1\\); \\(7\\); \\(10,65\\)\nSiempre que sea posible, utilizaremos el nombre del atributo o característica que estamos midiendo o analizando, o su abreviatura, para designar una variable; por ejemplo, si estamos recogiendo la altura de una serie de personas, llamaremos altura a la variable; si estamos recogiendo el peso, usaremos el nombre peso, etc.\nA veces, asignar un nombre a una variable no es todo lo fácil que podría parecer a simple vista. Por ejemplo, ¿qué nombre daríamos a una variable que va a recoger los valores de \\(pH\\) de la leche en una cuba de queso en el momento de añadir el cuajo? Está claro que \\(pH\\) no es suficiente, porque en el proceso hay varias medidas de \\(pH\\) y sería bueno que pudiésemos diferenciarlas con facilidad. En un caso como éste, es probable que necesitemos utilizar varias palabras o abreviaturas que describan mejor el nombre de la variable.\nPara la construcción correcta de estos nombres, se han establecido un conjunto de reglas, con el objetivo de evitar errores y facilitar el intercambio de los datos entre diferentes programas de análisis. Son éstas:\n\nUn nombre válido consiste en una combinación de letras, números y signo de subrayado (\\(\\_\\))\nUn nombre de variable no puede empezar por un número, un punto o un signo de subrayado (\\(\\_\\)); debe empezar siempre por una letra.\nLos nombres de variables irán siempre en minúsculas. Según esta regla, \\(Peso\\) no es un nombre válido, pero \\(peso\\) si lo es.\nNo utilizaremos espacios en blanco, acentos ni caracteres especiales como \\(\\tilde{n}\\), \\(\\%\\), guiones o paréntesis.\nHay veces en que nos interesa unir varias palabras para construir un nombre de variable. Se utilizan diferentes formas de unir palabras, por ejemplo:\n\nun punto, como en \\(peso.en.cm\\),\nlo que se ha llamado escritura de camello (camelCase), que se llama así por el uso de mayúsculas y minúsculas mezcladas (\\(PesoEnCm\\))\nel signo de subrayado \\(\\_\\), como en \\(peso\\_en\\_cm\\)\n\nAlgunas de estas opciones son utilizadas en distintas comunidades de usuarios, por ejemplo la opción 1 es utilizada en la guía de estilo de Google, y la opción 2 es muy utilizada por los programadores del entorno de los lenguajes de Microsoft. Nosotros utilizaremos el signo de subrayado (\\(\\_\\)), que es la forma más usada en el entorno de programación de R.\nSiempre se separarán las palabras mediante el signo de subrayado (_) para facilitar la lectura. Así, aunque \\(temperatura1\\) es un nombre válido, preferiremos \\(temp\\_1\\); es más corto y de lectura más clara. Igualmente, preferiremos \\(peso\\_empaquetado\\) a \\(pesoempaquetado\\)\nMantendremos los nombres razonablemente cortos para facilitar la lectura. Aunque podemos hacer los nombres todo lo largos que queramos, es más cómodo utilizar nombres cortos. Por ejemplo, podríamos utilizar \\(temperatura\\_de\\_la\\_leche\\_al\\_cuajar\\), pero preferiremos abreviarlo como \\(temp\\_cuajo\\).\n\nNombres no válidos:\n\n\\(peso\\ en\\ gramos\\) (contiene espacios)\n\\(pH\\_de\\_la\\_leche\\_en\\_Recepci\\acute{o}n\\) (demasiado largo, tiene un acento, tiene mayúsculas)\n\\(extracto\\_seco\\_total\\_a\\_la\\_salida\\_de\\_la\\_salmuera\\) (demasiado largo)\n\nAlternativas válidas:\n\n\\(peso\\_g\\)\n\\(pH\\_leche\\_rec\\) (en este caso, de manera excepcional, podemos mantener el uso de la mayúscula por corrección formal)\n\\(est\\_salida\\_sal\\)\n\nUn caso particular es el uso de la \\(\\tilde{n}\\), ya que no hay una alternativa fácil para el uso en las fechas (\\(a\\tilde{n}o\\)). R admite el uso de la \\(\\tilde{n}\\) en los nombres de variables, por lo que podremos usarlo con cuidado, poniendo atención a los posibles errores que se pudiesen producir en algunas librerías.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html",
    "href": "020-herramientas.html",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "",
    "text": "2.1 Introducción\nEn este capítulo veremos las herramientas que utiizaremos en el análisis de datos industriales. Nos enfocaremos en dos de ellas: la hoja de cálculo Microsoft Excel, y el software estadístico R. Ambas herramientas están ampliamente extendidas en las empresas y en las instituciones docentes de todo el mundo. R es, además, un software libre, y por lo tanto, con un coste de adquisición cero, lo que facilita su utilización. Microsoft Excel tiene versiones web que se pueden utilizar para un uso básico también sin coste. No son las únicas opciones: hay otros programas estadísticos y de análisis muy utilizados y potentes, tales como Matlab o Minitab, que son de gran interés en ingeniería, aunque tienen un coste bastante elevado, y hojas de cálculo como Google Docs o LibreOffice, casi totalmente compatibles con Microsoft Excel.\nEste capítulo no es un manual de aprendizaje de estas herramientas; se supone que se dispone del conocimiento básico para comprender las instrucciones que se proporcionarán aqui. Como se indicó en el prefacio (ver Sección 5), en internet hay multitud de alternativas de calidad para aprender tanto Excel como R de forma gratuita, en ese capítulo se presentaron algunas recomendaciones.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html#la-hoja-de-cálculo",
    "href": "020-herramientas.html#la-hoja-de-cálculo",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "2.2 La hoja de cálculo",
    "text": "2.2 La hoja de cálculo\nLa hoja de cálculo es una herramienta presente hoy día en todos los ámbitos de trabajo y educativos. Desde la aparición de Visicalc, en 1978, ha contribuido a la gestión de miles de empresas, se ha utilizado de manera general en análisis de datos y sus gráficos se han utilizado y se utilizan en publicaciones e informes de todas clases. En la década de los años 80 del pasado siglo, la hoja de cálculo Lotus 1-2-3 fue la aplicación más utilizada en los ordenadores IBM-PC y compatibles, y consiguió facturaciones millonarias para la empresa matriz. Lotus 1-2-3 dominó el mercado hasta la aparición de Microsoft Windows a finales de los años 80; este nuevo sistema operativo favoreció la implantación de Microsoft Excel, que desde entonces se convirtió en la hoja de cálculo dominante.\n\n\n\n\n\n\n\n\n\nVisicalc, primera hoja de cálculo para el ordenador Apple II (1979)\n\n\n\n\n\n\n\nHoja de cálculo Lotus 1-2-3 para MS-DOS (1983)\n\n\n\n\n\n\n\n\n\nMicrosoft Excel (2023)\n\n\n\n\n\n\nAplicaciones de la hoja de cálculo\nLas hojas de cálculo son muy útiles para recoger la información de un conjunto de observaciones. Entre sus principales usos, están:\n\nLa introducción, edición y almacenamiento datos.\nEl filtrado y corrección de errores.\nLa manipulación básica, por ejemplo, mediante tablas dinámicas\nLa preparación y edición de gráficos, incluyendo gráficos dinámicos\nLa presentación de la información, con el apoyo opcional de herramientas adicionales como Microsoft PowerPoint.\n\nLos datos se pueden recoger y guardar de múltiples formas. Cuando la recogida de datos se hace de forma manual en papel, es necesario registrar en el ordenador los datos recogidos. Lo más frecuente es que este registro se haga en hojas de cálculo, como Microsoft Excel o Google Sheets. En algunos casos, el almacenamiento se hace sobre bases de datos, genéricas o desarrolladas a medida.\nActualmente, la tendencia es recoger los datos o bien de forma automática, o bien de forma manual sobre sistemas informatizados (pantallas), lo que permite eliminar el papel y disponer directamente de los datos en un formato digitalizado.\nEn la actualidad, la mayoría de los equipos y líneas de producción se interconectan con los sistemas de información (ver IoT) y almacenan en tiempo real todos los datos necesarios, lo que libera al operario de la pesada tarea de reintroducirlos manualmente, a la vez que reduce los errores debidos a la imputación incorrecta.\nEn todos los casos, es imprescindible asegurar que los sistemas de información pueden exportar sus datos a ficheros de texto tipo fichero plano o tipo CSV, de forma que podamos importarlos tanto a Excel como a R, como veremos más adelante. Estos sistemas de exportación de datos deben diseñarse de forma flexible y abierta, para que tanto la captura como la exportación puedan modificarse y adaptar la recogida de la información a las necesidades de cada momento.\nEn este libro trataremos exclusivamente de lo que llamaremos datos rectangulares: grupos de valores que están asociados a una o más variables, y a varias observaciones. Hay muchos más datos que no se ajustan a esta organización tabular, es el caso de imágenes, sonidos o archivos documentales de texto. Pero la forma más común de almacenar datos industriales es la de las tablas rectangulares, organizadas según el principio de los datos arreglados, que detallaremos en el siguiente capítulo.\n\n\nEl uso correcto de una hoja de cálculo en el análisis de datos industriales\n[…]",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html#el-software-estadístico-r",
    "href": "020-herramientas.html#el-software-estadístico-r",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "2.3 El software estadístico R",
    "text": "2.3 El software estadístico R\nR es un lenguaje de programación y un entorno de software utilizado para el análisis estadístico, la visualización de datos y la modelización. Algunas de las características clave de R son:\n\nAmplio espectro de funcionalidades: R abarca una amplia gama de herramientas y paquetes diseñados para realizar diversos análisis estadísticos, exploración de datos y modelización.\nHerramientas gráficas: R dispone de algunas de las bibliotecas gráficas más potentes para la exploración y descripción de datos.\nEstadística descriptiva: R ofrece funciones para calcular estadísticas descriptivas básicas, como la media, mediana, desviación estándar, varianza, rango, cuartiles y percentiles. Estas funciones son esenciales para explorar y resumir datos.\nContrastes de hipótesis: R proporciona funciones para realizar pruebas estadísticas, como t-tests, test chi-cuadrado, ANOVA y pruebas no paramétricas. Estas pruebas permiten evaluar hipótesis y comparar grupos de datos.\nDistribuciones de probabilidad: R incluye una amplia variedad de funciones para trabajar con distribuciones de probabilidad (por ejemplo, normal, uniforme, binomial, Poisson). Esto es útil para generar números aleatorios, calcular probabilidades y cuantiles.\nBibliotecas de funciones (librerías): Además de las amplias funciones básicas de las que dispone, R es capaz de utilizar bibliotecas de funciones (llamadas librerías) que han sido desarrolladas por los propios usuarios, y que amplían sus funcionalidades a todos los campos imaginables, desde el análisis genético al análisis de riesgos bancarios o el control estadístico de procesos.\n\n\nUsos y aplicaciones de R en la estadística industrial\nEn el contexto de la estadística industrial, R se utiliza para:\n\nControl de calidad: R permite analizar datos de procesos industriales, identificar desviaciones y controlar la calidad de los productos.\nOptimización de procesos: Mediante técnicas estadísticas avanzadas, R ayuda a optimizar procesos industriales, reducir costos y mejorar la eficiencia.\nAnálisis de fiabilidad: R se utiliza para evaluar la confiabilidad de sistemas y componentes en la industria.\n\n\n\nUtilización práctica de R en el entorno industrial\n[..a desarrollar..]\n\nImportación de datos y exportación de datos\nManipulación de datos: depuración, corrección, filtrado de datos.\nExploración gráfica de los datos\nAnálisis estadísticos específicos.\nGráficos de control\nInformes automatizados",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html#otras-herramientas-python-julia",
    "href": "020-herramientas.html#otras-herramientas-python-julia",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "2.4 Otras herramientas: Python, Julia",
    "text": "2.4 Otras herramientas: Python, Julia\nTanto Python como Julia son excelentes alternativas a R en el análisis de datos industriales y la estadística industrial, cada uno con sus propias ventajas:\n\nPython\nPython es uno de los lenguajes de programación más populares en el análisis de datos industriales debido a su versatilidad y la amplia disponibilidad de bibliotecas como pandas, NumPy, SciPy, scikit-learn y Matplotlib. Su integración con otras tecnologías (como bases de datos y servicios en la nube) y su facilidad de uso lo convierten en una opción preferida para tareas de análisis de datos, machine learning y visualización de datos.\n\nVentajas:\n\nVersatilidad: Más allá del análisis de datos, Python es un lenguaje generalista con una amplia gama de aplicaciones, desde desarrollo web hasta machine learning. Esto lo convierte en una herramienta altamente versátil para diversas tareas industriales.\nEcosistema de librerías: Cuenta con una vasta colección de librerías especializadas en ciencia de datos (Pandas, NumPy, SciPy), machine learning (Scikit-learn, TensorFlow), y visualización (Matplotlib, Seaborn).\nIntegración con otras tecnologías: Se integra fácilmente con otros lenguajes y herramientas, lo que facilita la automatización de procesos y la construcción de pipelines de datos.\nComunidad activa: Tiene una comunidad enorme y en constante crecimiento, lo que significa una gran cantidad de recursos, tutoriales y soporte disponible.\n\nRecientemente, Microsoft ha introducido una nueva funcionalidad que permite a los usuarios escribir código Python directamente en las celdas de Excel. Esta integración permite a los usuarios aprovechar la potencia de Python para procesar datos en Excel, realizar cálculos complejos y visualizar datos mediante gráficos de matplotlib insertados directamente en la hoja de cálculo. Para usar esta funcionalidad, los usuarios pueden seleccionar una celda y, en la pestaña Fórmulas, seleccionar Insertar Python. También se puede habilitar Python en una celda escribiendo =PY y eligiendo PY en el menú de autocompletar.\n\n\n\nJulia\nJulia es un lenguaje más reciente, diseñado específicamente para el cálculo numérico y la ciencia de datos. Ofrece un rendimiento cercano al de lenguajes de bajo nivel como C, pero con la sintaxis y facilidad de uso de lenguajes de alto nivel como Python y R. Julia es especialmente útil en aplicaciones donde el rendimiento es crítico, como en simulaciones industriales y análisis de grandes volúmenes de datos.\n\nVentajas\n\nRendimiento: Diseñado específicamente para computación numérica de alto rendimiento, Julia ofrece velocidades comparables a lenguajes compilados como C o Fortran, pero con una sintaxis más cercana a los lenguajes de scripting. Es excelente para cálculos numéricos intensivos y proporciona soporte nativo para paralelismo y computación distribuida.\nSintaxis expresiva: Su sintaxis es intuitiva y similar a la de lenguajes matemáticos, lo que facilita la escritura de código conciso y legible.\nInteroperabilidad: Permite llamar a código escrito en otros lenguajes como C, Python o R, lo que facilita la integración con herramientas existentes.\nEn crecimiento: Aunque más joven que Python y R, Julia está ganando rápidamente popularidad en la comunidad científica y de datos.\n\n\nEn resumen, mientras que R sigue siendo una opción poderosa y preferida para la estadística industrial, Python y Julia se presentan como alternativas viables y, en algunos casos, superiores, dependiendo de los requisitos específicos del proyecto.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html#el-concepto-de-reproducibilidad",
    "href": "020-herramientas.html#el-concepto-de-reproducibilidad",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "2.5 El concepto de reproducibilidad",
    "text": "2.5 El concepto de reproducibilidad\nLa reproducibilidad de un ensayo o experimento es la capacidad de ser reproducido o replicado por otros, en particular, por la comunidad científica. La reproducibilidad se refiere a la capacidad de obtener resultados consistentes al replicar un estudio o experimento utilizando los mismos datos, metodología original y, en su caso, el mismo código informático empleado para los análisis.En otras palabras, cuando se replica un análisis de datos o un experimento, los resultados deben ser alcanzados nuevamente con un alto grado de confiabilidad.\nLa repetibilidad o replicabilidad se refiere a la posibilidad de obtener resultados consistentes al replicar un estudio con un conjunto distinto de datos, pero obtenidos siguiendo el mismo diseño experimental. Implica obtener resultados consistentes utilizando nuevos datos o nuevos resultados computacionales para responder a la misma pregunta científica.\nEl químico irlandés Robert Boyle, en el siglo XVII, subrayó la importancia de la reproducibilidad en la ciencia. Boyle sostenía que los fundamentos del conocimiento debían basarse en hechos producidos experimentalmente, que pudieran volverse creíbles para la comunidad científica por su reproducibilidad. La bomba de aire de Boyle, un aparato científico complicado y costoso en ese momento, condujo a una de las primeras disputas documentadas sobre la reproducibilidad de un fenómeno científico.\n\nImportancia en la Ciencia\nLa reproducibilidad es esencial para la investigación científica, ya que permite validar y verificar los resultados obtenidos. En las últimas décadas, ha habido una creciente preocupación por la falta de reproducibilidad en muchos resultados científicos publicados, lo que ha llevado a una crisis de reproducibilidad o replicación. La reproducibilidad garantiza que los resultados científicos sean confiables y puedan ser validados por otros investigadores. Es un pilar fundamental para el avance del conocimiento en todas las disciplinas.\n\n\nReproducibilidad en metrología\nEn metrología, la reproducibilidad es la capacidad de un instrumento de dar el mismo resultado en mediciones diferentes, realizadas en las mismas condiciones a lo largo de periodos dilatados de tiempo. Esta cualidad debe evaluarse a largo plazo.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "020-herramientas.html#ventajas-de-r-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "href": "020-herramientas.html#ventajas-de-r-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "title": "2  Herramientas para el análisis. La reproducibilidad.",
    "section": "2.6 Ventajas de R frente a la hoja de cálculo en la reproducibilidad de un análisis",
    "text": "2.6 Ventajas de R frente a la hoja de cálculo en la reproducibilidad de un análisis\nLas ventajas de utilizar R en lugar de hojas de cálculo tradicionales (como Microsoft Excel) incluyen:\n\nCódigo abierto (scripts): R permite crear flujos de trabajo basados en código, lo que mejora la reproducibilidad de los análisis y facilita la colaboración entre investigadores. Puedes escribir scripts en R para automatizar tareas y asegurar la reproducibilidad. Los scripts son transparentes y pueden ser compartidos y verificados por otros investigadores.\nFlexibilidad estadística: R es especialmente útil para técnicas avanzadas de análisis, lo que lo convierte en una excelente opción para investigadores que buscan análisis de vanguardia.\nCapacidad para tratar grandes cantidades de datos: R puede manejar grandes conjuntos de datos sin problemas, lo que es fundamental en la estadística industrial.\nPrecisión: R está diseñado específicamente para análisis estadístico, lo que lo hace más preciso que Excel en ciertos casos, como en análisis de regresión lineal.\nCapacidad avanzada: R ofrece una amplia gama de paquetes y funciones para realizar análisis estadísticos avanzados, como modelos lineales, series temporales, análisis multivariante y más.\n\nPor su parte, la hoja de cálculo tiene una curva de aprendizaje más sencilla y es en general más fácil de usar, pero tiene algunos inconvenientes:\n\nInexactitudes: Estudios han demostrado que Excel puede mostrar ciertas inexactitudes en análisis de regresión lineal y otros métodos estadísticos.\nLimitaciones estadísticas: Excel no está diseñado específicamente para análisis estadístico avanzado, por lo que puede carecer de algunas capacidades necesarias para investigaciones más complejas.\nFalta de transparencia para la auditoría: Las fórmulas y cálculos en Excel pueden ser difíciles de rastrear y verificar, lo que afecta la reproducibilidad.\n\nPor estas razones usaremos Excel para el almacenamiento de datos y el análisis básico, y usaremos R para el análisis gráfico más detallado y el análisis numérico y estadístico.",
    "crumbs": [
      "Introducción",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html",
    "href": "030-organizacion.html",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "3.1 Etapas en un flujo de trabajo estructurado.\nUn flujo de trabajo en análisis de datos es un proceso sistemático y estructurado que guía la manipulación, exploración y análisis de datos desde su recolección hasta la obtención de resultados finales y su comunicación. Es una hoja de ruta que asegura que cada paso se realice de manera ordenada, eficiente y reproducible, facilitando la comprensión y utilización de los datos.\nHadley Wickham (Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) ha propuesto un método de flujo de trabajo que se ha convertido en estándar en la ciencia de datos (hay versión en español: (Garret Grolemund Hadley Wickham 2023))\nEste flujo de trabajo abarca diversas actividades como la importación de datos, su limpieza y transformación, el análisis exploratorio, y el modelado, culminando en la interpretación y presentación de los resultados. Todo esto se hace siguiendo metodologías específicas para asegurar la calidad y precisión del análisis.\nLa importancia de seguir un flujo de trabajo bien definido radica en la capacidad de replicar estudios, minimizar errores y fomentar la transparencia, permitiendo que cualquier persona pueda entender y validar las decisiones tomadas durante el análisis. Además, mejora la eficiencia al estandarizar procedimientos y facilita la colaboración entre diferentes analistas o equipos de trabajo.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "href": "030-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "Recolección de datos\nLa primera etapa es la recolección de datos. Esto implica obtener datos desde diversas fuentes como archivos CSV, bases de datos, APIs, etc. La recolección de datos es fundamental porque la calidad del análisis depende de la calidad de los datos recolectados.\n\n\nInspección de los datos\nUna vez recolectados, se procede a inspeccionar los datos para entender su estructura y contenido. Esto incluye examinar los tipos de datos, la presencia de valores faltantes, duplicados y la distribución de las variables.\n\n\nLimpieza de los datos\nLa limpieza de datos es crucial para asegurar que la información sea precisa y esté en el formato adecuado. Esta etapa incluye:\n\nManejo de valores faltantes.\nEliminación de duplicados.\nCorrección de inconsistencias.\nTransformación de datos a un formato adecuado para el análisis.\n\n\n\nTransformación de los datos\nTransformar los datos a un formato ordenado o arreglado (tidy) es esencial. Según Wickham, los datos arreglados tienen una estructura clara: cada variable es una columna, cada observación es una fila, y cada valor tiene su propia celda. Este formato facilita el análisis y la visualización de datos.\n\n\nAnálisis exploratorio de datos (EDA)\nEl Análisis Exploratorio de Datos (EDA) busca entender los patrones y relaciones en los datos mediante estadísticas descriptivas y visualizaciones. Durante esta etapa se realizan:\n\nEstadísticas básicas (media, mediana, desviación estándar).\nGráficos y diagramas para visualizar la distribución de los datos y las relaciones entre variables.\n\n\n\nModelado de datos\nDependiendo del objetivo del análisis, se pueden aplicar diversos modelos estadísticos para extraer información y hacer predicciones. Esto puede incluir:\n\nModelos de regresión.\nAnálisis de clasificación.\nModelos de series temporales, entre otros.\n\n\n\nComunicación de resultados\nFinalmente, es fundamental comunicar los resultados de manera clara y efectiva. Esto se hace a través de:\n\nTablas y resúmenes interpretativos.\nGráficos y visualizaciones.\nInformes y presentaciones que expliquen los hallazgos y sus implicaciones.\n\nSiguiendo estos pasos, puedes manejar y analizar datos de manera organizada y reproducible, facilitando la colaboración y la toma de decisiones informadas. Este flujo de trabajo asegura que los datos se traten de manera sistemática, desde su recolección hasta la comunicación de los resultados.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "href": "030-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.2 Razones para seguir un flujo de trabajo",
    "text": "3.2 Razones para seguir un flujo de trabajo\n\nReproducibilidad: Un flujo de trabajo organizado permite que los análisis sean reproducibles. Otros pueden seguir los mismos pasos para obtener resultados similares, lo que es crucial en la investigación y en la toma de decisiones basadas en datos.\nConsistencia: Ayuda a asegurar que los pasos se realizan de manera consistente cada vez que se ejecuta el análisis, reduciendo la posibilidad de errores humanos.\nTransparencia: Proporciona un registro claro de los pasos tomados durante el análisis, facilitando la revisión y validación de los resultados.\nEficiencia: Mejora la eficiencia al estandarizar el proceso, permitiendo a los analistas concentrarse en el análisis y la interpretación de los datos en lugar de tareas repetitivas.\nColaboración: Facilita la colaboración entre equipos, ya que los flujos de trabajo bien documentados permiten que otros comprendan fácilmente los métodos y pasos utilizados.\nAdaptabilidad: Permite adaptar y ajustar el análisis de manera más fácil cuando se presentan nuevos datos o cuando cambian los objetivos del análisis.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "href": "030-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.3 Problemas de no seguir un flujo de trabajo estructurado",
    "text": "3.3 Problemas de no seguir un flujo de trabajo estructurado\n\nErrores y Sesgos: La falta de un enfoque estructurado puede resultar en errores y sesgos inadvertidos en el análisis, lo que puede llevar a conclusiones incorrectas.\nDificultad para Replicar Resultados: Sin un flujo de trabajo claro, replicar resultados se vuelve complicado, lo que puede afectar la credibilidad del análisis y la capacidad de validación por otros.\nFalta de Documentación: La ausencia de una documentación adecuada dificulta entender los pasos y las decisiones tomadas durante el análisis, lo que puede ser un obstáculo en auditorías y revisiones.\nIneficiencia: Sin una estructura clara, los analistas pueden gastar tiempo valioso realizando tareas repetitivas y resolviendo problemas que podrían haberse evitado con un enfoque más organizado.\nProblemas de Colaboración: La colaboración se vuelve más difícil si los miembros del equipo no pueden seguir o entender los pasos tomados por otros, lo que puede llevar a malentendidos y duplicación de esfuerzos.\nDificultad para Adaptarse a Cambios: Sin un flujo de trabajo definido, adaptar el análisis a nuevos datos o cambios en los objetivos puede ser más complejo y propenso a errores.\n\nEn resumen, seguir un flujo de trabajo estructurado es esencial para garantizar la precisión, eficiencia, y reproducibilidad del análisis de datos, evitando problemas que puedan comprometer la integridad y utilidad de los resultados. .",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "href": "030-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.4 Un ejemplo: revisando los datos existentes.",
    "text": "3.4 Un ejemplo: revisando los datos existentes.\nCuando nos incorporamos a un equipo de trabajo existente, lo más seguro es que ya se disponga de un sistema de archivo de los datos, de acuerdo con los métodos habituales del equipo. En muchos casos, el diseño de la captura de datos sigue aproximadamente el modelo manual en papel; se introducen los datos en la hoja de cálculo y una vez completados, se imprime el documento para su archivo.\nEl error más común que se suele cometer es, precisamente, tratar la hoja de cálculo como un bloc de notas, es decir, hacer anotaciones de forma libre, colocar los datos y el resultado de los análisis al lado y en cualquier parte de la hoja, y apoyarnos en el contexto para interpretar lo que hemos guardado. Pero para que el ordenador sea capaz de analizar nuestros datos de manera eficiente, debemos estructurarlos de tal forma que el programa use la información tal como nosotros queremos.\nEs común utilizar una hoja para guardar múltiples tablas de datos, tal como vemos en la Figura 3.1. Esta estructura, sin embargo, resulta enormemente confusa para su análisis, o lo imposibilita completamente.\n\n\n\n\n\n\nFigura 3.1: Hoja Excel desordenada: ¡No hagas esto!\n\n\n\nEn otros casos, los datos se guardan en hojas de cálculo que se componen de diferentes pestañas para cada semana, cada mes o cada año, como vemos en la Figura 3.2. Sin embargo, esta forma de almacenar los datos tampoco es la óptima para su análisis.\n\n\n\n\n\n\nFigura 3.2: Hoja Excel con una estructura no ordenada\n\n\n\nSi las diferentes tablas presentan situaciones diferentes, o datos que no están relacionados, podemos utilizar diferentes pestañas. Pero si los datos están vinculados, por ejemplo, se corresponden con las mismas medidas, hechas en fechas diferentes (meses, años), la respuesta es que las pestañas no son la forma correcta de almacenarlos datos; lo mejor es añadir una variable que nos permita diferenciar los datos por fecha; nuestro programa de análisis nos permitirá filtrar los datos según la fecha que deseemos, y todos estarán en una única tabla, facilitando la coherencia y el análisis posterior.\nHay muchas formas de almacenar la información en una hoja de cálculo, pero sólo la estructura de datos ordenados o arreglados facilita la utilización de los datos tanto por la hoja de cálculo como por otros programas de análisis.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "href": "030-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.5 Los datos ordenados o arreglados (tidy data)",
    "text": "3.5 Los datos ordenados o arreglados (tidy data)\nDe la misma manera que la gramática permite ordenar y estructurar un escrito de acuerdo a reglas comunes, hay reglas para que el almacenamiento de los datos sea lo más homogéneo posible y se reduzcan los errores al mínimo.\nLas reglas principales al almacenar nuestros datos en una hoja de cálculo son tres:\n\ncolumnas=variables,\nfilas=observaciones,\nceldas=valores.\n\nCada variable debe tener su propia columna, cada observación debe tener su propia fila, y cada valor debe tener su propia celda o casilla .\nEstas tres reglas básicas son las que hacen que nuestro conjunto de datos sea ordenado (o arreglado)(Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) (hay edición online en español: (Garret Grolemund Hadley Wickham 2023):\nLa Figura 3.3 muestra estas reglas de forma visual.\n\n\n\n\n\n\nFigura 3.3: Sigue estas tres reglas para tener un conjunto de datos ordenado: las variables están en columnas, las observaciones están en filas, y los valores están en celdas. Fuente de la imagen: Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel (2023)\n\n\n\nEstas tres reglas están interrelacionadas porque es imposible satisfacer sólo dos de tres.\nEn una hoja de cálculo, una tabla de datos arreglada tendría este aspecto:\n\n\n\n\n\n\nFigura 3.4: Hoja Excel con estructura rectangular de datos ordenados\n\n\n\nDatos rectangulares: formato tabla",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "href": "030-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.6 Qué es un fichero plano y un fichero CSV",
    "text": "3.6 Qué es un fichero plano y un fichero CSV\nSe suele llamar fichero plano a un fichero de datos de texto sin ningún tipo de formato, donde los datos están separados por espacios o tabulaciones. Muchos equipos automáticos, como balanzas de laboratorio o básculas de proceso, producen ficheros planos de texto, que se pueden importar a Excel o R. Un fichero CSV es un fichero plano en el que los valores están separados por un carácter especial, llamado separador. Este separador puede ser una coma , (cuando los decimales se separan mediante un punto, como en EEUU) o un punto y coma ; (cuando los decimales se separan mediante una coma, como en España)\n\n\n\n\n\n\n\n\nFichero plano separado por espacios\n\n\n\n\n\n\n\nFichero CSV separado por comas (USA, GB)\n\n\n\n\n\n\n\n\n\nFichero CSV separado por puntos y comas (Europa, España)\n\n\n\n\n\n\nFigura 3.5: Tres tipos de ficheros planos de texto.\n\n\n\nEn un fichero plano o en un fichero CSV, la primera fila puede contener los nombres de las columnas. En algunos casos, los elementos de texto pueden estar entre comillas. En estos casos, los programas de importación se ocupan de la conversión de formatos.\nLa importación de un fichero CSV en Excel en español es directa si se ha generado con puntos y comas como separador y comas para los decimales; si no es así, nos aparecerá como un fichero plano de texto sin formato, y tendremos que realizar una conversión.\n\nCómo exportar un fichero CSV desde Excel a R\nUna vez que tenemos nuestros datos en Excel, hay dos formas en las que podemos poner los datos a disposición de R para su análisis: exportarlos a un archivo de texto con formato CSV, o leer directamente los datos de Excel desde R utilizando la función read_excel() de la librería tidyverse. En ambos casos, el resultado en R es un dataframe o cuadro de datos, que es una estructura equivalente a la de nuestra tabla de datos en Excel.\nEn el ejemplo, utilizaremos un fichero CSV y lo leeremos utilizando una función de base R.\n\nPaso 1: Guardar el Fichero CSV desde Excel\n\nAbre tu fichero en Excel.\nSelecciona Archivo &gt; Guardar como.\nElige el formato CSV (delimitado por comas) (*.csv).\nGuarda el archivo.\n\n\n\nPaso 2: Importar el CSV en R usando Base R\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\nLínea 1: Lectura del archivo CSV\ndatos &lt;- read.csv2(\"ruta/al/archivo.csv\", header = TRUE, sep = \";\", dec = \",\")\n\ndatos &lt;-: Este símbolo &lt;- se utiliza para asignar el resultado de la función a un objeto llamado datos. En este caso, datos contendrá los datos leídos desde el archivo CSV.\nread.csv2(...): Esta es una función en R que se utiliza para leer archivos CSV. La versión read.csv2 se usa comúnmente en Europa y otros lugares donde el separador de campo estándar es el punto y coma (;) y el separador decimal es la coma (,).\nParámetros de read.csv2:\n\n\"ruta/al/archivo.csv\": Especifica la ruta y el nombre del archivo CSV que deseas leer.\nheader = TRUE: Indica que el archivo CSV tiene una fila de encabezado que contiene los nombres de las columnas.\nsep = \";\": Especifica que el separador de campos en el archivo CSV es el punto y coma (;).\ndec = \",\": Indica que el separador decimal en el archivo CSV es la coma (,).\n\n\nLínea 2: Mostrar las primeras filas del datosframe\nhead(datos)\n\nhead(datos): Esta función muestra las primeras 6 filas del dataframe datos (puedes cambiar el número de filas que muestra añadiendo un número como argumento: head(data, 10) mostrará las primeras 10 filas). Esta es una forma útil de inspeccionar rápidamente los datos que has cargado en R.\n\nResumen\nEste código lee un archivo CSV con datos delimitados por punto y coma y con comas como separadores decimales, y almacena los datos en un dataframe llamado datos. Luego, utiliza la función head() para mostrar las primeras 6 filas del dataframe, permitiéndote inspeccionar los datos rápidamente.\n\n\n\n\ndatos &lt;- read.csv2(\"excel-R/aula1.csv\", header = TRUE, sep = \";\", dec = \",\", fileEncoding = \"latin1\")\n\nhead(datos)\n\n   nombre altura_cm\n1    Luis       153\n2     Ana       135\n3    Iván       140\n4   Lucía       140\n5 Jessica       175\n6 Antonio       138\n\n\n\n\n\n\n\n\nPractica con el CSV\n\n\n\nPrueba a guardar y recuperar tus datos en formato CSV",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#uso-de-los-filtros-de-excel-para-revisar-y-editar-los-datos",
    "href": "030-organizacion.html#uso-de-los-filtros-de-excel-para-revisar-y-editar-los-datos",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.7 Uso de los filtros de Excel para revisar y editar los datos",
    "text": "3.7 Uso de los filtros de Excel para revisar y editar los datos",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#para-resolver",
    "href": "030-organizacion.html#para-resolver",
    "title": "3  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "3.8 Para resolver",
    "text": "3.8 Para resolver\nPoner aquí distintos ejemplos de nombres de variables para ver si son válidos o no. Describir medidas y preguntar cómo llamaríamos a esa variable (por ejemplo, temperatura de la leche que acabamos de descargar de una cisterna)\n\n\n\n\nHadley Wickham, Garret Grolemund. 2023. «R Para Ciencia de Datos». 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for Data Science, 2nd ed. 1005 Gravenstein Highway North, Sebastopol, CA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html",
    "href": "040-exploracion.html",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "4.1 Explorando los datos con Excel y R.\nEn este capítulo estudiaremos cómo describir conjuntos de datos de forma visual, utilizando varios tipos de gráficos distintos:\nVeremos la relación visual entre un histograma y un diagrama de caja, y aprenderemos también a construir tablas de frecuencias en Excel y en R. Finalmente, veremos algunos otros tipos de gráficos que son útiles para aplicaciones concretas, como los gráficos de densidad.\nUtilizaremos dos tablas de datos,\nSupongamos que queremos medir la altura de un grupo de alumnos de nuestra clase. Éste es nuestro grupo:\nRealizamos la medida de altura de cada persona y registramos los valores en una hoja de cálculo, siguiendo las buenas prácticas que hemos visto al estudiar los datos ordenados.\nGuardamos esta tabla en el archivo CSV aula1.csv, y lo importamos a un dataframe de R para su uso a lo largo del capítulo.\nlibrary(tidyverse)\ndf_aula &lt;- read.csv2(\"excel-R/aula1.csv\",fileEncoding='latin1')\nhead(df_aula)\n\n   nombre altura_cm\n1    Luis       153\n2     Ana       135\n3    Iván       140\n4   Lucía       140\n5 Jessica       175\n6 Antonio       138",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#explorando-los-datos-con-excel-y-r.",
    "href": "040-exploracion.html#explorando-los-datos-con-excel-y-r.",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "Explicación del código R\n\n\n\n\n\n\nCargar la librería tidyverse:\nlibrary(tidyverse)\n\nlibrary(tidyverse): Carga el paquete tidyverse, que es una colección de paquetes de R diseñados para el análisis de datos. Incluye paquetes como ggplot2 para la visualización de datos, dplyr para la manipulación de datos, tidyr para la organización de datos, entre otros.\n\nLeer el archivo CSV:\ndf_aula &lt;- read.csv2(\"excel-R/aula1.csv\", fileEncoding = 'latin1')\n\ndf_aula &lt;- read.csv2(\"excel-R/aula1.csv\", fileEncoding = 'latin1'): Lee el archivo CSV denominado aula1.csv que se encuentra en la carpeta excel-R y lo almacena en el dataframe df_aula. La función read.csv2 se utiliza para leer archivos CSV donde el separador de campo es un punto y coma (;) y fileEncoding = 'latin1' asegura que los caracteres especiales se manejen correctamente durante la lectura del archivo.\n\nMostrar el contenido del dataframe df_aula:\nhead(df_aula)\n\nhead(df_aula): Muestra las primeras líneas del contenido del dataframe df_aula, para que puedas visualizar los datos que has cargado.. Si simplemente escribimos el nombre del dataframe en la consola, sin usar la función head(), R imprimirá todo el dataframe.\n\n\nResumen\nEste código:\n\nCarga la colección de paquetes tidyverse para facilitar el análisis y manipulación de datos.\nLee un archivo CSV y lo almacena en un dataframe.\nMuestra el contenido del dataframe para que puedas ver los datos que se han cargado.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#el-diagrama-de-tallo-y-hojas-stem-and-leaf-plot-o-stemplot",
    "href": "040-exploracion.html#el-diagrama-de-tallo-y-hojas-stem-and-leaf-plot-o-stemplot",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.2 El diagrama de tallo y hojas (stem and leaf plot o stemplot)",
    "text": "4.2 El diagrama de tallo y hojas (stem and leaf plot o stemplot)\nEl diagrama de tallo y hojas, también conocido como stemplot, es una herramienta gráfica utilizada en estadística para representar la distribución de un conjunto de datos. Es especialmente útil para conjuntos de datos pequeños y proporciona una forma rápida y efectiva de visualizar la forma de los datos y su dispersión. El stemplot recibe este nombre porque el dibujo que resulta se asemeja a un tallo el que le salen las hojas que son los datos individuales.\nLos componentes de un stemplot son:\n\nTallo: Representa el grupo principal de los valores de los datos. Generalmente, se usa la parte más significativa del número. Por ejemplo, en el número 43, el tallo podría ser 4.\nHojas: Representan los dígitos finales o menos significativos de los valores de los datos. Siguiendo el ejemplo anterior, la hoja sería 3.\n\n\nConstrucción del diagrama\nVamos a utilizar los datos de medidas de altura de nuestro grupo de alumnos. Quitamos el último dígito a la derecha de nuestros valores y colocamos verticalmente los valores resultantes ordenándolos de menor a mayor, y evitando las repeticiones. Para evitar errores en la escala, debemos incluir los valores intermedios aunque no haya ninguno en nuestros datos (en el ejemplo, el valor 16 que correspondería a los 160). Esto forma el “tallo” de nuestro diagrama:\n\nA continuación añadimos las hojas en la celda a la derecha, que consisten en los valores que hemos “cortado” de nuestro árbol, uno al lado de otro, incluyendo esta vez los valores repetidos, en orden de menor a mayor. Por ejemplo, para el valor 135, descartamos 13 y utlizamos 5; para el valor 138, descartamos 13 y utilizamos 8, y así sucesivamente para todos los valores.\n\nR permite realizar el stemplot mediante la función \\(stem()\\) de forma automática:\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\ndf_aula$altura_cm:\n\ndf_aula: Este es el nombre del dataframe que contiene tus datos.\n$altura_cm: Este símbolo $ se utiliza para acceder a la columna altura_cm dentro del dataframe df_aula. Así que df_aula$altura_cm se refiere a los datos de la columna altura_cm.\n\nstem(...):\n\nLa función stem() en R genera un diagrama de tallo y hojas para los datos proporcionados. Este gráfico muestra la distribución de los datos de una manera similar a un histograma, pero utilizando valores numéricos en lugar de barras.\n\n\n\n\n\n\nstem(df_aula$altura_cm)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  13 | 58\n  14 | 005\n  15 | 23449\n  16 | \n  17 | 5\n\n\n\n\nInterpretación del diagrama de tallo y hojas\n\nTallo: Los números a la izquierda del símbolo | representan los valores base (o tallos), en este caso, las decenas de las alturas.\nHojas: Los números a la derecha del símbolo | representan los dígitos adicionales (o hojas). Por ejemplo, en la línea 13 | 58, el tallo es 13 (130), y las hojas son 5 y 8, que corresponden a los datos 135 y 138.\n\nEl diagrama nos dice que los valores en torno a 150 cm son los más frecuentes, y que hay un valor alto (175) que se separa un poco del resto.\n\n\nResumen\nEl stemplot es muy sencillo de hacer y nos da una visión rápida y compacta de la distribución de nuestros valores, así como de la posible existencia de valores que se separan del conjunto. Estos valores alejados, que se conocen en inglés como outliers, tienen mucha importancia en el analisis e interpretación de los datos, como veremos más adelante.\nLa ventaja principal del stemplot es que mantiene los valores originales de las observaciones, y puede hacerse fácilmente con bolígrafo y papel, sin necesidad de más herramientas.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#distribuciones-de-frecuencias",
    "href": "040-exploracion.html#distribuciones-de-frecuencias",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.3 Distribuciones de frecuencias",
    "text": "4.3 Distribuciones de frecuencias\nUna distribución de frecuencias es una tabla que muestra la frecuencia con la que ocurren los valores diferentes en un conjunto de datos. Esta herramienta es fundamental en la estadística descriptiva y permite resumir y visualizar cómo se distribuyen los datos de manera clara y comprensible. A partir de una tabla de frecuencias se pueden construir diagramas de barra o histogramas para visualizar la tabla de forma gráfica.\nPara construir una distribución de frecuencias, agrupamos nuestros valores por intervalos, y contamos el número de observaciones que aparecen en cada intervalo. Los componentes de una distribución de frecuencias son:\n\nlas categorías o clases son los intervalos o valores específicos de los datos que se están analizando. Cada categoría representa un rango de valores en caso de datos continuos, o valores específicos en caso de datos discretos.\nla frecuencia absoluta es un recuento simple de cuántas veces aparece cada valor en un conjunto de datos.\nla frecuencia relativa nos muestra la proporción de cada valor frente al total. Puede expresarse como fracción (entre 0 y 1) o como porcentaje (respecto a 100), y se calcula como: \\[\n\\text{Frecuencia Relativa} = \\frac{\\text{Frecuencia Absoluta}}{\\text{Número Total de Observaciones}}\n\\]\nla frecuencia acumulada nos dice cuántas observaciones están por debajo de un cierto valor.\nla frecuencia relativa acumulada es la proporción de valores que están por debajo de un cierto valor\n\n\nConstrucción en Excel y en R\nLa tabla a continuación muestra una distribución de frecuencias de las alturas de nuestro grupo de alumnos, calculada mediante una tabla dinámica de Excel.\n\n\n\n\n\n\nInstrucciones paso a paso en Excel\n\n\n\n\n\nPara crear una tabla de frecuencias de la variable altura_cm mediante tablas dinámicas en Excel, sigue estos pasos:\n1. Selecciona los Datos\n\nAbre tu archivo de Excel y selecciona toda la tabla que incluye los encabezados (nombre y altura_cm).\n\n2. Inserta una tabla dinámica\n\nVe a la pestaña Insertar en la barra de herramientas de Excel.\nHaz clic en Tabla Dinámica.\nEn el cuadro de diálogo que aparece, asegúrate de que el rango de datos seleccionado es correcto y elige dónde deseas colocar la tabla dinámica (en una nueva hoja de cálculo o en la hoja actual).\n\n3. Añade la frecuencia absoluta\n\nEn el panel de campos de la tabla dinámica, arrastra el campo altura_cm a la sección Filas.\nArrastra nuevamente el campo altura_cm a la sección Valores.\n\n4. Ajusta la configuración de valores\n\nHaz clic en el campo altura_cm en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento\nAcepta todo hasta volver a Excel.\n\n5. Añade la frecuencia relativa\n\nEn el panel de campos de la tabla dinámica, arrastra de nuevo el campo altura_cm a la sección Valores. Ahora la variable aparecerá como altura_cm2.\n\n6. Ajusta de nuevo la configuración de valores\n\nHaz clic en el campo altura_cm2 en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento.\nEn ese mismo cuadro, haz click en el botón Formato de número, selecciona Número y 2 decimales, y acepta.\nEn ese mismo cuadro, selecciona la pestaña que dice Mostrar valores como\nEn el menú desplegable, escoge la opción % del total de columnas.\nAcepta todo hasta volver a Excel.\n\n7. Ordena y formatea\n\nPuedes ordenar las alturas en orden ascendente o descendente haciendo clic en la flecha junto a altura_cm en la tabla dinámica.\nTambién puedes cambiar el formato de la tabla dinámica para que sea más fácil de leer.\nPuedes renombrar los encabezados de la tabla para que sea más fácil de leer, rotulando las columnas, por ejemplo, como frec_absy frec_rel, o cualquier otro encabezado que te resulte claro y útil.\n\n\n\n\n\nTambién podemos calcular las frecuencias absolutas y relativas de nuestra tabla en R. La formulación en R requiere una buena comprensión de las funciones de la libreria tidyverse utilizadas en la preparación de la tabla.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nCargar la librería tidyverse:\nlibrary(tidyverse)\n\nlibrary(tidyverse): Carga el paquete tidyverse, que es una colección de paquetes de R diseñados para el análisis de datos, como ggplot2, dplyr, tidyr, entre otros.\n\nLeer el archivo CSV:\ndf_aula1 &lt;- read.csv2(\"excel-R/aula1.csv\", fileEncoding = 'latin1')\n\ndf_aula1 &lt;- read.csv2(\"excel-R/aula1.csv\", fileEncoding = 'latin1'): Lee el archivo CSV denominado aula1.csv y lo almacena en el dataframe df_aula1. La opción fileEncoding = 'latin1' asegura que los caracteres especiales se manejen correctamente durante la lectura del archivo.\n\nCrear y agrupar por intervalos:\ndf_aula1 |&gt;\n  mutate(intervalo = cut(altura_cm, breaks = seq(130, 180, by = 10), include.lowest = TRUE)) |&gt;\n  group_by(intervalo)\n\ndf_aula1 |&gt;: Utiliza el operador pipe (|&gt;) para encadenar una serie de operaciones en el dataframe df_aula1.\nmutate(intervalo = cut(altura_cm, breaks = seq(130, 180, by = 10), include.lowest = TRUE)): Crea una nueva columna intervalo que contiene intervalos de la columna altura_cm con un rango desde 130 hasta 180 cm, con una amplitud de clase de 10 cm. La opción include.lowest = TRUE asegura que el valor más bajo esté incluido en el primer intervalo.\ngroup_by(intervalo): Agrupa los datos por los intervalos definidos en la columna intervalo.\n\nCompletar los intervalos faltantes\n complete(intervalo, fill = list(frec_abs = 0))\n\nUsa la función complete() para asegurarse de que todos los intervalos aparezcan en la tabla, incluso si su frecuencia es cero.\nEl argumento fill = list(frec_abs = 0) asegura que los intervalos faltantes tengan una frecuencia absoluta de cero.\n\nCalcular la frecuencia absoluta:\nsummarise(frec_abs = n())\n\nsummarise(frec_abs = n()): Crea un nuevo dataframe resumido que contiene la frecuencia absoluta (frec_abs) de cada intervalo. La función n() cuenta el número de observaciones en cada intervalo.\n\nCalcular la frecuencia relativa:\nmutate(frec_rel = frec_abs / sum(frec_abs))\n\nmutate(frec_rel = frec_abs / sum(frec_abs)): Añade una nueva columna frec_rel al dataframe que contiene la frecuencia relativa. Esto se calcula dividiendo la frecuencia absoluta de cada intervalo por la suma total de todas las frecuencias absolutas.\n\n\nEl resultado es un dataframe que muestra los intervalos de altura_cm (columna intervalo), junto con las frecuencias absolutas (frec_abs) y las frecuencias relativas (frec_rel) para cada intervalo. Este proceso facilita el análisis de la distribución de los datos en intervalos específicos.\n\n\n\n\nlibrary (tidyverse)\ndf_aula1 &lt;- read.csv2(\"excel-R/aula1.csv\",fileEncoding='latin1')\n\ndf_aula1 |&gt;\n  mutate (intervalo = cut(altura_cm, breaks = seq(130, 180, by = 10), include_lowest = TRUE)) |&gt;\n  group_by(intervalo) |&gt;\n  summarise (frec_abs = n()) |&gt;\n  complete(intervalo, fill = list(frec_abs = 0)) |&gt;\n  mutate(frec_rel = frec_abs / sum(frec_abs))\n\n# A tibble: 5 × 3\n  intervalo frec_abs frec_rel\n  &lt;fct&gt;        &lt;int&gt;    &lt;dbl&gt;\n1 (130,140]        4   0.364 \n2 (140,150]        1   0.0909\n3 (150,160]        5   0.455 \n4 (160,170]        0   0     \n5 (170,180]        1   0.0909\n\n\n\n\n\n\n\n\nImportante: uso de los símbolos para designar los intervalos\n\n\n\nR utiliza los símbolos ( y [ para definir los intervalos, tal como se hace en matemáticas.\n\nIntervalo abierto: El símbolo ( se utiliza para denotar un intervalo abierto. El límite correspondiente no está incluuido en el intervalo.\nIntervalo cerrado o semiabierto:El símbolo [ se utiliza para denotar un intervalo cerrado o semiabierto. EL límite correspondiente sí está incluido en el intervalo.\n\nEjemplos:\n\n\\((a, b)\\) representa todos los números reales mayores que \\(a\\) y menores que \\(b\\) (excluye los valores \\(a\\) y \\(b\\)).\n\\([a, b]\\) representa todos los números reales mayores o iguales que \\(a\\) y menores o iguales que \\(b\\) (incluye \\(a\\) y \\(b\\)).\n\\([a, b)\\) representa todos los números reales mayores o iguales que \\(a\\) y menores que \\(b\\) (incluye \\(a\\), pero excluye \\(b\\))\n\\((a, b]\\) representa todos los números reales mayores que \\(a\\) y menores o iguales que \\(b\\) (excluye \\(a\\), pero incluye \\(b\\)).\n\n\n\nSi comparamos los dos métodos que hemos utilizado para construir la tabla de frecuencias, vemos que:\n\nen Excel los pasos que hemos dado no están registrados y a la vista, y, por lo tanto, no son fácilmente revisables\nen R, todos los pasos y opciones que hemos utilizado están a la vista en el código del script\n\nSi otra persona quisiera modificar la tabla, le sería fácil editar el código R y relanzar el script, mientras que en Excel no sería fácil asegurarse de todos y cada uno de los pasos y clicks de ratón que hemos dado para construir y formatear la tabla.\nEs el caso, por ejemplo, de que enviásemos la tabla a otra persona y ésta tuviese que editarla en nuestra ausencia. En Excel, tendríamos que enviar a esa persona una explicación con las instrucciones oportunas; en cambio, en R, una vez que se comprende el código, no hacen falta más explicaciones adicionales. Incluso sin una comprensión total del código, se podría duplicar exactamente la tabla copiando y ejecutando el código.\nEsta es una de las principales razones de la conveniencia del aprendizaje de R incluso para las actividades más sencillas.\n\n\n\n\n\n\nEjercicio propuesto\n\n\n\nEn la tabla de frecuencias anterior, calcular frecuencia absoluta acumulada y frecuencia relativa acumulada en Excel y en R, e incluirlas en la tabla como dos columnas adicionales.\n\n\n\n\n\n\n\n\nRespuesta al ejercicio propuesto (en R)\n\n\n\n\n\nPara añadir las columnas de frecuencias absolutas acumuladas (frec_abs_acum) y frecuencias relativas acumuladas (frec_rel_acum) a tu dataframe df_aula1, puedes usar las funciones cumsum() y algunas mutaciones adicionales. Aquí tienes el código actualizado:\nCódigo R actualizado\n## Cargar las librerías necesarias\nlibrary(tidyverse)\n\n## Crear el dataframe con las nuevas columnas de frecuencias acumuladas\ndf_aula1 |&gt;\n  mutate(intervalo = cut(altura_cm, breaks = seq(130, 180, by = 10), include_lowest = TRUE)) |&gt;\n  group_by(intervalo) |&gt;\n  summarise(frec_abs = n()) |&gt;\n  complete(intervalo, fill = list(frec_abs = 0)) |&gt;\n  mutate(frec_rel = frec_abs / sum(frec_abs),\n         frec_abs_acum = cumsum(frec_abs),\n         frec_rel_acum = cumsum(frec_rel)) |&gt;\nDetalle del código\n\nCrear intervalos:\nmutate(intervalo = cut(altura_cm, breaks = seq(130, 180, by = 10), include_lowest = TRUE))\n\nAgrupa los valores de altura_cm en intervalos definidos por seq(130, 180, by = 10).\n\nAgrupar por intervalo y calcular frecuencia absoluta:\ngroup_by(intervalo) |&gt;\nsummarise(frec_abs = n())\n\nAgrupa por intervalo y calcula la frecuencia absoluta (frec_abs) de cada intervalo.\n\nCalcular frecuencia relativa:\nmutate(frec_rel = frec_abs / sum(frec_abs))\n\nCalcula la frecuencia relativa (frec_rel) dividiendo la frecuencia absoluta de cada intervalo por la suma total de frecuencias absolutas.\n\nCompletar los intervalos faltantes:\n complete(intervalo, fill = list(frec_abs = 0))\n\nUsa la función complete() para asegurarse de que todos los intervalos aparezcan en la tabla, incluso si su frecuencia es cero.\nEl argumento fill = list(frec_abs = 0) asegura que los intervalos faltantes tengan una frecuencia absoluta de cero.\n\nCalcular frecuencia absoluta acumulada:\nmutate(frec_abs_acum = cumsum(frec_abs))\n\nUsa cumsum() para calcular la frecuencia absoluta acumulada (frec_abs_acum).\n\nCalcular frecuencia relativa acumulada:\nmutate(frec_rel_acum = cumsum(frec_rel))\n\nUsa cumsum() para calcular la frecuencia relativa acumulada (frec_rel_acum).\n\n\nResultado:\nEl dataframe resultante tendrá las siguientes columnas:\n\nintervalo: Intervalos definidos de altura_cm.\nfrec_abs: Frecuencia absoluta de cada intervalo.\nfrec_rel: Frecuencia relativa de cada intervalo.\nfrec_abs_acum: Frecuencia absoluta acumulada.\nfrec_rel_acum: Frecuencia relativa acumulada.\n\n\n\n\nLas distribuciones de frecuencias se pueden visualizar mediante varios tipos de gráficos, como histogramas, gráficos de barras y polígonos de frecuencias.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#diagrama-de-barras",
    "href": "040-exploracion.html#diagrama-de-barras",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.4 Diagrama de barras",
    "text": "4.4 Diagrama de barras\nCuando nuestra variable es discreta, podemos representar las frecuencias de cada valor de forma gráfica utilizando un diagrama de barras. Este diagrama utiliza barras rectangulares para representar la frecuencia de cada categoría.\nEste gráfico es muy utilizado para representar, por ejemplo, resultados de encuestas, como el número de votos que han obtenido los diferentes partidos políticos en unas elecciones, o tablas discretas, como los kilos fabricados por meses en una fábrica.\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nEjemplos de diagramas de barras",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#histograma",
    "href": "040-exploracion.html#histograma",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.5 Histograma",
    "text": "4.5 Histograma\nPara visualizar las variables continuas se utiliza el histograma, que es un diagrama que utiliza las barras rectangulares para hacer un gráfico de la distribución de valores continuos, previamente agrupados en intervalos (bins en inglés), tal como se ha hecho en la tabla de frecuencias.\n\nComponentes de un histograma\n\nEje horizontal (X): Representa los intervalos de valores de la variable. Cada intervalo abarca un rango específico de valores.\nEje vertical (Y): Muestra la frecuencia o el número de veces que los valores caen dentro de cada intervalo.\nBarras: Cada barra en el histograma representa un intervalo. La altura de la barra indica la frecuencia de los valores dentro de ese intervalo.\n\n\n\nCómo interpretar un histograma\n\nSi las barras son altas en un intervalo específico, significa que muchos valores del conjunto de datos caen dentro de ese rango.\nUn histograma puede ayudar a identificar patrones, como si los datos están distribuidos de manera simétrica, sesgada, o si hay picos y valles significativos.\n\n\n\nConstrucción de un histograma en Excel\nUsaremos el ejemplo de la altura en cm. de un grupo de alumnos, cuyos datos hemos guardado en el fichero csv aula1.csv.\nPodemos utilizar dos métodos para hacer un histograma en Excel\n\nMétodo 1: histograma directo\n\nSeleccionamos el rango de datos. Podemos hacerlo mediante un click en el encabezado de la columna de la variable altura_cm, en este caso, la columna B.\nEn la opción de menú Insertar, seleccionamos el icono Seleccionar gráficos de estadística\nSeleccionamos Histograma\n\n\n\n\n\n\nExcel calcula automáticamente la amplitud de los intervalos y el número de columnas; estas opciones pueden modificarse seleccionando con el botón derecho del ratón el elemento a modificar. En este caso, utilizamos estas opciones:\n\nEliminamos título del gráfico\nModificamos anchura de las barras seleccionando Dar formato a serie de datos, Opciones de serie, Ancho del rango= 50%\nModificamos los intervalos seleccionando el eje X: Dar formato al eje, Opciones de eje, Ancho del rango = 10\n\nLa descripción de los intervalos utiliza los mismos símbolos que hemos visto en las tablas de frecuencias de R.\nEn el momento de escribir este manual, Excel no permite hacer histogramas múltiples ni agrupados por otra variable, por lo que para diseños más complejos, no hay más remedio que recurrir a otras opciones como las tablas dinámicas de Excel o, mejor aún, R.\n\n\nMétodo 2: utilizar una tabla dinámica\nLa tabla dinámica que hemos construido en Excel ha convertido nuestra variable continua, altura_cm, en una tabla de valores discretos, al agrupar los valores en intervalos. En Excel podemos representar las frecuencias absolutas de nuestra tabla gráficamente, insertando un gráfico de barras a partir de la tabla:\n\nCon el cursor dentro de la tabla, seleccionamos la opción de menú Insertar\nInsertamos un gráfico de barras\n\nOpcionalmente, aplicamos las siguientes opciones de formato:\n\nHacemos click sobre uno de los botones del gráfico dinámico con el boton derecho del ratón, y seleccionamos la opción ocultar todos los botones del gráfico.\nEliminamos el título del gráfico\nAbrimos el formato de la serie de datos, e introducimos en la opción Ancho del rango el valor \\(50\\%\\) para ensanchar las barras.\n\n\n\n\n\n\nAl utiizar la tabla dinámica para construir el gráfico, Excel utiliza las categorías de la tabla dinámica. Dado que estas categorías (los intervalos que ha formado la tabla dinámica) son discretas, Excel utiliza el resultado de la tabla dinámica para hacer el gráfico con un diagrama de barras. No es posible insertar un histograma a partir de una tabla dinámica.\n\n\n\nAnálisis de un caso real\nEl histograma muestra su utilidad cuando representamos la distribución de un conjunto de valores más grande que nuestros once alumnos. Veamos su aplicación a los datos diarios de una fabricación de queso camembert a lo largo de un año.Los datos de esta fabricación están en el fichero camembert.csv.\nLa tabla de datos tiene esta estructura:\n\n\n\n\n\nLa tabla está formada por 211 casos.\nVisualicemos la tabla con R:\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nCargar las librerías necesarias:\nlibrary(DT)\n\nlibrary(DT): Carga el paquete DT para crear tablas interactivas.\n\nLeer el archivo CSV:\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1')\nMostrar la tabla interactiva:\ndatatable(df_camembert, options = list(pageLength = 5), caption = 'Tabla de Datos de Camembert')\n\ndatatable(df_camembert, options = list(pageLength = 5), caption = 'Tabla de Datos de Camembert'): Crea una tabla interactiva con df_camembert, mostrando cinco registros por página (pageLength = 5). La tabla tiene una opción para desplazarse por el resto de los datos, buscar, y ordenar.\n\n\nResultado\nEste código generará una tabla interactiva donde podrás visualizar cinco registros a la vez y desplazarte por el resto de los datos de manera intuitiva. La función datatable() es excelente para manejar grandes conjuntos de datos y ofrecer una experiencia de usuario dinámica.\n\n\n\n\nlibrary (DT)\n\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\",fileEncoding='latin1')\n\n# Mostrar la tabla interactiva con datatable \ndatatable(df_camembert, options = list(pageLength = 5), caption = 'Tabla de Datos de Camembert')\n\n\n\n\n\n\nConstrucción del histograma con Excel\n\nMétodo directo\nSeleccionamos el rango del extracto seco (columna est) e insertamos el histograma, ajustando el ancho de intervalo a \\(1\\) en las opciones de gráfico:\n\n\n\n\n\n\n\nMétodo 2: mediante una tabla dinámica\nLos pasos a seguir son:\n\nConstruir la tabla dinámica\nAgrupar los datos\nInsertar el gráfico a partir de la tabla\n\nCon una agrupación de datos en intervalos de 1, esta es nuestra tabla dinámica:\n\nEn la figura siguiente vemos el histograma correspondiente a la tabla anterior, con un intervalo de clase de 1 punto de extracto seco total, y otras dos alternativas si el intervalo de clase fuese de 2 puntos o de de 0,5 puntos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa decisión de cambiar la amplitud del intervalo en un histograma influye en el aspecto del gráfico y es una opción personal; lo mejor es utilizar la que en nuestra opinión refleje mejor el aspecto de la distribución de datos, ni demasiado grande ni demasiado pequeña. En todo caso, debemos ser capaces de interpretar que la distribución de los valores es la misma en los tres casos: hay una mayoría de casos con valores entre 46 y 48, y muy pocos casos con valores muy bajos o muy altos. En este caso, la distribución de los valores es aproximadamente simétrica, y se reparten alrededor de una mayoría de valores centrales.\n\n\n\nHistograma en R en dos etapas\nComo hemos visto en Excel, la construcción de un histograma implica siempre dos etapas:\n\nagrupación de los datos en una tabla de frecuencia (tabla dinámica), y\ncreación del gráfico a partir de esta tabla.\n\nAunque R dispone de funciones que realizan automáticamente estas etapas, y que veremos más tarde, de forma didáctica vamos a construir por separado la tabla de frecuencias y el histograma de la variable esten R, siguiendo los pasos básicos que hemos dado en Excel. Nuestro script nos mostrará la tabla de frecuencias y, a continuación, el histograma.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\nCargar la biblioteca tidyverse\nlibrary(tidyverse)\n\nDescripción: Carga el paquete tidyverse, que es una colección de paquetes R diseñada para la manipulación y visualización de datos.\n\nPaquetes incluidos: Entre otros, incluye ggplot2 (para visualización), dplyr (para manipulación de datos), tidyr (para ordenar datos) y readr (para leer datos).\n\n\n\nLeer el archivo CSV\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1')\n\nDescripción: Lee un archivo CSV (camembert.csv) y lo almacena en el dataframe df_camembert.\nOpciones:\n\nfileEncoding = 'latin1': Especifica la codificación del archivo para asegurar que los caracteres se lean correctamente. latin1 es adecuado para idiomas europeos occidentales.\n\n\n\nCrear la tabla de frecuencias\ndf_camembert |&gt;\n  mutate(intervalo = cut(est, breaks = seq(40, 54, by = 1), include_lowest = TRUE)) |&gt;\n  group_by(intervalo) |&gt;\n  summarise(frec_abs = n()) |&gt;\n  mutate(frec_rel = frec_abs / sum(frec_abs)) |&gt;\n  print() |&gt;\n  ggplot(aes(x = intervalo, y = frec_abs)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", alpha = 0.7, color = \"black\") +\n  labs(title = \"Diagrama de Barras de Frecuencia absoluta\",\n       x = \"Intervalos de 'est'\",\n       y = \"Frecuencia Absoluta\") +\n  theme_minimal()\n1. mutate()\nmutate(intervalo = cut(est, breaks = seq(40, 54, by = 1), include_lowest = TRUE))\n\nDescripción: Añade una nueva columna intervalo al dataframe.\ncut: Corta la variable est en intervalos especificados.\n\nbreaks = seq(40, 54, by = 1): Define los puntos de corte desde 40 hasta 54, incrementando de 1 en 1.\ninclude_lowest = TRUE: Incluye el valor más bajo en el primer intervalo.\n\n\n2. group_by()\ngroup_by(intervalo)\n\nDescripción: Agrupa los datos por los intervalos creados en el paso anterior.\n\n3. summarise()\nsummarise(frec_abs = n())\n\nDescripción: Calcula la frecuencia absoluta (frec_abs) de cada intervalo.\nn(): Cuenta el número de elementos en cada grupo.\n\n4. mutate()\nmutate(frec_rel = frec_abs / sum(frec_abs))\n\nDescripción: Añade una nueva columna frec_rel al dataframe.\n\nfrec_rel = frec_abs / sum(frec_abs): Calcula la frecuencia relativa dividiendo cada frecuencia absoluta por la suma total de frecuencias absolutas.\n\n\n5. print()\nprint()\n\nDescripción: Imprime el dataframe resultante que contiene las frecuencias absolutas y relativas.\n\n\nCrear el diagrama de barras\nggplot(aes(x = intervalo, y = frec_abs)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", alpha = 0.7, color = \"black\") +\n  labs(title = \"Diagrama de Barras de Frecuencia absoluta\",\n       x = \"Intervalos de 'est'\",\n       y = \"Frecuencia Absoluta\") +\n  theme_minimal()\n1. ggplot()\nggplot(aes(x = intervalo, y = frec_abs))\n\nDescripción: Configura ggplot2 para usar intervalo en el eje X y frec_abs en el eje Y.\naes: Define las estéticas del gráfico, es decir, qué variables van en qué ejes.\n\n2. geom_bar()\ngeom_bar(stat = \"identity\", fill = \"lightblue\", alpha = 0.7, color = \"black\")\n\nDescripción: Crea un diagrama de barras utilizando los valores proporcionados.\nOpciones:\n\nstat = \"identity\": Usa los valores proporcionados para la altura de las barras, en lugar de calcularlos automáticamente.\nfill = \"lightblue\": Establece el color de relleno de las barras a azul claro.\nalpha = 0.7: Define la transparencia de las barras (0.7 significa 70% opaco).\ncolor = \"black\": Establece el color del borde de las barras a negro.\n\n\n3. labs()\nlabs(title = \"Diagrama de Barras de Frecuencia absoluta\",\n     x = \"Intervalos de 'est'\",\n     y = \"Frecuencia Absoluta\")\n\nDescripción: Añade títulos y etiquetas a los ejes del gráfico.\nOpciones:\n\ntitle: Título del gráfico.\nx: Etiqueta del eje X.\ny: Etiqueta del eje Y.\n\n\n4. theme_minimal()\ntheme_minimal()\n\nDescripción: Aplica un tema minimalista al gráfico, eliminando elementos innecesarios y enfocándose en la claridad del gráfico.\n\n\n\n\n\n\nlibrary (tidyverse)\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\",fileEncoding='latin1')\n\n# Crear tabla de frecuencias\ndf_camembert |&gt;\n  mutate (intervalo = cut(est, breaks = seq(40, 54, by =1), include_lowest = TRUE)) |&gt;\n  group_by(intervalo) |&gt;\n  summarise (frec_abs = n()) |&gt;\n  mutate(frec_rel = frec_abs / sum(frec_abs)) |&gt;\n  print() |&gt;\n  ggplot(aes(x = intervalo, y = frec_abs)) +\n   geom_bar(stat = \"identity\", fill = \"lightblue\", alpha = 0.7, color = \"black\") +\n   labs(title = \"Diagrama de Barras de Frecuencia absoluta\",\n       x = \"Intervalos de 'est'\",\n       y = \"Frecuencia Absoluta\") +\n   theme_minimal()\n\n# A tibble: 10 × 3\n   intervalo frec_abs frec_rel\n   &lt;fct&gt;        &lt;int&gt;    &lt;dbl&gt;\n 1 (42,43]          1  0.00474\n 2 (43,44]          4  0.0190 \n 3 (44,45]         23  0.109  \n 4 (45,46]         48  0.227  \n 5 (46,47]         56  0.265  \n 6 (47,48]         49  0.232  \n 7 (48,49]         17  0.0806 \n 8 (49,50]         10  0.0474 \n 9 (50,51]          2  0.00948\n10 (52,53]          1  0.00474\n\n\n\n\n\n\n\n\n\nModificando el criterio de la amplitud de clase podemos cambiar la tabla, igual que hemos hecho en Excel. Por ejemplo, cambiamos la opcion de agrupación en mutateindicando que los intervalos sean de amplitud 2 en vez de 1: breaks = seq(40, 54, by =2).\n\nlibrary (tidyverse)\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\",fileEncoding='latin1')\ndf_camembert |&gt;\n#--------------------------------------------------------------------------------------\n  mutate (intervalo = cut(est, breaks = seq(40, 54, by =2), include_lowest = TRUE)) |&gt;\n#--------------------------------------------------------------------------------------\n  group_by(intervalo) |&gt;\n  summarise (frec_abs = n()) |&gt;\n  mutate(frec_rel = frec_abs / sum(frec_abs)) |&gt;\n  print() |&gt;\n  ggplot(aes(x = intervalo, y = frec_abs)) +\n   geom_bar(stat = \"identity\", fill = \"lightblue\", alpha = 0.7, color = \"black\") +\n   labs(title = \"Diagrama de Barras de Frecuencia absoluta\",\n       x = \"Intervalos de 'est'\",\n       y = \"Frecuencia absoluta (núm. de casos)\") +\n   theme_minimal()\n\n# A tibble: 6 × 3\n  intervalo frec_abs frec_rel\n  &lt;fct&gt;        &lt;int&gt;    &lt;dbl&gt;\n1 (42,44]          5  0.0237 \n2 (44,46]         71  0.336  \n3 (46,48]        105  0.498  \n4 (48,50]         27  0.128  \n5 (50,52]          2  0.00948\n6 (52,54]          1  0.00474\n\n\n\n\n\n\n\n\n\n\n\nCreación del histograma en R directamente\nLas funciones de histograma de R permiten construir el histograma directamente sin necesidad de hacer previamente una tabla de frecuencias (en realidad, la tabla de frecuencias se calcula internamente). Es mucho más sencillo utilizar estas funciones, ya que el código se simplifica mucho.\nComo siempre, presentamos la opción básica de R junto con la opción de la función ggplot() de tidyverse, y utilizamos las opciones de personalización de ggplot()para trazar un histograma de aspecto semejante al gráfico básico; en esta ocasión, utilizamos el color de relleno gris.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\nPrimer Fragmento de Código: hist(df_camembert$est)\n\nCrear un Histograma Base:\nhist(df_camembert$est)\n\nhist(df_camembert$est): Esta línea de código crea un histograma básico de la variable est del dataframe df_camembert utilizando la función hist() de base R.\n\ndf_camembert$est: Especifica la columna est del dataframe df_camembert para la cual se generará el histograma.\n\nLa función hist() por defecto elige la cantidad y el ancho de las barras basándose en los datos proporcionados.\n\n\nSegundo Fragmento de Código: ggplot y geom_histogram()\n\nUtilizar ggplot2 para Crear un Histograma Personalizado:\ndf_camembert |&gt;\n  ggplot(aes(x = est)) +\n  geom_histogram(binwidth = 1, fill = \"grey\", alpha = 0.7, color = \"black\") +\n  theme_minimal()\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las Estéticas (aes):\nggplot(aes(x = est)) +\n\nggplot(aes(x = est)): Inicia un objeto ggplot y define las estéticas (aes), mapeando la variable est al eje X.\n\nAgregar las Barras del Histograma:\ngeom_histogram(binwidth = 1, fill = \"grey\", alpha = 0.7, color = \"black\") +\n\ngeom_histogram(binwidth = 1, fill = \"grey\", alpha = 0.7, color = \"black\"): Añade una capa de barras al histograma.\n\nbinwidth = 1: Define la amplitud de clase en 1 unidad.\nfill = \"grey\": Colorea las barras del histograma de gris.\nalpha = 0.7: Establece la transparencia de las barras (valor entre 0 y 1, donde 0 es completamente transparente y 1 es opaco).\ncolor = \"black\": Colorea los bordes de las barras en negro.\n\n\nAplicar un Tema Minimalista:\ntheme_minimal()\n\ntheme_minimal(): Aplica un tema minimalista al gráfico, proporcionando un diseño limpio y simple, sin distracciones innecesarias.\n\n\nResumen completo del código\n\nPrimer Fragmento:\nhist(df_camembert$est)\ncrea un histograma básico de la variable est usando la función base hist() de R.\nSegundo Fragmento:\ndf_camembert |&gt;\n  ggplot(aes(x = est)) +\n  geom_histogram(binwidth = 1, fill = \"darkblue\", alpha = 0.7, color = \"black\") +\n  theme_minimal()\nEste conjunto de instrucciones crea un histograma más personalizable y estilizado usando ggplot2, con opciones para el ancho de las barras, color de relleno, transparencia y estilo del tema.\n\n\n\n\nhist(df_camembert$est)\n\ndf_camembert |&gt;\n  ggplot(aes(x=est)) +\n    geom_histogram(binwidth=1, fill = \"grey\", alpha = 0.7, color = \"black\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(a) Función básica de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Función ggplot()\n\n\n\n\n\n\n\nFigura 4.1: Ejemplos de histogramas\n\n\n\nVemos que los dos gráficos no son idénticos a pesar de provenir de los mismos datos, porque la construcción de los intervalos subyacente es ligeramente diferente. Esto no debe preocuparnos, porque el aspecto general de la distribución de los datos es el mismo.\nEn la fase de exploración nos importa más entender estas propiedades de los datos (aspecto, forma de la distribución) que la precisión en la construcción del gráfico. En todo caso, en qqplot()podemos usar infinidad de opciones que nos permiten configurar el gráfico hasta los más mínimos detalles, por lo que esta función es mucho más útil que las funciones básicas para preparar gráficos que vayan destinados a informes o presentaciones. La función básica hist(), por su parte, es más sencilla de usar al principio cuando no tenemos todavía experiencia con ggplot(), y también para una visualización rápida.\n\n\n\nOtros ejemplos\nEn ocasiones, nos encontramos con datos que son asimétricos: hay una mayoría de valores bajos o bien de valores altos. Veamos un caso: los recuentos bacterianos de bacterias coliformes, que tenemos en la última columna a la derecha de nuestra tabla, en la variable ´coliformes´.\n\nEn este caso, vemos que la mayoría de los casos tienen valor cero. Es el caso de los recuentos de bacterias contaminantes, en el que la mayoría de los análisis tienen recuentos cero o muy bajos, y sólo en pocos casos tienen valores altos. Veremos con más detalle cómo tratar estas distribuciones cuando hablemos de las distribuciones de probabilidad, en capítulos posteriores.\nEn los histogramas de los recuentos bacterianos,utilizamos una opción para aumentar el número de barras que queremos en el histograma: breaks= en la función base, bins= en ggplot():\nhist(df_camembert$coliformes, breaks=50)\n\ndf_camembert |&gt;\n  ggplot(aes(x=coliformes)) +\n    geom_histogram(bins=50, fill = \"lightblue\", colour=\"black\")+\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n(a) Función básica de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Función ggplot()\n\n\n\n\n\n\n\nFigura 4.2: Ejemplos de histogramas\n\n\n\nEn casos de distribuciones muy asimétricas, a veces es conveniente aplicar una transformación a los datos, tal como el logaritmo decimal, mediante la función log10(); esto facilita la interpretación del gráfico:\n\ndf_camembert |&gt;\n  ggplot(aes(x=log10(coliformes+1))) +\n    geom_histogram(bins=50, fill = \"lightblue\", colour=\"black\")+\n    theme_bw()",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-densidad",
    "href": "040-exploracion.html#gráficos-de-densidad",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.6 Gráficos de densidad",
    "text": "4.6 Gráficos de densidad\nUn gráfico de densidad en R es una representación visual suavizada de la distribución de un conjunto de datos. A diferencia de los histogramas, que dividen los datos en intervalos y cuentan las frecuencias, los gráficos de densidad utilizan técnicas estadísticas no paramétricas para estimar la función de densidad de probabilidad.\nUn gráfico de densidad, también conocido como density plot, es una representación visual suavizada de la distribución de un conjunto de datos.A diferencia de los histogramas, que dividen los datos en intervalos y cuentan las frecuencias, los gráficos de densidad utilizan técnicas estadísticas no paramétricas para estimar la función de densidad de probabilidad de una variable continua. El gráfico de densidad utiliza suavizamiento para proporcionar una estimación más continua de la distribución de los datos.\nExcel no permite la representación de los gráficos de densidad; en R pueden hacerse con la función ggplot()simplemente añadiendo la geometría geom_density()\ndf_camembert |&gt;\n  ggplot(aes(x=est)) +\n    geom_density()\n\ndf_camembert |&gt;\n  ggplot(aes(x=log(coliformes+1))) +\n    geom_density()\n\n\n\n\n\n\n\n\n\n\n\n(a) EST\n\n\n\n\n\n\n\n\n\n\n\n(b) coliformes\n\n\n\n\n\n\n\nFigura 4.3: Ejemplos de diagramas de densidad\n\n\n\nPodemos representar simultáneamente el histograma y la función de densidad; hay que tener en cuenta que para representar la densidad y el histograma superpuestos, nos vemos obligados a cambiar la escala del eje Y a los valores de densidad en vez de a las frecuencias, de manera que los dos gráficos puedan solaparse sin problemas.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nIniciar el pipeline con df_camembert:\ndf_camembert |&gt;\n\nUtiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = est)) +\n\nggplot(aes(x = est)): Inicia un objeto ggplot y define las estéticas (aes). La variable est se mapea al eje X.\n\nAgregar la capa del histograma:\ngeom_histogram(aes(y = ..density..), bins = 20, fill = \"lightblue\", color = \"black\") +\n\ngeom_histogram(aes(y = ..density..), bins = 20, fill = \"lightblue\", color = \"black\"):\n\naes(y = ..density..): Ajusta el eje Y para que muestre la densidad en lugar de la frecuencia absoluta.\nbins = 20: Especifica el número de barras (bins) en el histograma.\nfill = \"lightblue\": Colorea las barras del histograma de azul claro.\ncolor = \"black\": Colorea los bordes de las barras en negro.\n\n\nAgregar la capa de densidad:\ngeom_density(color = \"red\", size = 1) +\n\ngeom_density(color = \"red\", size = 1): Añade una capa de densidad al gráfico.\n\ncolor = \"red\": Colorea la línea de densidad en rojo.\nsize = 1: Establece el grosor de la línea de densidad.\n\n\nAgregar etiquetas y título:\nlabs(title = \"Histograma y densidad\", x = \"Valores\", y = \"Densidad\")\n\nlabs(title = \"Histograma y densidad\", x = \"Valores\", y = \"Densidad\"): Añade etiquetas y título al gráfico.\n\ntitle = \"Histograma y densidad\": Establece el título del gráfico.\nx = \"Valores\": Etiqueta el eje X como “Valores”.\ny = \"Densidad\": Etiqueta el eje Y como “Densidad”.\n\n\n\nResumen Completo\nEl código realiza las siguientes acciones:\n\nInicia el pipeline con el dataframe df_camembert.\nDefine la estética (aes) para mapear la variable est al eje X.\nAñade un histograma con barras azules claras, bordes negros, 20 bins y ajusta el eje Y para mostrar la densidad.\nAñade una capa de densidad con una línea roja y grosor de 1.\nAñade un título y etiquetas a los ejes X y Y.\n\nEste enfoque combinado permite visualizar tanto la distribución de los datos (histograma) como la densidad estimada (línea de densidad) en un solo gráfico.\n\n\n\n\ndf_camembert |&gt;\n  ggplot(aes(x=est)) +\n    geom_histogram(aes(y = ..density..), bins = 20, fill = \"lightblue\", color = \"black\") +\n    geom_density(color = \"red\", size = 1) +\n    labs(title = \"Histograma y densidad\", x = \"Valores\", y = \"Densidad\")\n\n\n\n\n\n\n\n\nLa ventaja de los gráficos de densidad es que como no tenemos que fraccionar los datos en intervalos arbitrarios, no estamos afectados por el efecto visual de la anchura de estos intervalos. También hay otras ventajas desde el punto de vista estadístico, que veremos al hablar de las distribuciones de probabilidad.\nLa flexibilidad de configuración de los gráficos de qqplot()permite la personalización de los gráficos hasta el último detalle. A continuación vemos un gráfico de densidad agrupado por meses, que nos muestra aparentes diferencias en el extracto seco est según los meses.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n1. Leer el archivo CSV\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1')\n\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1'): Esta línea lee el archivo CSV llamado camembert.csv desde la carpeta excel-R y lo almacena en el dataframe df_camembert. La opción fileEncoding = 'latin1' asegura que los caracteres especiales sean manejados correctamente durante la lectura del archivo.\n\n2. Iniciar el pipeline con el dataframe df_camembert y realizar transformaciones\ndf_camembert |&gt; \n  mutate(fecha = as.Date(fecha, format = \"%d/%m/%Y\"), # Convertir a tipo Date \n  mes = format(fecha, \"%m\")) |&gt; # Extraer el mes\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\nmutate(fecha = as.Date(fecha, format = \"%d/%m/%Y\"), mes = format(fecha, \"%m\")):\n\nfecha = as.Date(fecha, format = \"%d/%m/%Y\"): Convierte la columna fecha a tipo Date usando el formato día/mes/año.\nmes = format(fecha, \"%m\"): Extrae el mes de la columna fecha y lo almacena en una nueva columna mes.\n\n\n3. Crear el gráfico de densidad\nggplot(aes(x = est, color = mes, fill = mes)) + # Crear el gráfico de densidad\n  geom_density(alpha = 0.5) + # Añadir capa de densidad con transparencia\n  labs(title = \"Curvas de Densidad de 'est' por Mes\", \n       x = \"Valor de 'est'\", \n       y = \"Densidad\", \n       color = \"Mes\", \n       fill = \"Mes\") + \n  theme_minimal()\n\nggplot(aes(x = est, color = mes, fill = mes)): Inicia un objeto ggplot y define las estéticas (aes):\n\nx = est: Mapea la variable est al eje X.\ncolor = mes, fill = mes: Usa la variable mes para colorear y rellenar las curvas de densidad, permitiendo diferenciar los meses.\n\ngeom_density(alpha = 0.5): Añade una capa de densidad al gráfico:\n\nalpha = 0.5: Establece la transparencia de las curvas de densidad al 50%, lo que permite ver superposiciones.\n\nlabs(title = \"Curvas de Densidad de 'est' por Mes\", x = \"Valor de 'est'\", y = \"Densidad\", color = \"Mes\", fill = \"Mes\"): Añade etiquetas y título al gráfico:\n\ntitle = \"Curvas de Densidad de 'est' por Mes\": Título del gráfico.\nx = \"Valor de 'est'\": Etiqueta del eje X.\ny = \"Densidad\": Etiqueta del eje Y.\ncolor = \"Mes\", fill = \"Mes\": Etiquetas para la leyenda de colores y rellenos.\n\ntheme_minimal(): Aplica un tema minimalista al gráfico para un diseño limpio y simple.\n\nResumen\nEste código realiza las siguientes acciones: 1. Lee un archivo CSV y lo almacena en un dataframe. 2. Convierte la columna fecha a tipo Date y extrae el mes de la fecha. 3. Crea un gráfico de densidad de la variable est diferenciada por mes con transparencia para facilitar la visualización de superposiciones y aplica un diseño minimalista al gráfico.\n\n\n\n\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1') \ndf_camembert |&gt; \n  mutate(fecha = as.Date(fecha, format = \"%d/%m/%Y\"), # Convertir a tipo Date \n  mes = format(fecha, \"%m\")) |&gt; # Extraer el mes \n  ggplot(aes(x = est, color = mes, fill = mes)) + # Crear el gráfico de densidad \n    geom_density(alpha = 0.5) + # Añadir capa de densidad con transparencia \n    labs(title = \"Curvas de Densidad de 'est' por Mes\", x = \"Valor de 'est'\", y = \"Densidad\", color = \"Mes\", fill = \"Mes\")+    \n    theme_minimal()",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#diagrama-de-caja-o-boxplot",
    "href": "040-exploracion.html#diagrama-de-caja-o-boxplot",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.7 Diagrama de caja o boxplot",
    "text": "4.7 Diagrama de caja o boxplot\nEste gráfico fue creado por el estadístico John Tukey en 1977, y es una herramienta fundamental en la exploración de datos. Se basa en un grupo de medidas que se utiliza ampliamente en la descripción de conjuntos de datos, el conjunto de cuartiles. Si dividimos un grupo de datos ordenados en cuatro partes iguales, mediante tres puntos de corte, llamamos primer cuartil o \\(Q1\\) al valor que se situa en el 25%; segundo cuartil, o \\(Q2\\), al valor que se sitúa en el centro (50%), y tercer cuartil, o \\(Q3\\), al punto que se situa en el 75% de los datos. A estos tres valores añadimos el mínimo y el máximo, y tenemos un conjunto de cinco números que nos permiten describir la forma de la distribución de datos con cierta precisión. El segundo cuartil (\\(Q2\\)), que corresponde al 50% de los datos, se conoce habitualmente como mediana. El valor resultante de restar \\(Q3-Q1\\) es lo que se conoce como rango intercuartil o \\(IQR\\), y es una medida de la dispersión de la distribución de datos (mide la amplitud de la distribución).\nEl diagrama de caja, también conocido como boxplot, es un gráfico que permite resumir las características principales de un conjunto de datos utilizando estos cinco números, tal como se explica a continuación. Sus ventajas son:\n\nMuestra la mediana y los cuartiles (Q1 y Q3) de los datos.\nPermite identificar la simetría de la distribución: si la mediana no está en el centro, la distribución no es simétrica.\nPermite detectar posibles valores atípicos, representando los valores atípicos (outliers) que están lejos del resto de los datos (un valor es atípico si está más allá de (Q3 + 1.5 IQR) o (Q1 - 1.5 IQR).\n\nLa construcción de un diagrama de caja es como sigue:\n\nMicrosoft Excel no dispone de un diseño de gráficos de caja que sea práctico, por lo que recurriremos siempre a R para realizarlos.\nComo casi siempre, hay una función de base que dibuja un boxplot y también una geometría de ggplot()que lo hace: geom_boxplot(), con muchas más opciones de diseño y formato que la opción de base.\n\nboxplot(df_aula$altura_cm)\n\n\n\n\n\n\n\nFigura 4.4: Boxplot con los gráficos básicos de R\n\n\n\n\n\nVamos a repetir el gráfico para los datos de producción de queso camembert, usando ahora los gráficos básicos y los de ggplot():\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nLeer el archivo CSV:\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1')\n\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\", fileEncoding = 'latin1'): Lee el archivo CSV denominado camembert.csv y lo almacena en el dataframe df_camembert. La opción fileEncoding = 'latin1' asegura que los caracteres especiales se manejen correctamente durante la lectura del archivo.\n\nConvertir la columna fecha a tipo Date:\ndf_camembert$fecha &lt;- as.Date(df_camembert$fecha, format(\"%d/%m/%Y\"))\n\ndf_camembert$fecha &lt;- as.Date(df_camembert$fecha, format(\"%d/%m/%Y\")): Convierte la columna fecha en el dataframe df_camembert al tipo de dato Date usando el formato de fecha \"%d/%m/%Y\", que corresponde a día/mes/año.\n\nCrear un boxplot básico de la variable est:\nboxplot(df_camembert$est)\n\nboxplot(df_camembert$est): Crea un diagrama de caja y bigotes (boxplot) básico para la variable est del dataframe df_camembert. Este gráfico muestra la distribución de los datos, destacando la mediana, cuartiles y posibles valores atípicos.\n\nCrear un boxplot personalizado utilizando ggplot2:\ndf_camembert |&gt;\n  ggplot(aes(x = \"\", y = est)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = element_blank(),  # Oculta las etiquetas del eje x\n        axis.ticks.x = element_blank()) # Oculta las marcas del eje x\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = \"\", y = est)) +\n\nggplot(aes(x = \"\", y = est)): Inicia un objeto ggplot y define las estéticas (aes). La variable est se mapea al eje Y. No se especifica una variable en el eje X, por lo que se deja como una cadena vacía \"\".\n\nAgregar la capa del boxplot:\ngeom_boxplot() +\n\ngeom_boxplot(): Añade una capa de boxplot al gráfico, que muestra la distribución de la variable est.\n\nAplicar un tema en blanco y negro:\ntheme_bw() +\n\ntheme_bw(): Aplica un tema en blanco y negro al gráfico, proporcionando un diseño limpio y clásico.\n\nPersonalizar el tema:\ntheme(axis.text.x = element_blank(),  # Oculta las etiquetas del eje x\n      axis.ticks.x = element_blank()) # Oculta las marcas del eje x\n\ntheme(axis.text.x = element_blank(), axis.ticks.x = element_blank()): Personaliza el tema para ocultar las etiquetas del eje X y las marcas del eje X, dejando un gráfico más claro y limpio.\n\n\nEl código realiza las siguientes acciones:\n\nLee un archivo CSV y lo convierte en un dataframe.\nConvierte la columna fecha a tipo Date.\nCrea un boxplot básico de la variable est.\nUtiliza ggplot2 para crear un boxplot más personalizable de la variable est, aplicando un tema en blanco y negro y ocultando las etiquetas y marcas del eje X.\n\n\n\n\ndf_camembert &lt;- read.csv2(\"excel-R/camembert.csv\",fileEncoding='latin1')\ndf_camembert$fecha &lt;- as.Date(df_camembert$fecha, format(\"%d/%m/%Y\"))\n\nboxplot(df_camembert$est)\n\ndf_camembert |&gt;\n  ggplot(aes(x=\"\", y=est))+\n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = element_blank(),  # Oculta las etiquetas del eje x\n        axis.ticks.x = element_blank()) # Oculta las marcas del eje x\n\n\n\n\n\n\n\n\n\n\n\n(a) Boxplot básico de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Boxplot utilizando ggplot()\n\n\n\n\n\n\n\nFigura 4.5: Ejemplos de boxplot\n\n\n\nUn uso muy interesante del boxplot en R consiste en agrupar los boxplot de una variable en funcion de otra. En este caso, agrupamos el extracto seco por meses previa agrupación de la fecha. Esta agrupación puede hacerse tanto en los gráficos básicos de R como en ggplot():\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nCrear una nueva columna mes a partir de fecha:\ndf_camembert$mes &lt;- format(df_camembert$fecha, \"%m\")\n\ndf_camembert$mes &lt;- format(df_camembert$fecha, \"%m\"): Crea una nueva columna mes en el dataframe df_camembert. La función format se utiliza para convertir las fechas en la columna fecha al formato de mes (%m), extrayendo solo el mes de cada fecha.\n\nCrear un boxplot básico de est por mes:\nboxplot(df_camembert$est ~ df_camembert$mes)\n\nboxplot(df_camembert$est ~ df_camembert$mes): Crea un diagrama de caja y bigotes (boxplot) básico que muestra la distribución de la variable est para cada mes, donde df_camembert$est se grafica en el eje Y y df_camembert$mes en el eje X.\n\nUtilizar ggplot2 para crear un boxplot más personalizado:\ndf_camembert |&gt;\n  mutate(mes = format(fecha, \"%m\")) |&gt;\n  ggplot(aes(x = mes, y = est)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot de 'est' por Meses\",\n       x = \"Mes\",\n       y = \"Valor de est\") +\n  theme_minimal()\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nCrear y mutar la columna mes dentro del pipeline:\nmutate(mes = format(fecha, \"%m\")) |&gt;\n\nmutate(mes = format(fecha, \"%m\")): Añade una nueva columna mes al dataframe utilizando la función format para extraer el mes de la columna fecha.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = mes, y = est)) +\n\nggplot(aes(x = mes, y = est)): Inicia un objeto ggplot y define las estéticas (aes). La variable mes se mapea al eje X y la variable est al eje Y.\n\nAgregar la capa del boxplot:\ngeom_boxplot() +\n\ngeom_boxplot(): Añade una capa de boxplot al gráfico, mostrando la distribución de la variable est para cada mes.\n\nAgregar etiquetas y título:\nlabs(title = \"Boxplot de 'est' por Meses\",\n     x = \"Mes\",\n     y = \"Valor de est\") +\n\nlabs(title = \"Boxplot de 'est' por Meses\", x = \"Mes\", y = \"Valor de est\"): Añade etiquetas y título al gráfico.\n\ntitle = \"Boxplot de 'est' por Meses\": Establece el título del gráfico.\nx = \"Mes\": Etiqueta el eje X.\ny = \"Valor de est\": Etiqueta el eje Y.\n\n\nAplicar un tema minimalista:\ntheme_minimal()\n\ntheme_minimal(): Aplica un tema minimalista al gráfico, proporcionando un diseño limpio y simple, sin distracciones innecesarias.\n\n\nResumen\nEl código realiza las siguientes acciones:\n\nAñade una columna mes al dataframe df_camembert, que contiene solo el mes extraído de la columna fecha.\nCrea un boxplot básico que muestra la distribución de est por mes.\nUtiliza ggplot2 para crear un boxplot más personalizado de est por mes, con etiquetas y un tema minimalista para un diseño limpio.\n\n\n\n\ndf_camembert$mes &lt;- format(df_camembert$fecha, \"%m\")\nboxplot (df_camembert$est~df_camembert$mes)\n\ndf_camembert |&gt;\n  mutate (mes = format(fecha, \"%m\")) |&gt;\n  ggplot(aes(x = mes, y = est)) +\n    geom_boxplot() +\n    labs(title = \"Boxplot de 'est' por Meses\",\n       x = \"Mes\",\n       y = \"Valor de est\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(a) Boxplot básico de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Boxplot utilizando ggplot()\n\n\n\n\n\n\n\nFigura 4.6: Ejemplos de boxplot\n\n\n\nLa agrupación de los boxplots por meses nos pone claramente de manifiesto las diferencias en el extracto seco estque ya habíamos visto con los gráficos de densidad. Estas diferencias son más claras en el mes de julio.\n\nRelación entre el boxplot y el histograma\nResulta muy útil comprender visualmente la relación entre el boxplot y el histograma para entender la distribución de los datos. En la gráfica siguiente se representan ambos simultáneamente\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\n\nIniciar el pipeline con df_camembert:\ndf_camembert |&gt;\n\nUtiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = est)) +\n\nggplot(aes(x = est)): Inicia un objeto ggplot y define las estéticas (aes). La variable est se mapea al eje X.\n\nAgregar la capa del histograma:\ngeom_histogram(fill = \"lightblue\", color = \"black\", bins = 20, alpha = 0.7) +\n\ngeom_histogram(fill = \"lightblue\", color = \"black\", bins = 20, alpha = 0.7): Añade una capa de histograma al gráfico.\n\nfill = \"lightblue\": Colorea las barras del histograma de azul claro.\ncolor = \"black\": Colorea los bordes de las barras en negro.\nbins = 20: Especifica el número de barras (bins) en el histograma.\nalpha = 0.7: Establece la transparencia de las barras (valor entre 0 y 1, donde 0 es completamente transparente y 1 es opaco).\n\n\nAgregar la capa del boxplot:\ngeom_boxplot(width = 2, fill = \"darkgrey\", alpha = 0.7, position = position_nudge(y = -2)) +\n\ngeom_boxplot(width = 2, fill = \"darkgrey\", alpha = 0.7, position = position_nudge(y = -2)): Añade una capa de boxplot al gráfico.\n\nwidth = 2: Especifica el ancho del boxplot.\nfill = \"darkgrey\": Colorea el boxplot de gris oscuro.\nalpha = 0.7: Establece la transparencia del boxplot (valor entre 0 y 1).\nposition = position_nudge(y = -2): Ajusta la posición del boxplot, desplazándolo hacia abajo en el eje Y para que no se superponga con el histograma.\n\n\nAgregar etiquetas y título:\nlabs(title = \"Histograma y Boxplot\", y = \"Frecuencias\") +\n\nlabs(title = \"Histograma y Boxplot\", y = \"Frecuencias\"): Añade etiquetas y título al gráfico.\n\ntitle = \"Histograma y Boxplot\": Establece el título del gráfico.\ny = \"Frecuencias\": Etiqueta el eje Y como “Frecuencias”.\n\n\n\nResumen Completo\nEl código realiza las siguientes acciones:\n\nInicia el pipeline con el dataframe df_camembert.\nDefine la estética (aes) para mapear la variable est al eje X.\nAñade un histograma con barras azules claras, bordes negros, 20 bins y una transparencia del 70%.\nAñade un boxplot con ancho de 2, relleno gris oscuro, transparencia del 70% y lo desplaza hacia abajo en el eje Y para evitar superposición con el histograma.\nAñade un título y etiqueta el eje Y como “Frecuencias”.\n\n\n\n\n\ndf_camembert |&gt;\nggplot(aes(x = est)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20, alpha = 0.7) +\n  geom_boxplot(width = 2, fill = \"darkgrey\", alpha = 0.7, position = position_nudge(y = -2)) +\n  labs(title = \"Histograma y Boxplot\", y = \"Frecuencias\")\n\n\n\n\n\n\n\n\n\ndf_camembert |&gt;\nggplot(aes(x = log10(coliformes+1))) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20, alpha = 0.7) +\n  geom_boxplot(width = 4, fill = \"darkgrey\", alpha = 0.7, position = position_nudge(y = -4)) +\n  labs(title = \"Histograma y Boxplot\", y = \"Frecuencias\")",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-dispersión",
    "href": "040-exploracion.html#gráficos-de-dispersión",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.8 Gráficos de dispersión",
    "text": "4.8 Gráficos de dispersión\nUn gráfico de dispersión, también conocido como diagrama de dispersión o scatter plot, es una representación gráfica que utiliza puntos para mostrar la relación entre dos variables numéricas. Cada punto en el gráfico representa una observación del conjunto de datos y se coloca en el plano cartesiano de acuerdo con sus valores en las dos variables que se están comparando.\nUn gráfico de dispersión se compone mediante puntos:\n\nCada punto en el gráfico representa una observación.\nLa posición del punto en el gráfico está determinada por los valores de las dos variables para esa observación.\n\nLos gráficos de dispersión son útiles para identificar varios aspectos de la relación entre las dos variables:\n\nSi los puntos tienden a agruparse a lo largo de una línea recta ascendente, esto indica una correlación positiva (a medida que una variable aumenta, la otra también lo hace).\nSi los puntos se agrupan a lo largo de una línea descendente, esto indica una correlación negativa (a medida que una variable aumenta, la otra disminuye).\nSi los puntos forman una curva en lugar de una línea recta, esto sugiere una relación no lineal entre las variables.\nLa dispersión de los puntos puede indicar la variabilidad de los datos. Puntos que están muy lejos del patrón general pueden ser valores atípicos.\n\nComo siempre, vemos el gráfico de dispersión en Excel y a continuación en R. Utilizamos la tabla de datos camembert.csv y representamos las variables esty mg.\n\n\n\n\n\n\n\n\n\n\n\n(a) Tabla de origen mostrando una parte de los datos seleccionados\n\n\n\n\n\n\n\n\n\n\n\n(b) Gráfico de dispersión\n\n\n\n\n\n\n\nFigura 4.7: Gráfico de dispersión en Excel\n\n\n\nA continuación, el código R para realizar el gráfico de dispersión, con la función básica y con ggplot().\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\nPrimer Fragmento de Código: plot\n\nCrear un gráfico de dispersión básico:\nplot(df_camembert$est, df_camembert$mg)\n\nplot(df_camembert$est, df_camembert$mg): Crea un gráfico de dispersión usando la función base plot.\n\ndf_camembert$est: Mapea la variable est al eje X.\ndf_camembert$mg: Mapea la variable mg al eje Y.\n\n\n\nEste código genera un gráfico de dispersión que muestra la relación entre est y mg.\nSegundo Fragmento de Código: ggplot2\n\nUtilizar ggplot2 para crear un gráfico de dispersión más personalizable:\ndf_camembert |&gt;\n  ggplot(aes(x = est, y = mg)) +\n  geom_point() +\n  theme_bw()\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = est, y = mg)) +\n\nggplot(aes(x = est, y = mg)): Inicia un objeto ggplot y define las estéticas (aes). La variable est se mapea al eje X y la variable mg al eje Y.\n\nAgregar la capa de puntos:\ngeom_point() +\n\ngeom_point(): Añade una capa de puntos al gráfico, mostrando cada observación en est y mg.\n\nAplicar un tema en blanco y negro:\ntheme_bw()\n\ntheme_bw(): Aplica un tema en blanco y negro al gráfico, proporcionando un diseño limpio y clásico.\n\n\nResumen Completo\n\nPrimer Fragmento:\nplot(df_camembert$est, df_camembert$mg)\nEste código crea un gráfico de dispersión básico utilizando la función plot de base R, mapeando est al eje X y mg al eje Y.\nSegundo Fragmento:\ndf_camembert |&gt;\n  ggplot(aes(x = est, y = mg)) +\n  geom_point() +\n  theme_bw()\nEste conjunto de instrucciones utiliza ggplot2 para crear un gráfico de dispersión más personalizable, mapeando est al eje X y mg al eje Y, añadiendo una capa de puntos y aplicando un tema en blanco y negro para un diseño limpio y claro.\n\n\n\n\nplot(df_camembert$est, df_camembert$mg)\n\ndf_camembert |&gt;\n  ggplot(aes(x=est,y=mg))+\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n(a) Gráfico de dispersion básico de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Gráfico de dispersion utilizando ggplot()\n\n\n\n\n\n\n\nFigura 4.8: Ejemplos de gráfico de dispersión\n\n\n\nLo esperable en nuestros datos es que el valor de est y el de mg estén asociados, y a valores altos de la primera variable correspondan valores altos de la segunda (para el mismo producto y sin cambios de tecnología). Sin embargo, algunos valores parecen no encajar en este modelo; es el caso de un valor de estpor encima de 52% con un valor de mginferior a 23%, o el caso de un valor de mg en torno al 28% con un valor de est por debajo de 47%. Debemos revisar estos valores aparentemente anormales para verificar si ha habido un error en la toma de muestras o un error analítico. En fábricas con productos diferentes cuyos resultados analíticos se recogen en una tabla de datos común, a veces puede haber errores en la introducción de los códigos de producto, de forma que la toma de muestras y los análisis pueden ser correctos, pero estar mal asignados al guardar los datos.\nComo vemos, los gráficos de dispersión son una herramienta esencial en el análisis de datos exploratorio, ya que permiten visualizar relaciones y patrones en los datos, identificar correlaciones y detectar posibles anomalías. Esta información es crucial para realizar análisis estadísticos más profundos y tomar decisiones basadas en datos.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-series-temporales",
    "href": "040-exploracion.html#gráficos-de-series-temporales",
    "title": "4  La exploración de los datos mediante gráficos.",
    "section": "4.9 Gráficos de series temporales",
    "text": "4.9 Gráficos de series temporales\nHasta ahora hemos utilizado gráficos y tablas que describen la estructura y forma de una variable, o las relaciones entre dos variables. Hay otros gráficos que tienen en cuenta la forma en la que esos datos cambian con el tiempo. En este caso, será necesario que hayamos recogido en una variable de nuestra tabla los intervalos de tiempo en los que se han producido nuestros valores.\n\n\n\n\n\n\nEjemplos de series temporales\n\n\n\n¿Se te ocurren algunos ejemplos de series temporales?\n\n\n\n\n\n\n\n\nAlgunos ejemplos\n\n\n\n\n\n\nproceso de llenado de envases de queso crema: se llena una tarrina cada 15 segundos. Nuestros datos deben recoger el tiempo y el peso.\nnuestro fichero de fabricación de queso camembert recoge los valores analíticos medios diarios del producto fabricado.\nUna fábrica recoge leche diariamente y analiza cada día la composición de la leche que entra en la fábrica.\n\n\n\n\nEn un gráfico de series temporales,\n\nel eje horizontal (X) representa el tiempo. Los puntos de tiempo pueden ser minutos, horas, días, meses, años, etc.\nel eje vertical (Y) representa los valores de la variable que se está estudiando. Estos valores pueden ser medidas como temperatura, ventas, precios, etc.\ncada valor individual corresponde a un punto\nlos valores se conectan mediante una línea que conecta los puntos de datos, mostrando cómo cambian los valores de la variable a lo largo del tiempo.\nnormalmente, en un gráfico de series temporales no suelen representarse los puntos individuales para facilitar la legibilidad del gráfico.\n\nEn nuestro conjunto de datos de fabricación de queso camembert, la primera columna de la tabla recoge la variable fecha, lo que nos permite ordenar nuestros valores en el tiempo.\nCuando representamos valores en el tiempo, nunca usaremos el diagrama de barras, sino el gráfico de líneas.\n\nCómo hacer los gráficos de series temporales en Excel y en R\nPara hacer el gráfico en Excel, seleccionamos la columna este insertamos un gráfico de líneas. A continuación, con el cursor sobre el gráfico, pulsamos el boton derecho y seleccionamos la opción Seleccionar datos. Una vez abierto el cuadro de opciones, editamos las etiquetas del eje X y seleccionamos el rango de la variable fecha desde la fila 2 hasta la última.Aceptamos, y a continuación editamos el formato del eje Y para sustituir el valor mínimo de \\(0\\) por \\(42\\), que es el valor que queremos como mínimo para nuestro gráfico.\n\n\n\n\n\n\nFigura 4.9: Tabla y gráfico de series\n\n\n\nA continuación, los gráficos en R, como siempre con la opción de gráficos base y con ggplot(). R formatea automáticamente el rango del eje Y y no tenemos que hacer ninguna corrección de formato.\n\n\n\n\n\n\nExplicación del código R\n\n\n\n\n\nPrimer Fragmento de Código: plot\n\nCrear un gráfico de líneas básico:\nplot(df_camembert$fecha, df_camembert$est, type = \"l\")\n\nplot(df_camembert$fecha, df_camembert$est, type = \"l\"): Crea un gráfico de líneas usando la función base plot.\n\ndf_camembert$fecha: Mapea la variable fecha al eje X.\ndf_camembert$est: Mapea la variable est al eje Y.\ntype = \"l\": Especifica que el gráfico debe ser de líneas. La \"l\" indica que las observaciones deben conectarse con líneas.\n\n\n\nSegundo Fragmento de Código: ggplot2\n\nUtilizar ggplot2 para crear un gráfico de líneas:\ndf_camembert |&gt;\n  ggplot(aes(x = fecha, y = est)) +\n  geom_line() +\n  theme_bw()\n\ndf_camembert |&gt;: Utiliza el operador pipe (|&gt;) para encadenar operaciones sobre el dataframe df_camembert.\n\nIniciar ggplot con las estéticas (aes):\nggplot(aes(x = fecha, y = est)) +\n\nggplot(aes(x = fecha, y = est)): Inicia un objeto ggplot y define las estéticas (aes). La variable fecha se mapea al eje X y la variable est al eje Y.\n\nAgregar la capa de línea:\ngeom_line() +\n\ngeom_line(): Añade una capa de líneas al gráfico, conectando los puntos de datos en fecha y est.\n\nAplicar un tema en blanco y negro:\ntheme_bw()\n\ntheme_bw(): Aplica un tema en blanco y negro al gráfico, proporcionando un diseño limpio y clásico.\n\n\nResumen Completo\n\nPrimer Fragmento:\nplot(df_camembert$fecha, df_camembert$est, type = \"l\")\nEste código crea un gráfico de líneas básico utilizando la función plot de base R, mapeando fecha al eje X y est al eje Y.\nSegundo Fragmento:\ndf_camembert |&gt;\n  ggplot(aes(x = fecha, y = est)) +\n  geom_line() +\n  theme_bw()\nEste conjunto de instrucciones utiliza ggplot2 para crear un gráfico de líneas más personalizable, mapeando fecha al eje X y est al eje Y, añadiendo una capa de líneas y aplicando un tema en blanco y negro para un diseño limpio y claro.\n\n\n\n\nplot(df_camembert$fecha, df_camembert$est, type=\"l\")\n\ndf_camembert |&gt;\n  ggplot(aes(x=fecha, y=est))+\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n(a) Gráfico de series temporales básico de R\n\n\n\n\n\n\n\n\n\n\n\n(b) Gráfico de series temporales utilizando ggplot()\n\n\n\n\n\n\n\nFigura 4.10: Ejemplos de gráfico de series temporales\n\n\n\n\n\nResumen\nLos gráficos de series temporales son útiles para:\n\nIdentificar Tendencias:\n\nUna tendencia es una dirección general en la que los datos se mueven a lo largo del tiempo. Puede ser creciente, decreciente o constante.\n\nDetección de Estacionalidad:\n\nLa estacionalidad se refiere a patrones que se repiten en intervalos regulares de tiempo, como las ventas de productos estacionales.\n\nIdentificar Ciclos:\n\nLos ciclos son fluctuaciones que ocurren en intervalos no regulares y pueden deberse a factores económicos o de otra índole.\n\nDetección de Anomalías:\n\nLos picos y caídas repentinas pueden indicar eventos inusuales o errores en los datos.\n\n\nLos gráficos de series temporales son cruciales en diversas áreas:\n\nEconomía y Finanzas:\n\nSeguimiento de precios de acciones, tasas de interés y otros indicadores económicos.\n\nCiencia y Tecnología:\n\nMonitoreo de variables ambientales, datos meteorológicos y medidas científicas.\n\nNegocios:\n\nAnálisis de ventas, demanda de productos y desempeño empresarial a lo largo del tiempo.\n\n\nLos gráficos de series temporales proporcionan una visión clara y concisa de cómo cambian los datos a lo largo del tiempo. Esta visualización es fundamental para el análisis predictivo, la toma de decisiones y la identificación de patrones y anomalías en los datos.\nhttps://tidyplots.org/",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html",
    "href": "050-estad-simple.html",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "",
    "text": "5.1 La media aritmética: un ejemplo de cálculo.\nAl estudiar el diagrama de caja (boxplot) hemos visto que este gráfico se describe mediante los cinco números, que son:\nDe estos valores, el segundo cuartil, que se corresponde con la mitad de los valores, representa una estimación del centro de la distribución, y por eso lo llamamos mediana. La distancia entre el primer y tercer cuartil es lo que se conoce como rango intercuartil (se suele representar por sus siglas en inglés, IQR), y nos da una indicación de la dispersión: cuanto mayor mayor es la dispersión de nuestros valores, más alejados estarán del centro, y por lo tanto habrá mayor distancia entre el primer y el tercer cuartil.\nUna de las mayores ventajas de la mediana y del rango intercuartil es que son estadísticos robustos, es decir, tiene una alta robustez a los valores atípicos. Como en el cálculo del rango intercuartil no se tienen en cuenta los valores extremos, su valor variará muy poco si aparecen nuevas observaciones atípicas (outliers). Como sus valores no dependen de la distribución de los datos, a estos estadísticos se los conoce como no paramétricos.\nExisten otras estadísticos, llamados paramétricos, que, en determinadas condiciones, tienen ventajas frente a los no paramétricos. Los principales son la media aritmética, o simplemente, media, y la varianza.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#la-media-aritmética-un-ejemplo-de-cálculo.",
    "href": "050-estad-simple.html#la-media-aritmética-un-ejemplo-de-cálculo.",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "",
    "text": "Definición\nLa media aritmética de un conjunto de valores es el valor central que se obtiene al dividir la suma de todos los valores por la cantidad de valores. Es una medida de tendencia central que proporciona un punto de referencia para el conjunto de datos.\n\n\nDeducción de la Fórmula:\nSupongamos que tenemos un conjunto de \\(n\\) valores numéricos: \\(x_1, x_2, x_3, \\ldots, x_n\\).\n\nSuma de Todos los Valores: Primero, sumamos todos los valores del conjunto. Matemáticamente, esto se expresa como:\n\\[\nS = x_1 + x_2 + x_3 + \\cdots + x_n\n\\]\nCantidad de Valores: Luego, contamos cuántos valores hay en el conjunto. Este número es \\(n\\).\nDivisión de la Suma por la Cantidad de Valores: Finalmente, dividimos la suma total \\(S\\) por la cantidad de valores \\(n\\) para obtener la media aritmética:\n\\[\n\\text{Media Aritmética} = \\frac{S}{n}\n\\]\nExpresión General: Sustituyendo la suma \\(S\\) en la fórmula, tenemos:\n\\[\n\\text{Media Aritmética} = \\frac{x_1 + x_2 + x_3 + \\cdots + x_n}{n}\n\\]\n\n\n\nEjemplo:\nSupongamos que tenemos los siguientes valores: \\(5, 7, 9\\).\n\nSuma de los Valores: \\[\nS = 5 + 7 + 9 = 21\n\\]\nCantidad de Valores: \\[\nn = 3\n\\]\nCálculo de la Media Aritmética: \\[\n\\text{Media Aritmética} = \\frac{21}{3} = 7\n\\]\n\nEntonces, la media aritmética de los valores \\(5, 7, 9\\) es \\(7\\).\nLa media de una muestra se representa habitualmente mediante el símbolo\n\\[\\bar{x}\\] y, de una manera más formal, su valor se obtiene mediante la fórmula siguiente:\n\\[{\\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}}\\] El signo \\(\\sum\\) se conoce como sumatorio, e indica que ese término consiste en la suma de los \\(x\\) valores desde el primero hasta el valor \\(n\\). Expresado mediante una formulación matemática,\n\\[{\\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}}\\] lo quiere quiere decir: “la suma de todos los valores observados dividido entre el número de estos valores”.\nLa media es lo que conocemos como un valor central, ya que representa el centro de nuestro conjunto de números. Como es el centro de nuestro conjunto de datos, la suma de las distancias de todos los valores a este valor central es \\(cero\\). Más adelante veremos la importancia de este hecho, al hablar de la dispersión y las formas de cálculo de la misma. Como hemos visto, la media de una muestra se representa como \\(\\bar{x}\\), mientras que la media de una población se representa con la letra griega mu: \\(\\mu\\). En ambos casos, el cálculo se realiza de forma idéntica.\nVolvamos a nuestro ejemplo de la altura de un grupo de alumnos, para realizar los cálculos según el modelo que hemos descrito. En nuestro caso, la altura media de nuestros alumnos (la media de nuestro conjunto de números) se calcula como:\n\\[\n\\bar{x} = \\frac{153+135+140+140+175+138+145+154+152+159+154}{11} = 149,54\n\\] Utilicemos una hoja de cálculo para guardar nuestros valores.\n\nLa fórmula para obtener la media en la hoja de cálculo, por ejemplo en la versión en español de Microsoft Excel, es =PROMEDIO(...), donde los puntos suspensivos deben sustituirse por el rango a calcular. En nuestro ejemplo, introduciríamos la fórmula en la celda B13como =PROMEDIO(B2..B12) (Para más detalles, verificar la hoja Excel adjunta).\nPara representar más cómodamente nuestros valores, dibujamos un punto a la altura de cada alumno,\n\ny eliminamos del gráfico los dibujos de nuestros alumnos; así hemos convertido nuestro dibujo en un diagrama de puntos:\n\nPara representar la media, aunque la media es un valor único, necesitamos añadir una columna a la derecha de nuestros datos, que rotulamos en la fila 1, celda C como altura media, e introducimos en cada una de las celdas desde C2hasta C12la fórmula del promedio, con el valor de nuestro rango de datos (Verificar hoja de cálculo). A continuación, designamos nuestro rango de datos para hacer un gráfico de puntos, y hacemos un zoom en los valores de manera que el eje Y se escale mejor entre los valores mínimo y máximo. Por último, hacemos unos ajustes en el formato para dibujar las líneas verticales que nos representan la distancia de cada valor a la media.\n\nSi verificamos el eje \\(Y\\) , veremos que en este gráfico hemos ajustado la escala respecto al gráfico anterior, situando el mínimo en \\(130\\). Esto permite visualizar las diferencias con mucha más claridad. Hemos representado la media \\(\\bar{x}\\) como una línea, y hemos dibujado unas líneas que unen cada valor individual con la media, que se sitúa en el valor \\(149,55\\), tal como calculamos más arriba.\nHemos representado la media como una serie de puntos unidos por una línea amarilla. Representamos en azul nuestros valores, uniendo cada valor con la línea media mediante una línea de puntos vertical. A partir de ahora, por conveniencia, eliminaremos los puntos en la linea media, dejando sólo la línea.\n\n\n\nHoja de cálculo con los valores y el gráfico de puntos\n\n\nEsta línea azul de puntos representa la distancia de cada valor a la media. Usaremos esta distancia para calcular una distancia media, que será una medida de la dispersión de nuestros valores.\nHemos visto que para describir un conjunto de números, en nuestro ejemplo, las medidas de la altura de un grupo de estudiantes, existe un valor, la media de este conjunto, que nos describe el centro de los valores. En nuestro ejemplo, si nuestro grupo tuviese un solo niño, éste tendría \\(149,55{\\ }cm\\) de altura.\n¿Es suficiente con este valor para describir el conjunto de valores? Vamos a ver que no: diferentes conjuntos de valores pueden proporcionar el mismo valor medio, y sin embargo los grupos pueden ser muy diferentes.\nVeamos un caso extremo. Comparemos dos grupos, uno formado por individuos iguales y otro formado por diez individuos iguales y uno distinto. Para ello usaremos nuestra hoja de cálculo:\n\n\n\n\nDos grupos de valores con la misma media\n\n¿Podemos describir adecuadamente los valores de la altura de cada uno de los grupos utilizando el valor medio? Parece evidente que no, ya que a partir de diferentes valores de altura estamos obteniendo el mismo valor medio. Sin embargo, uno de los grupos es más alto que el otro, si no fuera por un sólo individuo que aparentemente distorsiona el cálculo. Podríamos incluir nuestro grupo original, y veremos que los tres grupos son diferentes, aunque su valor medio es idéntico.\n\n\n\n\nTres grupos de valores con la misma media\n\nSi nos ayudamos de un gráfico equivalente al que hemos utilizado antes, vemos estas diferencias con claridad:\n\n\n\nGráfico de tres grupos de valores\n\n\nAunque el valor medio de estos tres grupos de datos es idéntico, parece claro que los tres grupos son muy distintos en su composición, y por lo tanto la media no es suficiente para describir con suficiente precisión cada uno de los grupos. Necesitamos un valor adicional, que nos indique de qué forma los valores se alejan del valor medio. Para ello, vamos a introducir un concepto nuevo: la medida de la dispersión.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#las-medidas-de-dispersión-la-varianza-y-la-desviación-típica",
    "href": "050-estad-simple.html#las-medidas-de-dispersión-la-varianza-y-la-desviación-típica",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "5.2 Las medidas de dispersión: la varianza y la desviación típica",
    "text": "5.2 Las medidas de dispersión: la varianza y la desviación típica\nComo hemos visto en el apartado anterior, diferentes conjuntos de datos pueden tener el mismo valor medio y sin embargo ser muy diferentes. En la última gráfica que hemos visto, el primer grupo se caracteriza por tener todos sus valores idénticos; el segundo tiene todos sus valores idénticos menos uno, que está muy apartado del resto, y el tercero tiene todos sus valores diferentes.\nAhora que conocemos cómo calcular un valor resumen de un conjunto de datos, podríamos utilizar una medida semejante para describir de qué forma en cada caso los valores se separan de la media. Podríamos utilizar una distancia media: calculamos las diferencias entre cada valor y la media, y hacemos su promedio: esto debería darnos una indicación de la magnitud de la separación de los valores en cada uno de los tres grupos.\nUsemos la hoja de cálculo para ello:\n\n\n\nTres grupos de valores en la hoja de cálculo\n\n\nAlgo parece que no está funcionando aquí: el promedio de las diferencias es cero en los tres casos; no podemos usar este cálculo para calcular la dispersión. Pero esto es esperable: ya que la media es un valor central, como hemos visto antes, la suma de las diferencias de todos los valores respecto de su media debe ser forzosamente cero, y esto es lo que estamos obteniendo.\nPara encontrar una solución, vamos a recurrir al viejo teorema de Pitágoras, que si recuerdas, nos dice que, en un triángulo rectángulo, el cuadrado de la hipotenusa es igual a la suma de los cuadrados de los catetos (una explicación gráfica muy divertida en el anexo …): \\[\nh^2= a^2+b^2\n\\] Esta fórmula es la base del cálculo de la distancia entre dos puntos:\n\n\n\n\nDistancia entre dos puntos\n\n\\[\nd(A,B)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\\] Podemos adaptar esta fórmula al cálculo de nuestra distancia media. Como estamos calculando la distancia en una dimensión, sólo necesitamos la coordenada \\(X\\). Si tenemos en cuenta un solo punto, esta distancia \\(d\\) sería: \\[\n(d{\\ }del{\\ }valor{\\ }1{\\ }a{\\ }la{\\ }media)^2=(x_1-\\bar{x})^2\n\\] ¡El hecho de elevar al cuadrado las diferencias nos da la solución! Las diferencias negativas ya no son un problema porque sabemos que al elevar un numero negativo al cuadrado, el resultado es positivo; de esta manera conseguimos que las diferencias no se anulen. Ahora sí podemos calcular una distancia media \\(\\bar{d}\\) entre el conjunto de puntos y su media, calculando el promedio de las diferencias elevadas al cuadrado: \\[\n(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2=\\frac{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2}{n}\n\\]\ny utilizando la notación que hemos aprendido antes,\n\\[\n(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2\n\\]\nAl igual que en el cálculo de la distancia entre dos puntos, sólo tenemos que extraer la raíz cuadrada de este valor para obtener la distancia media, que es el parámetro que estábamos buscando.\nLa distancia media \\[(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2\\] se conoce en estadística como varianza, y su raíz cuadrada es lo que se conoce como desviación típica. La varianza de una población se representa en estadística con el signo de la letra griega sigma minúscula elevada al cuadrado, \\(\\sigma^2\\), y la desviación típica, mediante la letra \\(\\sigma\\). En el caso de una muestra, la varianza se representa como \\(s_x^2\\), y la desviación típica, como \\(s_x\\).\nEs importante resaltar que la desviación típica es una medida de la distancia media de los valores de una población a su media, y por lo tanto tiene dimensión, la misma que las medidas originales. La varianza, al estar elevada al cuadrado, no tiene una dimensión, o, mejor dicho, tiene la de la medida al cuadrado.\nCon estos nuevos hallazgos, recalculamos nuestra hoja de cálculo:\n\n\n\nTres grupos de valores en la hoja de cálculo, con la misma media y distinta desviación típica\n\n\nVamos a analizar con detalle esta tabla.\nEn la columna J tenemos nuestra población original de 11 alumnos, con las alturas que hemos medido. En la columna B hemos supuesto que todos los alumnos fuesen iguales, con la misma altura del valor medio de los datos originales. En la columna F hemos simulado otro grupo, con todos los valores iguales excepto uno, y con la misma media que los otros dos grupos.\nA la derecha de cada columna de medias, tenemos la columna de diferencias (columnas D, H y L), y en la fila 13, nuestro primer intento de calcular una dispersión media; intento fallido, puesto que obteníamos el valor \\(0\\) para los tres grupos.\nEn la siguiente columna a la derecha, para los tres grupos (columnas E, Iy M), hemos elevado al cuadrado la distancia de cada valor a la media, siguiendo los hallazgos que nos ha proporcionado el teorema de Pitágoras y la fórmula de la distancia entre dos puntos. En la fila 13 de estas columnas, calculamos el promedio de la distancia a la media al cuadrado: esta vez el resultado ya no es cero, sino que obtenemos el valor de la varianza, de acuerdo con la fórmula que hemos deducido más arriba. En la fila 14 (columnas B, F y J)utilizamos la fórmula de la hoja de cálculo para la varianza poblacional (más detalles posteriormente), y vemos que coincide exactamente con el promedio de las diferencias al cuadrado, tal como debe ser, ya que en eso consiste la fórmula que hemos deducido.\nPor último, en la fila 15calculamos la desviación típica de ambas formas, con la fórmula de la hoja de cálculo para la desviación típica poblacional (columnas B, Fy J), que Excel llama desviación estándar, y como la raíz cuadrada del promedio calculado antes (columnas E, Iy M). De nuevo, ambos valores coinciden exactamente, como esperamos.\n\n\n\nGráfico con tres conjuntos de datos con la misma media y diferente desviación típica\n\n\nAhora sí tenemos una forma más completa de describir nuestro conjunto de valores. Aunque el valor medio es el mismo en los tres casos, la dispersión de los valores es muy distinta.\n¿Son suficientes estos dos parámetros que hemos calculado para describir un conjunto de datos? La respuesta a esta pregunta es sí y no. La explicación es que, más allá de los valores numéricos que hemos obtenido, la visualización gráfica de los valores nos debe hacer reflexionar.\nEn el primer grupo, todos los valores son iguales a la media. La variación es cero. Son valores que hemos simulado en nuestra hoja de cálculo, pero difícilmente en el mundo real encontraremos una población en la que todos sus valores, en este caso, la altura de un grupo de alumnos, sean idénticos.\nEn el segundo grupo, todos los valores son idénticos, salvo uno, que se distancia mucho. ¿Debemos aceptar esto como bueno? En realidad, ¿es cierto que el valor medio de este grupo sea el mismo que el del primero? Para responder a esta pregunta debemos recurrir a nuestra experiencia, la estadística no nos da fórmulas mágicas. Pero, con un poco de sentido común, parece que el caso extremo que aparece en este grupo no es coherente con el resto de valores. Es lo que se llama un valor anormal o extraño (en inglés, outlier), y debe hacernos reflexionar sobre si el valor es correcto y realmente pertenece a esta población, o es un error de medida. O, simplemente, un valor que corresponde a otro grupo y que por error hemos situado en éste. La decisión de eliminar o no un valor anormal es una de las decisiones más complejas en estadística, que pueden tener una influencia enorme en la interpretación de los datos, y por lo tanto, hay que hacer con sumo cuidado. En este caso, extremo y artificial, el valor anormal debería ser eliminado, ya que, en realidad, todos los valores restantes son idénticos y más bajos que los del grupo 1. No tiene sentido lógico decir que sus medias son idénticas.\nEn el tercer grupo todos los valores son diferentes, y no podemos decir nada especial sobre sus valores individuales. Hay un valor que se destaca del resto, pero ¿podemos afirmar que es anormal? Seguramente, no con seguridad. De nuevo la experiencia debe indicarnos cómo proceder, aunque en este caso no tendría sentido eliminar este valor. En la situación real, todos conocemos a niños que han pegado el estirón antes que sus compañeros, y en algunos casos, pueden llegar a ser mucho más altos (o más bajos, si han tenido un retraso en este estirón) La experiencia nos dice que no es seguro que este valor sea realmente anormal, y por lo tanto, deberíamos conservarlo.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#las-limitaciones-de-la-media-y-la-desviación-típica",
    "href": "050-estad-simple.html#las-limitaciones-de-la-media-y-la-desviación-típica",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "5.3 Las limitaciones de la media y la desviación típica",
    "text": "5.3 Las limitaciones de la media y la desviación típica\nEn ocasiones nos enfrentamos a conjuntos de datos con valores de media y desviación típica idénticos o muy parecidos, pero que en realidad son muy diferentes. Veamos un ejemplo, semejante a los que hemos visto hasta ahora.\n\n\n\n\nHoja de cálculo con dos conjuntos de datos diferentes, con la misma media y desviación típica\n\n(cambiar a caso 3)\n\n\n\nDiagrama de puntos de las alturas de los alumnos, con indicación del valor medio\n\n\n\n\n\nDiagrama de puntos indicando la variabilidad\n\n\nEn este caso, vemos que tanto la media como la desviación típica son idénticos, y sin embargo los datos son muy diferentes, tal como nos muestra el gráfico de dispersión que hemos estado utilizando:\n\n\n\nGráfico con dos conjuntos de datos diferentes, con la misma media y desviación típica\n\n\nLa existencia de valores anormales o extremos muestra una de las debilidades de la media y la desviación típicas como descriptores de una población: ambos parámetros son muy sensibles a los casos extremos. En realidad, sólo deberíamos utilizar la media y la desviación típica para describir un conjunto de datos cuando estamos seguros de que la distribución de estos datos tienen una forma determinada, la de la campana de Gauss. En otros casos, la mediana y el rango intercuartil son estadísticos más robustos, y por lo tanto más seguros.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#resumen",
    "href": "050-estad-simple.html#resumen",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "5.4 Resumen",
    "text": "5.4 Resumen\nLas medidas paramétricas, como la media y la varianza, y las no paramétricas, como la mediana y el rango intercuartil, tienen diferentes ventajas e inconvenientes según el contexto y los datos con los que se trabaja.\n\nVentajas e inconvenientes de las medidas paramétricas\n\nVentajas\n\nPrecisión y Sensibilidad: La media y la varianza son muy precisas y sensibles a todos los valores del conjunto de datos.\nPropiedades Matemáticas: La media y la varianza tienen propiedades matemáticas deseables, como la facilidad para realizar operaciones algebraicas.\nDistribución Normal: Son especialmente útiles si los datos siguen una distribución normal, ya que permiten aprovechar las propiedades de esta distribución.\n\n\n\nInconvenientes\n\nSensibilidad a Valores Atípicos: La media y la varianza pueden ser distorsionadas significativamente por valores atípicos.\nRequieren Suposiciones: Su uso eficaz a menudo requiere que los datos sigan ciertas suposiciones, como la normalidad y la homogeneidad de la varianza.\n\n\n\n\nVentajas e inconvenientes de las medidas no paramétricas\n\nVentajas\n\nRobustez: La mediana y el rango intercuartil son menos sensibles a valores atípicos y distribuciones asimétricas.\nFlexibilidad: No requieren suposiciones fuertes sobre la distribución de los datos, lo que las hace útiles para una amplia variedad de distribuciones.\nResumir Datos: Son excelentes para resumir datos en situaciones en las que los valores extremos podrían distorsionar la interpretación.\n\n\n\nInconvenientes\n\nMenor Sensibilidad: La mediana y el rango intercuartil no utilizan toda la información de los datos y pueden ser menos sensibles a cambios en los datos.\nMenor Precisión en Ciertos Contextos: En situaciones donde los datos siguen una distribución normal, las medidas no paramétricas pueden ser menos precisas.\n\nLas medidas paramétricas son útiles para datos que siguen suposiciones específicas, como la normalidad, y son precisas y sensibles, pero pueden ser distorsionadas por valores atípicos. Las medidas no paramétricas son robustas y flexibles, ideales para distribuciones no normales y resistentes a valores atípicos, aunque pueden perder sensibilidad y precisión en ciertos contextos.\nModelo: práctica de puntos con un dado, dos dados, tres dados, etc hasta 30\nsuma &lt;- rowSums(replicate(30, sample(6, 10^6, replace=T))) length(suma) hist(suma)",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#la-media-aritmética-como-centro-de-gravedad-de-un-grupo-de-datos",
    "href": "050-estad-simple.html#la-media-aritmética-como-centro-de-gravedad-de-un-grupo-de-datos",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "5.5 La media aritmética como centro de gravedad de un grupo de datos",
    "text": "5.5 La media aritmética como centro de gravedad de un grupo de datos\nPara ver cómo la media aritmética equivale al punto de equilibrio o centro de gravedad de un grupo de datos, imaginaremos que tenemos un conjunto de datos cuyos valores se distribuyen en un eje longitudinal X. Podemos hacer equivalentes estos valores a un conjunto de pesos que se distribuyen a lo largo de una barra, y vamos a suponer que existe un punto a una distancia \\(d_i\\) del origen de la barra en el cual dicha barra está en equilibrio. Para encontrar ese valor, empezaremos considerando el principio o ley de la palanca.\nEl Principio de la Palanca o la Ley de la Palanca fue formulada por el científico y matemático griego Arquímedes. Este principio dice que, en equilibrio, el producto de la fuerza aplicada (potencia) por su distancia al punto de apoyo (brazo de la potencia) es igual al producto de la resistencia por su distancia al punto de apoyo (brazo de la resistencia). Matemáticamente, se expresa como: \\[\nP \\cdot d_p = R \\cdot d_r\n\\]\nDonde: - \\(P\\) es la potencia o fuerza aplicada. - \\(d_p\\) es la distancia desde el punto de apoyo hasta el punto donde se aplica la potencia. - \\(R\\) es la resistencia o carga. - \\(d_r\\) es la distancia desde el punto de apoyo hasta el punto donde se aplica la resistencia.\n\nEjemplo\nSi tienes una palanca con una longitud de 5 metros y aplicas una fuerza de 10 Newtons a 1 metro del punto de apoyo, para mantener el equilibrio, la fuerza de resistencia en el otro extremo a 4 metros del punto de apoyo debería ser: \\[\nP \\cdot d_p = R \\cdot d_r\n\\] \\[\n10 \\, \\text{N} \\cdot 1 \\, \\text{m} = R \\cdot 4 \\, \\text{m}\n\\] \\[\nR = \\frac{10 \\cdot 1}{4} = 2,5 \\text{N}\n\\]\nAhora vamos a considerar el punto de equilibrio de una barra de la que se cuelgan diferentes pesos a diferente distancias de su origen.\nCalcular el punto de equilibrio de una barra de la que se cuelgan diferentes pesos a diferentes distancias de su origen es un problema clásico de física que se puede resolver usando el principio de momentos.\n\n\nPaso a Paso\n\nIdentificar las fuerzas: Supongamos que tienes varios pesos \\(W_1, W_2, W_3, \\ldots, W_n\\) colgados a distancias \\(d_1, d_2, d_3, \\ldots, d_n\\) del origen (punto de apoyo).\nCalcular los momentos: El momento (\\(M\\)) de un peso alrededor del punto de apoyo se calcula como el producto de la fuerza (peso) y la distancia al punto de apoyo: \\[\nM_i = W_i \\times d_i\n\\]\nSumar los momentos: Calcula la suma de todos los momentos: \\[\nM_{\\text{total}} = W_1 \\times d_1 + W_2 \\times d_2 + W_3 \\times d_3 + \\ldots + W_n \\times d_n\n\\]\nCalcular el peso total: Suma todos los pesos: \\[\nW_{\\text{total}} = W_1 + W_2 + W_3 + \\ldots + W_n\n\\]\nDeterminar el punto de equilibrio (\\(x\\)): El punto de equilibrio se encuentra dividiendo la suma de los momentos por el peso total: \\[\nx = \\frac{\\sum (W_i \\times d_i)}{\\sum W_i}\n\\]\n\n\n\nEjemplo Práctico\nSupongamos que tenemos tres pesos de 4, 9 y 1 kg, situados, respectivamente, a 2, 1 y 3 m respectivamente del origen de la barra.\n\n\\(W_1 = 4 \\, \\text{kg}\\) a \\(d_1 = 2 \\, \\text{m}\\)\n\\(W_2 = 9 \\, \\text{kg}\\) a \\(d_2 = 1 \\, \\text{m}\\)\n\\(W_3 = 1 \\, \\text{kg}\\) a \\(d_3 = 3 \\, \\text{m}\\)\n\n\nCalcular los momentos: \\[\nM_1 = 4 \\times 2 = 8 \\, \\text{kg·m}\n\\] \\[\nM_2 = 9 \\times 1 = 9 \\, \\text{kg·m}\n\\] \\[\nM_3 = 1 \\times 3 = 3 \\, \\text{kg·m}\n\\]\nSumar los momentos: \\[\nM_{\\text{total}} = 8 + 9 + 3 = 20 \\, \\text{kg·m}\n\\]\nCalcular el peso total: \\[\nW_{\\text{total}} = 4 + 9 + 1 = 14 \\, \\text{kg}\n\\]\nDeterminar el punto de equilibrio:\n\\[\nx = \\frac{20}{14} \\approx 1.43 \\, \\text{m}\n\\]\n\nEl punto de equilibrio se encuentra a aproximadamente 1.43 metros del origen.\n\n\nMedia Ponderada\nEn muchas situaciones, el centro de gravedad de un sistema de masas puede interpretarse como una media ponderada de las posiciones de las masas.\n\nMedia Aritmética\nPara un conjunto de números \\(x_1, x_2, \\ldots, x_n\\), la media aritmética es: \\[\n\\text{Media} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\n\n\nMedia Ponderada\nPara un conjunto de valores \\(x_1, x_2, \\ldots, x_n\\) con pesos asociados \\(w_1, w_2, \\ldots, w_n\\), la media ponderada es: \\[\n\\text{Media ponderada} = \\frac{w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n}{w_1 + w_2 + \\cdots + w_n}\n\\]\n\n\n\nCentro de Gravedad como Media Ponderada\nCuando calculamos el centro de gravedad (\\(x_{\\text{cg}}\\)) de un sistema de masas, estamos esencialmente calculando una media ponderada de las posiciones (\\(x_i\\)) de esas masas (\\(m_i\\)):\n\\[\nx_{\\text{cg}} = \\frac{\\sum (m_i \\cdot x_i)}{\\sum m_i}\n\\]\nAquí, las posiciones \\(x_i\\) son ponderadas por las masas \\(m_i\\).\n\n\nEjemplo Numérico\nEn nuestro ejemplo anterior,\n\nMasa 1: \\(m_1 = 4 \\, \\text{kg}\\) en posición \\(x_1 = 2 \\, \\text{m}\\)\nMasa 2: \\(m_2 = 9 \\, \\text{kg}\\) en posición \\(x_2 = 1 \\, \\text{m}\\)\nMasa 3: \\(m_3 = 1 \\, \\text{kg}\\) en posición \\(x_3 = 3 \\, \\text{m}\\)\n\nLa media aritmética de las posiciones sería: \\[\n\\text{Media} = \\frac{2 + 1 + 3}{3} = 2 \\, \\text{m}\n\\]\nEl punto de equlibrio de la barra, o centro de gravedad, calculado como media ponderada, sería:\n\\[\nx_{\\text{cg}} = \\frac{(4 \\cdot 2) + (9 \\cdot 1) + (1 \\cdot 3)}{4 + 9 + 1} = \\frac{8 + 9 + 3}{14} = \\frac{20}{14} \\approx 1,43 \\, \\text{m}\n\\] que es el mismo resultado que obteníamos formulando el cálculo de los momentos; la fórmula resulta ser idéntica.\n\n\nMedia Aritmética y Centro de Gravedad con Masas Idénticas\nEn los ejemplos anteriores, calculábamos el punto de equilibrio para diferentes pesos colocados a lo largo de una barra. Si en vez de eso suponemos que la distancia al origen de la barra equivale a nuestro eje \\(X\\), que recoge los valores de los que queremos calcular nuestra media aritmética, podemos eliminar el efecto de la masa suponiendo que todas las masas son iguales.\nSi las masas de los diferentes objetos son idénticas, podemos decir que la media aritmética de las posiciones coincide con el centro de gravedad. Esto se debe a que, en este caso, cada masa tiene el mismo peso o influencia en el cálculo del centro de gravedad. Esta es la explicación detallada:\n\nSuposición\nCada objeto tiene la misma masa \\(m\\).\n\n\nMedia Aritmética\nPara un conjunto de posiciones \\(x_1, x_2, \\ldots, x_n\\), la media aritmética es: \\[\n\\text{Media} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\n\n\nCentro de Gravedad\nEl centro de gravedad (\\(x_{\\text{cg}}\\)) para un conjunto de masas idénticas en posiciones \\(x_1, x_2, \\ldots, x_n\\) es: \\[\nx_{\\text{cg}} = \\frac{\\sum (m \\cdot x_i)}{\\sum m}\n\\]\nDado que las masas \\(m\\) son idénticas, el numerador se convierte en: \\[\n\\sum (m \\cdot x_i) = m \\cdot (x_1 + x_2 + \\cdots + x_n)\n\\]\nY el denominador se convierte en: \\[\n\\sum m = n \\cdot m\n\\]\nAl sustituir estos en la fórmula del centro de gravedad, obtenemos: \\[\nx_{\\text{cg}} = \\frac{m \\cdot (x_1 + x_2 + \\cdots + x_n)}{n \\cdot m}\n\\]\nAl simplificar, los términos \\(m\\) se cancelan, y nos queda: \\[\nx_{\\text{cg}} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\] que es exactamente la fórmula de la media aritmética.\nSi repetimos nuestro ejemplo anterior, suponiendo en este caso masas idénticas\n\nMasa 1: \\(m_1 = 1 \\, \\text{kg}\\) en posición \\(x_1 = 2 \\, \\text{m}\\)\nMasa 2: \\(m_2 = 1 \\, \\text{kg}\\) en posición \\(x_2 = 1 \\, \\text{m}\\)\nMasa 3: \\(m_3 = 1 \\, \\text{kg}\\) en posición \\(x_3 = 3 \\, \\text{m}\\)\n\nLa media aritmética de las posiciones sería: \\[\n\\text{Media} = \\frac{2 + 1 + 3}{3} = 2 \\, \\text{cm}\n\\]\nEl punto de equlibrio de la barra, o centro de gravedad, calculado como media ponderada, sería:\n\\[\nx_{\\text{cg}} = \\frac{(1 \\cdot 2) + (1 \\cdot 1) + (1 \\cdot 3)}{1 + 1 + 1} = \\frac{2 + 1 + 3}{3} = \\frac{6}{3} = 2 \\, \\text{m}\n\\] que es el mismo resultado que obteníamos con la fórmula de la media aritmética de las distancias.\n\n\n\nConclusión\nCuando las masas son idénticas, la media aritmética de las posiciones coincide con el centro de gravedad. Esto nos permite afirmar que en el caso de una dimensión (por ejemplo, el peso) la media aritmética de un conjunto de valores coincide con el centro de gravedad o punto de equilibrio de ese conjunto de valores.Este es un resultado interesante que muestra la conexión entre conceptos estadísticos y físicos.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#importancia-del-concepto-de-la-media-como-centro-de-gravedad-o-punto-de-equilibrio-de-un-conjunto-de-datos",
    "href": "050-estad-simple.html#importancia-del-concepto-de-la-media-como-centro-de-gravedad-o-punto-de-equilibrio-de-un-conjunto-de-datos",
    "title": "5  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "5.6 Importancia del concepto de la media como centro de gravedad o punto de equilibrio de un conjunto de datos",
    "text": "5.6 Importancia del concepto de la media como centro de gravedad o punto de equilibrio de un conjunto de datos\nEfecto de los outliers: desequilibrio, desplazamiento de la media, incremento de la varianza (gan diferencia), mientras que la mediana y el rango intercuartil no se ven afectados",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "060-forma-de-los-datos.html",
    "href": "060-forma-de-los-datos.html",
    "title": "6  La forma de los datos",
    "section": "",
    "text": "Introducción al Estudio de la Forma de los Datos\nEl estudio de la forma de los datos es fundamental en el análisis de datos y la estadística. Nos permite entender la distribución, tendencias, patrones y anomalías presentes en los datos, lo cual es esencial para tomar decisiones informadas y realizar análisis precisos. En esta introducción, exploraremos varios métodos y herramientas para examinar la forma de los datos, utilizando ejemplos en R, un lenguaje de programación muy utilizado en análisis de datos.\n\n\nAnálisis Descriptivo\nEl primer paso para entender la forma de los datos es realizar un análisis descriptivo. Este análisis incluye el cálculo de medidas como la media, mediana, moda, desviación estándar y percentiles, que resumen las características principales de los datos.\n\n# Cargar librería\nlibrary(dplyr)\n\n# Crear un conjunto de datos\ndatos &lt;- c(23, 19, 23, 21, 22, 21, 20, 23, 22, 19)\n\n# Calcular medidas descriptivas\nmedia &lt;- mean(datos)\nmediana &lt;- median(datos)\nmoda &lt;- as.numeric(names(sort(table(datos), decreasing=TRUE)[1]))\ndesviacion_estandar &lt;- sd(datos)\n\n# Mostrar resultados\ncat(\"Media:\", media, \"\\n\")\n\nMedia: 21.3 \n\ncat(\"Mediana:\", mediana, \"\\n\")\n\nMediana: 21.5 \n\ncat(\"Moda:\", moda, \"\\n\")\n\nModa: 23 \n\ncat(\"Desviación Estándar:\", desviacion_estandar, \"\\n\")\n\nDesviación Estándar: 1.567021 \n\n\n\n\nVisualización de Datos\nLa visualización de datos es una herramienta poderosa para estudiar la forma de los datos. Gráficos como histogramas, diagramas de cajas y gráficos de dispersión permiten visualizar la distribución y las relaciones en los datos.\n\n# Cargar librería\nlibrary(ggplot2)\n\n# Crear un histograma\nggplot(data.frame(datos), aes(x = datos)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", alpha = 0.7, color = \"black\") +\n  labs(title = \"Histograma de Datos\", x = \"Valores\", y = \"Frecuencia\")\n\n\n\n\n\n\n\n# Crear un diagrama de cajas\nggplot(data.frame(datos), aes(y = datos)) +\n  geom_boxplot(fill = \"cyan\", color = \"black\") +\n  labs(title = \"Diagrama de Cajas de Datos\", y = \"Valores\")\n\n\n\n\n\n\n\n\n\n\nAnálisis Exploratorio de Datos (EDA)\nEl Análisis Exploratorio de Datos (EDA) implica técnicas gráficas y cuantitativas para descubrir patrones, detectar anomalías y verificar hipótesis. EDA es una fase preliminar clave en el análisis de datos.\n\n# Crear un conjunto de datos de ejemplo\nset.seed(123)\ndatos_eda &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Crear un gráfico de dispersión\nggplot(datos_eda, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Gráfico de Dispersión\", x = \"Variable X\", y = \"Variable Y\")\n\n\n\n\n\n\n\n\n\n\nPruebas Estadísticas\nLas pruebas estadísticas, como la prueba de normalidad (Kolmogorov-Smirnov, Shapiro-Wilk), ayudan a determinar si los datos siguen una distribución específica.\n\n# Prueba de normalidad Shapiro-Wilk\nshapiro_test &lt;- shapiro.test(datos)\n\n# Mostrar resultado\ncat(\"Prueba de Shapiro-Wilk: p-value =\", shapiro_test$p.value, \"\\n\")\n\nPrueba de Shapiro-Wilk: p-value = 0.1400044 \n\n\n\n\nImportancia del Estudio de la Forma de los Datos\nComprender la forma de los datos es crucial para identificar patrones, detectar anomalías, seleccionar modelos apropiados y visualizar datos de manera efectiva. Un buen entendimiento de la forma de los datos permite tomar decisiones más informadas y basadas en evidencia, mejorando la calidad del análisis y la interpretación de los resultados.\nEn resumen, el estudio de la forma de los datos a través del análisis descriptivo, visualización de datos, EDA y pruebas estadísticas, nos proporciona una comprensión profunda de los datos y nos guía en el proceso de análisis. Utilizar herramientas como R facilita este estudio y nos permite trabajar con datos de manera más eficiente y precisa.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La forma de los datos</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html",
    "href": "070-relacion-xy.html",
    "title": "7  La relación entre las variables",
    "section": "",
    "text": "7.1 Diagramas de puntos \\((x,y\\))",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html#correlación",
    "href": "070-relacion-xy.html#correlación",
    "title": "7  La relación entre las variables",
    "section": "7.2 Correlación",
    "text": "7.2 Correlación\n\nCorrelación y causalidad\nAnscombe",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html#tablas-de-asociación",
    "href": "070-relacion-xy.html#tablas-de-asociación",
    "title": "7  La relación entre las variables",
    "section": "7.3 Tablas de asociación",
    "text": "7.3 Tablas de asociación",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "080-comunicacion.html",
    "href": "080-comunicacion.html",
    "title": "8  La comunicación de los resultados",
    "section": "",
    "text": "quarto markdown PowerPoint Gráficos",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>La comunicación de los resultados</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html",
    "href": "090-distr-prob.html",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "",
    "text": "¿Qué es la Probabilidad?\nLa probabilidad es una rama de las matemáticas que se ocupa del estudio de los fenómenos aleatorios y la incertidumbre. Nos ayuda a medir la posibilidad de que ocurran ciertos eventos y a tomar decisiones informadas en situaciones de incertidumbre.",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html#frecuencia-relativa-y-probabilidad",
    "href": "090-distr-prob.html#frecuencia-relativa-y-probabilidad",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "9.1 Frecuencia relativa y probabilidad",
    "text": "9.1 Frecuencia relativa y probabilidad\n\nFrecuencia Relativa\nDefinición: La frecuencia relativa se refiere a la proporción de veces que ocurre un evento en relación con el número total de ensayos o experimentos realizados.\nCálculo: La frecuencia relativa se calcula como: \\[\n\\text{Frecuencia Relativa} = \\frac{\\text{Número de veces que ocurre el evento}}{\\text{Número total de ensayos}}\n\\]\nEjemplo: Si lanzas una moneda 100 veces y obtienes 52 caras, la frecuencia relativa de obtener cara es: \\[\n\\text{Frecuencia Relativa} = \\frac{52}{100} = 0.52\n\\]\nAplicación: La frecuencia relativa se utiliza principalmente en el análisis de datos experimentales y observacionales para estimar la probabilidad empírica de eventos basándose en la evidencia observada.\n\n\nProbabilidad\nDefinición La probabilidad es una medida teórica de la probabilidad de que ocurra un evento, basada en un modelo matemático o un conjunto de supuestos.\nCálculo La probabilidad de un evento \\(A\\), denotada como \\(P(A)\\), se calcula como:\n\\[\nP(A) = \\frac{\\text{Número de casos favorables}}{\\text{Número total de casos posibles}}\n\\]\nEjemplo: Para una moneda justa, la probabilidad de obtener cara es: \\[\nP(\\text{Cara}) = \\frac{1}{2} = 0.5\n\\] Aplicación: La probabilidad se utiliza en teoría de la probabilidad y estadística para predecir la ocurrencia de eventos en base a modelos matemáticos y supuestos teóricos.\n\n\nComparación\n\n\n\n\n\n\n\n\nCaracterística\nFrecuencia Relativa\nProbabilidad\n\n\n\n\nDefinición\nProporción de veces que ocurre un evento en un número de ensayos\nMedida teórica de la probabilidad de que ocurra un evento\n\n\nCálculo\n\\(\\frac{\\text{Número de veces que ocurre el evento}}{\\text{Número total de ensayos}}\\)\n\\(\\frac{\\text{Número de casos favorables}}{\\text{Número total de casos posibles}}\\)\n\n\nBasado en\nDatos experimentales u observacionales\nModelos matemáticos y supuestos teóricos\n\n\nEjemplo\nFrecuencia relativa de obtener cara en 100 lanzamientos de moneda es \\(0.52\\)\nProbabilidad de obtener cara en una moneda justa es \\(0.5\\)\n\n\nAplicación\nEstimación empírica de probabilidades\nPredicción de eventos basados en modelos teóricos\n\n\n\nEn resumen, la frecuencia relativa se basa en datos observados y se utiliza para estimar probabilidades empíricas, mientras que la probabilidad es una medida teórica basada en modelos matemáticos. Ambas son herramientas útiles en el análisis de datos y la toma de decisiones informadas.",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html#distribuciones-de-probabilidad",
    "href": "090-distr-prob.html#distribuciones-de-probabilidad",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "9.2 Distribuciones de probabilidad",
    "text": "9.2 Distribuciones de probabilidad\n\nDistribución normal\ndirtib de frecuencia y distr normal: área bajo la curva\nHow to predict record-shattering weather events | The Economist\n\n\n\nmedia-y-varianza\n\n\nExplained: Sigma | MIT News | Massachusetts Institute of Technology\n\n\n\nimg\n\n\n\n\nOtras distribuciones\n\n\nGráficos de probabilidad",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html#fútbol",
    "href": "090-distr-prob.html#fútbol",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "9.3 Fútbol",
    "text": "9.3 Fútbol",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html#proceso-de-dosificación-de-líquidos",
    "href": "090-distr-prob.html#proceso-de-dosificación-de-líquidos",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "9.4 Proceso de dosificación de líquidos",
    "text": "9.4 Proceso de dosificación de líquidos",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "090-distr-prob.html#ley-de-contenido-efectivo",
    "href": "090-distr-prob.html#ley-de-contenido-efectivo",
    "title": "9  Probabilidad y distribuciones de probabilidad",
    "section": "9.5 Ley de contenido efectivo",
    "text": "9.5 Ley de contenido efectivo",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probabilidad y distribuciones de probabilidad</span>"
    ]
  },
  {
    "objectID": "100-anova.html",
    "href": "100-anova.html",
    "title": "10  El análisis de la varianza",
    "section": "",
    "text": "10.1 Introducción\nSupongamos que un analista de la OMS ha realizado la medida de la altura de un grupo de niños y niñas niña siguiendo rigurosamente el método establecido, y, por lo tanto, que está razonablemente seguro de su resultado.\nAl cabo de varias jornadas de trabajo, habrá realizado varias medidas, que representarán al conjunto de niños de la población en la que ha estado trabajando. Otros investigadores pueden haber estado trabajando a la vez en otras poblaciones, y al final de sus jornadas de trabajo, quieren comparar sus resultados: ¿Hay alguna de estas poblaciones en las que los niños sean significativamente más altos (o más bajos) que en las otras? ¿Cómo describir la altura de un conjunto de individuos de manera que se puedan hacer comparaciones con otros conjuntos?\nPara responder a estas preguntas, vamos a cambiar el entorno de trabajo a un grupo de niños imaginario, que vamos a llamar aula1: son nuestros compañeros y compañeras, a los cuales realizaremos una medida de altura siguiendo el procedimiento especificado en nuestro método. ## Análisis de la varianza de un factor",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>El análisis de la varianza</span>"
    ]
  },
  {
    "objectID": "100-anova.html#análisis-de-la-varianza-de-dos-factores",
    "href": "100-anova.html#análisis-de-la-varianza-de-dos-factores",
    "title": "10  El análisis de la varianza",
    "section": "10.2 Análisis de la varianza de dos factores",
    "text": "10.2 Análisis de la varianza de dos factores",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>El análisis de la varianza</span>"
    ]
  },
  {
    "objectID": "100-anova.html#section",
    "href": "100-anova.html#section",
    "title": "10  El análisis de la varianza",
    "section": "10.3 ",
    "text": "10.3",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>El análisis de la varianza</span>"
    ]
  },
  {
    "objectID": "110-disexp.html",
    "href": "110-disexp.html",
    "title": "11  Diseño de experimentos",
    "section": "",
    "text": "11.1 Experimentos factoriales\n¿La aspirina reduce el riesgo de infarto? ¿Una marca de abono es más eficaz para el cultivo de rosas que otra? ¿El cansancio es tan peligroso para un conductor como la influencia del alcohol? Este tipo de preguntas se responden con experimentos aleatorios.\nEl propósito de un experimento es investigar la relación entre dos o más variables. Cuando una variable provoca un cambio en otra, llamamos a la primera variable la variable independiente o explicativa. La variable afectada se llama variable dependiente o variable de respuesta: estímulo, respuesta. En un experimento aleatorio, el investigador manipula los valores de la variable explicativa y mide los cambios resultantes en la variable de respuesta. Los diferentes valores de la variable explicativa se denominan tratamientos. Una unidad experimental es un único objeto o persona que se va a medir.\nSupongamos que usted quiere investigar la eficacia de la vitamina E en la prevención de enfermedades. Usted recluta a un grupo de sujetos y les pregunta si toman regularmente vitamina E. Observa que los sujetos que toman vitamina E, en promedio, presentan una salud mejor que quienes no la toman. ¿Esto prueba que la vitamina E es eficaz en la prevención de enfermedades? No es así. Hay muchas diferencias entre los dos grupos comparados, además del consumo de vitamina E. Las personas que toman vitamina E con regularidad suelen tomar otras medidas para mejorar su salud: ejercicio, dieta, otros suplementos vitamínicos, elección de no fumar, etc. Cualquiera de estos factores podría estar influyendo en la salud. Como se ha descrito, este estudio no demuestra que la vitamina E sea la clave para la prevención de enfermedades.\nLas variables adicionales que pueden enturbiar un estudio se denominan variables ocultas. Para demostrar que la variable explicativa provoca un cambio en la variable de respuesta, es necesario aislar la variable explicativa. La investigadora debe diseñar su experimento de forma que solo haya una diferencia entre los grupos que se comparan: los tratamientos previstos. Esto se consigue mediante la asignación aleatoria de unidades experimentales a grupos de tratamiento. Cuando los sujetos se asignan a los tratamientos de forma aleatoria, todas las variables ocultas potenciales se reparten por igual entre los grupos. En este punto, la única diferencia entre los grupos es la impuesta por el investigador. Los diferentes resultados medidos en la variable de respuesta, por tanto, deben ser una consecuencia directa de los diferentes tratamientos. De este modo, un experimento puede demostrar una conexión causa-efecto entre las variables explicativas y las de respuesta.\nEl poder de la sugestión puede tener una importante influencia en el resultado de un experimento. Los estudios han demostrado que la expectativa del participante en el estudio puede ser tan importante como el medicamento real. En un estudio sobre fármacos que mejoran el desempeño, los investigadores señalaron:\nCuando la participación en un estudio provoca una respuesta física del participante, es difícil aislar los efectos de la variable explicativa. Para contrarrestar el poder de la sugestión, los investigadores reservaron un grupo de tratamiento como grupo de control . Este grupo recibe un tratamiento placebo, es decir, un tratamiento que no puede influir en la variable de respuesta. El grupo de control ayuda a los investigadores a equilibrar los efectos de estar en un experimento con los efectos de los tratamientos activos. Por supuesto, si usted participa en un estudio y sabe que está recibiendo una píldora que no contiene ningún medicamento real, entonces el poder de la sugestión ya no es un factor. Que un experimento aleatorio sea ciego preserva el poder de la sugestión. Cuando una persona participa en un estudio de investigación ciego, no sabe quién recibe el tratamiento activo y quién el placebo. Un experimento doble ciego es aquel en el que tanto los sujetos como los investigadores que participan en él no conocen la información del fármaco.\nNo un factyor de cada vez - explciar interaccion",
    "crumbs": [
      "Conceptos avanzados",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html",
    "href": "120-control-proc.html",
    "title": "12  El control estadístico de procesos",
    "section": "",
    "text": "12.1 La mejora de la calidad y el control estadístico de procesos\nEl control estadístico de la calidad empezó con W.E. Deming a mediados del pasado siglo XX, y fueron las empresas japonesas, sobre todo las automovilísticas, las que inicialmente recogieron el testigo de Deming y aplicaron estos principios a la mejora de la producción industrial, difundiendo su conocimiento a todos los niveles jerárquicos de las organizaciones, desde los operarios de línea hasta los más altos directivos. Desde ese momento hasta la difusión actual de los métodos Six Sigma gracias a General Electric y Motorola, la mejora industrial de los procesos ha estado siempre apoyada en la correcta utilización de estas metodologías.",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#introducción-a-los-gráficos-de-control",
    "href": "120-control-proc.html#introducción-a-los-gráficos-de-control",
    "title": "12  El control estadístico de procesos",
    "section": "12.2 Introducción a los gráficos de control",
    "text": "12.2 Introducción a los gráficos de control\n\nCausas comunes y causas especiales de variación\n\n\nVariación a corto plazo y variación a largo plazo",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#la-capacidad-de-un-proceso",
    "href": "120-control-proc.html#la-capacidad-de-un-proceso",
    "title": "12  El control estadístico de procesos",
    "section": "12.3 La capacidad de un proceso",
    "text": "12.3 La capacidad de un proceso",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#ejemplo-establecer-las-especificaciones-de-un-producto",
    "href": "120-control-proc.html#ejemplo-establecer-las-especificaciones-de-un-producto",
    "title": "12  El control estadístico de procesos",
    "section": "12.4 Ejemplo: establecer las especificaciones de un producto",
    "text": "12.4 Ejemplo: establecer las especificaciones de un producto",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html",
    "href": "130-sistema-medicion.html",
    "title": "13  El análisis del sistema de medición",
    "section": "",
    "text": "13.1 ¿Qué es una medida?\nUna medida es el resultado de la acción de medir. Normalmente, medir quiere decir comparar lo que va a ser medido con un patrón de referencia; esta comparación es realizada por una o varias personas que llamaremos analistas, los cuales utilizarán un método analítico, siguiendo un procedimiento de medida. Por ejemplo, en la imagen a continuación, la analista está midiendo la altura de un niño utilizando un instrumento de medida, una cinta métrica.\nHasta aquí parece que todo está suficientemente claro, por lo que el resultado de la medida debe ser un valor que no nos ofrecerá dudas sobre su veracidad. Sin embargo, si analizamos el proceso con atención, veremos que hay algunos elementos que pueden hacer que nuestro resultado no sea todo lo preciso que habíamos pensado. Por ejemplo, en la imagen no vemos si el niño está calzado o no. Es evidente que deberíamos decir al analista que el niño debe estar descalzo, ya que diferentes tipos de calzado podrían alterar el resultado de formas diferentes. EL niño podría estar ligeramente encorvado, o sus rodillas dobladas, y entonces la altura que estamos midiendo será menor de la altura real. Además de esto, la forma de realizar la evaluación de la altura se ve influenciada por la posición de los ojos del analista: si están demasiado bajos, no verá correctamente la parte superior de la cabeza y tenderá a sobreestimar el verdadero valor por un efecto de perspectiva. Por otra parte, no sabemos si el instrumento utilizado tiene una escala de medida construida de forma fiable o sólo aproximada. Si nuestro instrumento (la cinta métrica) no es fiable, o su escala difiere de la de otros instrumentos semejantes, es posible que el valor de la medida varíe.",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#qué-es-una-medida",
    "href": "130-sistema-medicion.html#qué-es-una-medida",
    "title": "13  El análisis del sistema de medición",
    "section": "",
    "text": "Cinta métrica de sastre\n\n\n\n\nMetro de albañilería",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#los-estándares-de-medida",
    "href": "130-sistema-medicion.html#los-estándares-de-medida",
    "title": "13  El análisis del sistema de medición",
    "section": "13.2 Los estándares de medida",
    "text": "13.2 Los estándares de medida\nPor lo que hemos visto, cuando hacemos una medida, debemos establecer un procedimiento de medida, que debe indicar al analista cual es la forma correcta de realizar los pasos para hacer que la medida sea veraz. Deberemos definir también el instrumento de medida, de manera que cuando se repita el procedimiento no se introduzca un factor de variación debido al uno de un instrumento inapropiado. LO mejor es que este instrumento disponga de una homologación por un servicio de homologación externo, que nos asegure, por ejemplo, que los intervalos de medida de que dispone se corresponden con valores de referencia, en este caso, centímetros y milímetros. El procedimiento debe establecer también con claridad las condiciones en que las debe estar el objeto a medir (la persona, en este caso): descalzo, perfectamente estirado, con sus rodillas rectas, etc. Seguramente, el procedimiento incluirá un dibujo para que el analista visualice con claridad los puntos claves que debe revisar para hacer una buena medida. En el dibujo a continuación, se indican algunos de estos puntos claves, incluyendo la necesidad de que el niño se apoye en un plano vertical (la pared) y que el analista se situe correctamente para que su vista sea perpendicular al plano, utilizando una guía para la valoración correcta de la altura medida.\n\nPero ¿y si la niña tiene una altura tal que el analista no puede mantener una posición estable? El procedimiento real puede llegar a ser mucho más complejo, tal como vemos en el último gráfico, que proviene de un documento médico de la Organización Mundial de la Salud, en donde la correcta estimación del peso y altura de los niños es fundamental para determinar su estado de salud nutricional, y por lo tanto es necesario minimizar el riesgo de errores de medida, garantizando que todos los analistas realizan correctamente el mismo procedimiento aunque estén en diferentes ubicaciones y en momentos diferentes:\n\nEL procedimiento de la OMS ha estimado que es necesario que sean dos los analistas que realizan la medida, utilizando un aparato especialmente diseñado para ello, y que utiliza una pieza para ajustar a la cabeza de manera que la posición de lectura no esté sometida al error de la posición del analista que realiza la medida.\nLa descripción del procedimiento de análisis, junto con el detalle de los instrumentos necesarios y sus homologaciones requeridas, constituye lo que se conoce como método analítico. El analista o analistas deberán estudiar este método analítico para estar seguros de que son capaces de llevarlo a la práctica sin error.\nVeremos en un apartado posterior que cada uno de estos elementos dan lugar a un tipo de error concreto, que son principalmente dos: los debidos al analista y los debidos al procedimiento (incluyendo aquí los errores instrumentales) Veremos cómo evaluar la magnitud de cada uno de estos errores y la forma de establecer un plan de trabajo para reducirlos, mejorando así la calidad de nuestras medidas.",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#la-precisión-analítica",
    "href": "130-sistema-medicion.html#la-precisión-analítica",
    "title": "13  El análisis del sistema de medición",
    "section": "13.3 La precisión analítica",
    "text": "13.3 La precisión analítica",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#análisis-de-repetibilidad-y-reproducibilidad-grr-analysis",
    "href": "130-sistema-medicion.html#análisis-de-repetibilidad-y-reproducibilidad-grr-analysis",
    "title": "13  El análisis del sistema de medición",
    "section": "13.4 Análisis de repetibilidad y reproducibilidad (GR&R analysis)",
    "text": "13.4 Análisis de repetibilidad y reproducibilidad (GR&R analysis)\n\nRound only on the final calculation result[edit]\nWhen performing multiple stage calculations, do not round intermediate stage calculation results; keep as many digits as is practical (at least one more digit than the rounding rule allows per stage) until the end of all the calculations to avoid cumulative rounding errors while tracking or recording the significant figures in each intermediate result. Then, round the final result, for example, to the fewest number of significant figures (for multiplication or division) or leftmost last significant digit position (for addition or subtraction) among the inputs in the final calculation.[15]\n\n(2.3494 + 1.345) × 1.2 = 3.6944 × 1.2 = 4.43328 ≈ 4.4.\n(2.3494 × 1.345) + 1.2 = 3.159943 + 1.2 = 4.359943 ≈ 4.4.",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#precision-of-measuring-tools-and-significant-figures",
    "href": "130-sistema-medicion.html#precision-of-measuring-tools-and-significant-figures",
    "title": "13  El análisis del sistema de medición",
    "section": "13.5 Precision of Measuring Tools and Significant Figures",
    "text": "13.5 Precision of Measuring Tools and Significant Figures\nAccuracy, Precision, and Significant Figures | Physics (lumenlearning.com)\nAn important factor in the accuracy and precision of measurements involves the precision of the measuring tool. In general, a precise measuring tool is one that can measure values in very small increments. For example, a standard ruler can measure length to the nearest millimeter, while a caliper can measure length to the nearest 0.01 millimeter. The caliper is a more precise measuring tool because it can measure extremely small differences in length. The more precise the measuring tool, the more precise and accurate the measurements can be.\nWhen we express measured values, we can only list as many digits as we initially measured with our measuring tool. For example, if you use a standard ruler to measure the length of a stick, you may measure it to be 36.7 cm. You could not express this value as 36.71 cm because your measuring tool was not precise enough to measure a hundredth of a centimeter. It should be noted that the last digit in a measured value has been estimated in some way by the person performing the measurement. For example, the person measuring the length of a stick with a ruler notices that the stick length seems to be somewhere in between 36.6 cm and 36.7 cm, and he or she must estimate the value of the last digit. Using the method of significant figures, the rule is that the last digit written down in a measurement is the first digit with some uncertainty. In order to determine the number of significant digits in a value, start with the first measured value at the left and count the number of digits through the last digit written on the right. For example, the measured value 36.7cm has three digits, or significant figures. Significant figures indicate the precision of a measuring tool that was used to measure a value.\n\nLa “mano del quesero”\nComparar pros y contras de la práctica basada en la experiencia con la práctica basada ne el método y la cuantificación\n\nsubjetividad\npérdida de conocimiento si el experto deja la empresa\n\n\n\nMétodo cientifico\nAlgoritmos - recetas cocina- DMAIC - método cientifico\nMontgomery 1.1\nLa reproducibilidad de los análisis de datos\nLiterate programming - Wikipedia\nReproducible Research (hbiostat.org)\nrr (hbiostat.org)\nEn el mundo científico y técnico cada vez cobra más importancia el concepto de reproducibilidad de los análisis, sobre todo cuando se trata de comunicar o publicar el resultado de un trabajo o de una investigación. Medios, como la prestigiosa revista Science, se han hecho eco de ello (Buck 2015). Por otra parte, la utilización de un flujo de trabajo basado en hojas de cálculo hace difícil garantizar esta reproducibilidad, y a veces puede llevar a cometer errores de consecuencias graves (Ferrero 2018; Ryssdal 2013).\nJesse Sadler (Sadler 2017) lo explica así:\nEl peligro de la hoja de cálculo deriva de su propia estructura. La mezcla de entrada de datos, análisis y visualización hace que sea fácil confundir las celdas que contienen datos sin procesar con las que son el resultado del análisis. La forma de definir la lógica programática, tal como la selección de qué celdas se van a sumar, mediante clics del mouse, significa que una acción errónea de clic o arrastre puede provocar errores o la sobreescritura de datos. Solo hace falta pensar en el pavor del momento en el que vas a cerrar una hoja de cálculo y el programa te pregunta si te gustaría guardar los cambios. Te hace preguntarte. ¿Quiero guardar? ¿Qué cambios hice? Debido a que la lógica en una hoja de cálculo se realiza a través de clics del mouse, no hay forma de rastrear de manera efectiva qué cambios se han realizado en una sesión o en la producción de un gráfico. Los errores cometidos con Excel pueden tener consecuencias graves, como se puso de manifiesto tras la controversia alrededor del artículo de Carmen Reinhart y Kenneth Rogoff sobre la deuda nacional de los EEUU.\nCiertamente hay razones legítimas por las que las personas usan por defecto hojas de cálculo para el análisis de datos en lugar de usar un lenguaje de programación como R. Las hojas de cálculo son mucho más atractivas y confortables de lo que cualquier lenguaje de programación podría ser para un recién llegado. Aprender a programar es intimidante y no es algo que se pueda hacer rápida o fácilmente. Las aplicaciones de interfaz gráfica de usuario (GUI) son mucho menos desalentadoras que una interfaz de línea de comandos. En segundo lugar, las hojas de cálculo son una buena herramienta para la entrada de datos, y es tentador pasar directamente al análisis de datos, manteniendo todo en el mismo documento. Finalmente, la naturaleza interactiva de las hojas de cálculo y la capacidad de crear gráficos que cambian en función de las entradas es muy atractiva, incluso si desbloquear completamente este potencial implica un conocimiento bastante complejo sobre cómo funciona el programa. La primera ventaja de las hojas de cálculo sobre la programación no se supera fácilmente, pero las dos últimas se basan en lo que creo que es un flujo de trabajo problemático. En lugar de usar un par de aplicaciones monolíticas, a menudo un conjunto de aplicaciones de oficina, para hacer todo, creo que es mejor dividir el flujo de trabajo entre varias aplicaciones que hacen una cosa bien.\nCrear una división clara entre la entrada y el análisis de datos es una de las principales razones por las que el análisis de datos en un lenguaje de programación es preferible al software de hoja de cálculo. Todavía uso hojas de cálculo, pero su limito su uso estrictamente a la entrada de datos. En un programa de hoja de cálculo, el análisis manipula directamente la única copia de los datos sin procesar. Por el contrario, con R se importan los datos, creando un objeto que es una copia de los datos sin procesar. Todas las manipulaciones de los datos se realizan en esta copia, y los datos originales nunca se alteran de ninguna manera. Esto significa que no hay forma de estropear los datos sin procesar. La manipulación de una copia de los datos le permite experimentar más libremente. Los errores son intrascendentes, incluso aunque a veces puedan llegar a ser frustrantes. Una línea de código que devuelve un error se puede ajustar y volver a ejecutar, repitiendo el proceso las veces necesarias hasta que se devuelva el resultado esperado.\nTrabajar en una copia de los datos sin procesar puede incluso simplificar el proceso de entrada de datos. El análisis de datos tabulares en R da como resultado la creación de múltiples objetos, que se conocen como data frames y pueden considerarse equivalentes a tablas en una hoja de cálculo. La capacidad de dividir, muestrear y transformar el conjunto de datos original en muchos data frames diferentes tiene la ventaja de reducir drásticamente la complejidad de la entrada de datos. En lugar de necesitar hojas de cálculo a medida con múltiples hojas y tablas interrelacionadas, cada pieza de datos solo debe ingresarse una vez y todas las manipulaciones se pueden realizar en el código. Los diferentes data frames que se crean en el proceso de análisis ni siquiera tienen que ser guardados, porque son muy fácilmente reproducidos por el script de código.\nLa separación de la entrada y el análisis de los datos reduce en gran manera el potencial de errores, pero tal vez aún más significativamente, el uso de código para el análisis de datos permite la creación de investigaciones reproducibles que no son posibles en hojas de cálculo. […] Con un lenguaje de programación, los pasos del análisis se pueden establecer claramente en el código […] Guardar el análisis en código tiene el beneficio inmediato de que se puede volver a ejecutar fácilmente en cualquier momento que se agreguen nuevos datos. El código también se puede aplicar a un conjunto de datos completamente nuevo de una manera mucho más transparente que con las hojas de cálculo. El beneficio a largo plazo es que con el código todo el análisis se documenta en lugar de ocultarse detrás de los clics del mouse. Esto hace que sea más fácil revisar los propios análisis mucho después de haber terminado con ellos, así como que otros entiendan lo que se ha hecho y comprueben si hay errores.\n\n\n\n\nBuck, Stuart. 2015. “Solving Reproducibility.” Science 348 (6242): 1403. https://www.science.org/doi/full/10.1126/science.aac8041.\n\n\nFerrero, Rosana. 2018. “Los Errores de Reinhart & Rogoff: R y La Reproducibilidad.” 2018. https://www.maximaformacion.es/blog-dat/los-errores-de-reinhart-rogo/.\n\n\nRyssdal, Karl. 2013. “The Excel Mistake Heard Round the World.” 2013. https://www.marketplace.org/2013/04/17/economy/excel-mistake-heard-round-world/.\n\n\nSadler, Jesse. 2017. “Excel Vs r: A Brief Introduction to r.” 2017. https://www.jessesadler.com/post/excel-vs-r/.",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html",
    "href": "140-mejora-procesos.html",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "",
    "text": "14.1 Six Sigma y mejora de la calidad",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#definir-un-problema-opex-lean-sixsigma-16",
    "href": "140-mejora-procesos.html#definir-un-problema-opex-lean-sixsigma-16",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.2 Definir un problema [OPEX Lean SixSigma #16]",
    "text": "14.2 Definir un problema [OPEX Lean SixSigma #16]\n\n¿Cómo es una buena definición de un problema?\n\nBreve\nEvitar lenguaje técnico: debes ser capaz de explicarlo a cualquier persona de la organización usando términos sencillos\nCuantificar el problema, usando los datos disponibles\nIntegra y explica el coste real del problema, para justificar la necesidad del análisis. Puedes relacionarlo con los costes de no calidad\nDefine el ámbito del problema: usa los términos que sean necesarios para delimitarlo con precisión.\nLa definición de un problema debe conseguir formularlo de forma que sea específico, medible, realizable, relevante y acotado en el tiempo (plazo)",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#estrategia-de-resolucion-de-problemas",
    "href": "140-mejora-procesos.html#estrategia-de-resolucion-de-problemas",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.3 Estrategia de resolucion de problemas",
    "text": "14.3 Estrategia de resolucion de problemas",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#dmaic---sixsigma",
    "href": "140-mejora-procesos.html#dmaic---sixsigma",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.4 DMAIC - SixSigma",
    "text": "14.4 DMAIC - SixSigma",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#el-papel-de-los-métodos-estadísticos-en-la-mejora-six-sigma",
    "href": "140-mejora-procesos.html#el-papel-de-los-métodos-estadísticos-en-la-mejora-six-sigma",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.5 El papel de los métodos estadísticos en la mejora Six Sigma",
    "text": "14.5 El papel de los métodos estadísticos en la mejora Six Sigma",
    "crumbs": [
      "Aplicaciones prácticas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "150-anexo-R.html",
    "href": "150-anexo-R.html",
    "title": "Instalación de R y conceptos básicos",
    "section": "",
    "text": "Instalación de R en Windows",
    "crumbs": [
      "Anexos",
      "Instalación de R y conceptos básicos"
    ]
  },
  {
    "objectID": "150-anexo-R.html#instalación-de-r-en-windows",
    "href": "150-anexo-R.html#instalación-de-r-en-windows",
    "title": "Instalación de R y conceptos básicos",
    "section": "",
    "text": "Accede al sitio oficial de CRAN (Comprehensive R Archive Network), que es el repositorio central de software de R⁴.\nHaz clic en el enlace “Download R for Windows”.\nSelecciona “install R for the first time” en la parte superior de la página.\nElige la versión de R que deseas instalar (por ejemplo, R 4.0.3).\nGuarda el archivo ejecutable en tu computadora (puedes guardarlo en el escritorio).\nHaz doble clic en el archivo descargado para ejecutarlo.\nAcepta los permisos para que la aplicación realice cambios en tu dispositivo.\nSelecciona el idioma de tu preferencia y sigue las opciones de instalación (puedes aceptar las opciones preestablecidas).",
    "crumbs": [
      "Anexos",
      "Instalación de R y conceptos básicos"
    ]
  },
  {
    "objectID": "150-anexo-R.html#instalación-de-rstudio-en-windows",
    "href": "150-anexo-R.html#instalación-de-rstudio-en-windows",
    "title": "Instalación de R y conceptos básicos",
    "section": "Instalación de RStudio en Windows",
    "text": "Instalación de RStudio en Windows\n\nAbre tu navegador y dirígete al sitio oficial de RStudio.\nHaz clic en “DOWNLOAD”.\nBusca la opción “RStudio Desktop” y selecciona “DOWNLOAD”.\nHaz clic en “DOWNLOAD RSTUDIO FOR WINDOWS”.\nGuarda el archivo ejecutable.\nEjecuta el archivo descargado y sigue las instrucciones de instalación.",
    "crumbs": [
      "Anexos",
      "Instalación de R y conceptos básicos"
    ]
  },
  {
    "objectID": "150-anexo-R.html#introducción-a-r",
    "href": "150-anexo-R.html#introducción-a-r",
    "title": "Instalación de R y conceptos básicos",
    "section": "Introducción a R",
    "text": "Introducción a R\n\nTipos de datos en R\n\nNumérico (numeric):\n\nRepresenta valores decimales, tanto enteros como de punto flotante (double).\nEjemplo: 3.14, 42, 0.5.\n\nEntero (integer):\n\nRepresenta números enteros.\nEjemplo: 5, -10, 100.\n\nLógico (logical):\n\nRepresenta valores booleanos: TRUE o FALSE.\nEjemplo: TRUE, FALSE.\n\nCarácter (character):\n\nRepresenta cadenas de texto.\nEjemplo: \"Hola, mundo\", \"R es genial\".\n\nComplejo (complex):\n\nRepresenta números complejos con parte real e imaginaria.\nEjemplo: 1 + 2i, 3 - 4i.\n\nRaw:\n\nRepresenta datos en formato binario sin procesar.\nEjemplo: as.raw(0:5) (crea un vector de bytes).\n\n\n\n\nVectores\nEn R, un vector es una estructura de datos fundamental que almacena elementos del mismo tipo, ya sean números, caracteres o lógicos². Aquí tienes una breve explicación:\n\n¿Qué es un vector en R?\n\nUn vector es una secuencia de elementos de datos del mismo tipo básico.\nLos miembros de un vector se llaman oficialmente componentes.\nPueden ser de dos tipos: vectores atómicos y listas².\n\n\nLos vectores atómicos son los más comunes y se utilizan para almacenar datos homogéneos. Algunos ejemplos de vectores atómicos incluyen:\n\nNuméricos (numeric):\n\nRepresentan valores decimales, tanto enteros como de punto flotante.\nEjemplo: c(3.14, 42, 0.5).\n\nCaracteres (character):\n\nAlmacenan cadenas de texto.\nEjemplo: c(\"Hola\", \"Mundo\").\n\nLógicos (logical):\n\nContienen valores booleanos: TRUE o FALSE.\nEjemplo: c(TRUE, FALSE, TRUE).\n\nEnteros (integer):\n\nRepresentan números enteros.\nEjemplo: c(5, -10, 100).\n\n\nLos vectores son fundamentales para realizar operaciones matemáticas, análisis de datos y manipulación de información en R.\n\nEjemplos de creación de vectores en R\nLos vectores en R pueden contener datos del mismo tipo y se crean utilizando la función c().\n\nVectores numéricos:\n\nPuedes crear un vector numérico utilizando la función c():\nnumeros &lt;- c(1, 2, 3, 4, 5)\nEsto crea un vector con los números del 1 al 5.\n\nVectores de caracteres:\n\nPara crear un vector de caracteres, simplemente proporciona las cadenas de texto entre comillas:\nfrutas &lt;- c(\"manzana\", \"banana\", \"cereza\")\nEsto crea un vector con los nombres de algunas frutas.\n\nVectores lógicos:\n\nLos vectores lógicos contienen valores TRUE o FALSE. Puedes crear uno así:\nlogico &lt;- c(TRUE, FALSE, TRUE)\nEsto crea un vector con tres valores lógicos.\n\nVectores con nombres:\n\nPuedes asignar nombres a los elementos de un vector:\nmi_vector &lt;- c(naranja = 4, manzana = 6)\nEsto crea un vector con dos elementos nombrados: “naranja” y “manzana”.\n\nData frames:\n\nIntroduce los data frames como tablas de datos con filas y columnas.\nExplica cómo crear, acceder y modificar data frames.\nDestaca la importancia de los data frames para el análisis de datos.\n\n\nUn data frame en R es una estructura de datos bidimensional que se utiliza para almacenar información tabular. Es similar a una tabla o una hoja de cálculo, donde cada columna puede contener diferentes tipos de datos (números, cadenas de texto, etc.). Los data frames son muy útiles para trabajar con datos estructurados en R. Puedes acceder a las columnas y realizar análisis estadísticos sobre ellos.\nAquí tienes una descripción más detallada y algunos ejemplos:\n\nConcepto de Data Frame:\n\nUn data frame es una estructura especializada de tipo lista en R.\nCada componente del data frame tiene la misma longitud y forma una columna.\nLos componentes individuales forman las filas del data frame.\nPuedes pensar en un data frame como una matriz donde cada columna puede contener diferentes tipos de datos¹.\n\nEjemplo de Creación de Data Frame desde Vectores:\n\nPuedes crear un data frame utilizando la función data.frame().\nPor ejemplo, consideremos los siguientes datos:\nName &lt;- c(\"Jon\", \"Bill\", \"Maria\", \"Ben\", \"Tina\")\nAge &lt;- c(23, 41, 32, 58, 26)\ndf &lt;- data.frame(Name, Age)\nprint(df)\nEsto creará un data frame con dos columnas: “Name” y “Age”. Los valores coincidirán con los datos proporcionados.\n\nEjemplo de Creación de Data Frame desde una Matriz:\n\nTambién puedes crear un data frame a partir de una matriz existente.\nSupongamos que tenemos una matriz llamada my_matrix:\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), ncol = 2)\ndf_from_matrix &lt;- data.frame(my_matrix)\nprint(df_from_matrix)\nEsto convertirá la matriz en un data frame con dos columnas.\n\nEjemplo de Creación de Data Frame desde Valores Iniciales:\n\nSi deseas crear un data frame con valores específicos, puedes hacerlo directamente:\ndf_custom &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Carol\"),\n  Age = c(30, 25, 28)\n)\nprint(df_custom)\nEsto creará un data frame personalizado con los valores proporcionados.\n\nEjemplo de Creación de Data Frame Vacío con Nombres de Columna:\n\nSi necesitas un data frame vacío con nombres de columna, puedes hacerlo así:\nempty_df &lt;- data.frame(Name = character(0), Age = numeric(0))\nprint(empty_df)\nEsto crea un data frame sin filas pero con las columnas “Name” y “Age”.\n\n\n\n\n\nCreación de un dataframe a partir de un fichero CSV\ncrear un data frame en R a partir de la lectura de un archivo CSV es una tarea común. Aquí tienes los pasos para hacerlo:\n\nLeer el archivo CSV:\n\nPrimero, necesitas tener un archivo CSV con los datos que deseas cargar en un data frame.\nUtiliza la función read.csv() para leer el archivo CSV y convertirlo en un data frame. Por ejemplo:\nmi_data_frame &lt;- read.csv(\"ruta/al/archivo.csv\")\nReemplaza \"ruta/al/archivo.csv\" con la ubicación real de tu archivo CSV.\n\nExplorar el data frame:\n\nUna vez que hayas leído el archivo, puedes explorar el contenido del data frame utilizando funciones como head(mi_data_frame) para ver las primeras filas o summary(mi_data_frame) para obtener estadísticas resumidas.\n\nAcceder a los datos:\n\nPuedes acceder a las columnas del data frame utilizando el operador $. Por ejemplo:\nprimera_columna &lt;- mi_data_frame$NombreColumna\n\nManipular los datos:\n\nPuedes realizar operaciones, filtrar filas y modificar los valores en el data frame según tus necesidades.\n\nGuardar cambios:\n\nSi realizas modificaciones en el data frame, puedes guardar los cambios en un nuevo archivo CSV utilizando la función write.csv():\nwrite.csv(mi_data_frame, \"ruta/nuevo_archivo.csv\", row.names = FALSE)\nEsto creará un nuevo archivo CSV con los datos actualizados.\n\n\nRecuerda adaptar los nombres de las columnas y las rutas de los archivos según tu caso específico.\n\nVisualización de datos:\n\nMenciona la importancia de la visualización en el análisis estadístico.\nSi hay tiempo, muestra cómo crear gráficos básicos con R (histogramas, scatter plots, etc.).\n\n\n¡Por supuesto! La visualización de datos es fundamental en el análisis de datos, y R ofrece varias opciones para crear gráficos. Aquí te presento una breve descripción y ejemplos de cómo utilizar tanto el programa base como la biblioteca ggplot2 del paquete tidyverse:\n\nPrograma Base de R:\n\nEl programa base de R proporciona funciones para crear gráficos básicos directamente desde los datos.\nAlgunos ejemplos de gráficos básicos son:\n\nGráfico de dispersión:\n# Crear un gráfico de dispersión\nplot(mpg$displ, mpg$hwy, main = \"Consumo de combustible\", xlab = \"Desplazamiento\", ylab = \"Millas por galón\")\nHistograma:\n# Crear un histograma\nhist(mpg$hwy, main = \"Distribución de millas por galón\", xlab = \"Millas por galón\")\nGráfico de barras:\n# Crear un gráfico de barras\nbarplot(table(mpg$class), main = \"Distribución de clases de vehículos\", xlab = \"Clase\", ylab = \"Frecuencia\")\n\n\nBiblioteca ggplot2:\n\nggplot2 es una poderosa biblioteca para crear gráficos basada en la “Gramática de Gráficos”.\nAquí tienes un ejemplo de cómo crear un gráfico de dispersión utilizando ggplot2:\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  labs(title = \"Consumo de combustible\", x = \"Desplazamiento\", y = \"Millas por galón\")\nEn este ejemplo, aes() define las variables estéticas (mapeo de datos a elementos visuales), geom_point() agrega los puntos al gráfico y labs() establece etiquetas para el título y los ejes.\n\nVentajas de ggplot2:\n\nDeclarativo: Describe lo que deseas visualizar y ggplot2 se encarga de los detalles.\nCapas: Puedes agregar capas (geometrías, escalas, facetas) para personalizar tus gráficos.\nEcosistema rico: ggplot2 tiene muchas extensiones y opciones para gráficos más avanzados¹.\n\n\nRecuerda que ggplot2 es especialmente útil para crear gráficos más complejos y personalizados. Si tienes más preguntas, no dudes en preguntar. 😊\nLa visualización de datos es una faceta esencial en el campo de la estadística industrial. Permite convertir conjuntos de datos complejos en representaciones visuales accesibles y fáciles de interpretar. Aquí te explico por qué es tan relevante:\n\nComunicación efectiva:\n\nLa visualización gráfica permite comunicar hallazgos y resultados de manera más clara y efectiva.\nLos gráficos y diagramas facilitan la comprensión de patrones, tendencias y relaciones entre variables.\n\nIdentificación de patrones:\n\nAl visualizar datos, es más sencillo detectar patrones ocultos o anómalos.\nLos gráficos pueden revelar información valiosa sobre procesos industriales, como fluctuaciones en la producción o tendencias en la calidad del producto.\n\nToma de decisiones informada:\n\nLos líderes y analistas industriales pueden tomar decisiones basadas en evidencia visual.\nLa visualización ayuda a evaluar el rendimiento de la producción, identificar áreas de mejora y optimizar procesos.\n\n\nEn la estadística industrial, la visualización de datos es crucial para comprender patrones, tendencias y relaciones en los procesos de producción. R, como lenguaje estadístico, ofrece varias herramientas poderosas para crear gráficos y representaciones visuales. A continuación, menciono algunas de las más útiles:\n\nggplot2:\n\nggplot2 es una biblioteca ampliamente utilizada para la visualización de datos en R.\nSe basa en la “Gramática de Gráficos”, lo que permite crear gráficos personalizados y versátiles.\nEjemplo de uso:\nlibrary(ggplot2)\nggplot(datos, aes(x = variable1, y = variable2)) +\n  geom_point() +\n  labs(title = \"Relación entre dos variables\")\nEn este ejemplo, aes() define las variables estéticas, geom_point() agrega puntos al gráfico y labs() establece etiquetas para el título y los ejes².\n\nPaquete base de R:\n\nR tiene funciones básicas para gráficos, como plot, hist, boxplot, entre otras.\nEstas funciones son parte del paquete base y son útiles para gráficos simples.\nEjemplo de uso:\nplot(datos$variable1, datos$variable2, main = \"Gráfico de dispersión\")\nEsto crea un gráfico de dispersión entre dos variables¹.\n\nGalería de gráficos de R:\n\nLa Galería de gráficos de R es una colección de ejemplos reproducibles creados en R.\nMuestra cientos de gráficos con su código disponible para aprender y adaptar.\nPuedes explorar diferentes tipos de gráficos y encontrar inspiración para tus propios análisis¹.\n\n\n\n\nLas funciones en R\nLas funciones en R son bloques de código que realizan tareas específicas y se pueden reutilizar en diferentes partes de un programa. Aquí tienes una breve descripción y algunos ejemplos:\n\nSintaxis Básica de una Función en R:\n\nPara crear una función en R, utilizamos la siguiente sintaxis:\nnombre_funcion &lt;- function(arg1, arg2, ...) {\n  # Código\n}\n\nnombre_funcion: El nombre que le das a tu función.\narg1, arg2, …: Los argumentos de entrada que la función acepta.\n# Código: El bloque de código que realiza la tarea deseada.\n\n\nEjemplo de Función en R:\n\nSupongamos que queremos calcular el término general de una progresión geométrica.\nCreamos la función an que calcula el término general:\nan &lt;- function(a1, r, n) {\n  a1 * r^(n - 1)\n}\n\na1: Primer término.\nr: Razón o ratio.\nn: Número de términos.\n\nEjemplos de uso:\nan(a1 = 1, r = 2, n = 5)  # Resultado: 16\nan(a1 = 4, r = -2, n = 6)  # Resultado: -128\n\nFunciones Integradas en R:\n\nR tiene muchas funciones incorporadas, como print(), min(), max(), sum(), etc.\nPor ejemplo:\nprint(\"¡Hola, mundo!\")\nmin(1, 2, 3)  # Resultado: 1\nsum(1:5)  # Resultado: 15\nLas funciones en R te permiten modularizar tu código y reutilizarlo.",
    "crumbs": [
      "Anexos",
      "Instalación de R y conceptos básicos"
    ]
  },
  {
    "objectID": "160-bibliografia.html",
    "href": "160-bibliografia.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Buck, Stuart. 2015. “Solving Reproducibility.”\nScience 348 (6242): 1403. https://www.science.org/doi/full/10.1126/science.aac8041.\n\n\nFerrero, Rosana. 2018. “Los Errores de Reinhart & Rogoff: R y\nLa Reproducibilidad.” 2018. https://www.maximaformacion.es/blog-dat/los-errores-de-reinhart-rogo/.\n\n\nHadley Wickham, Garret Grolemund. 2023. “R Para Ciencia de\nDatos.” 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for\nData Science, 2nd Ed. 1005 Gravenstein Highway North, Sebastopol,\nCA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.\n\n\nRyssdal, Karl. 2013. “The Excel Mistake Heard Round the\nWorld.” 2013. https://www.marketplace.org/2013/04/17/economy/excel-mistake-heard-round-world/.\n\n\nSadler, Jesse. 2017. “Excel Vs r: A Brief Introduction to\nr.” 2017. https://www.jessesadler.com/post/excel-vs-r/.\n\n\nWorld Economic Forum. 2023. “Future of Jobs Report, 2023.”\n91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World\nEconomic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2023/.",
    "crumbs": [
      "Anexos",
      "Bibliografía"
    ]
  }
]