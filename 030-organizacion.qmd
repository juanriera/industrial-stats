---
lang: es
---

# La organización de los datos y el flujo de trabajo.Los datos ordenados o _arreglados_ (_tidy data_)

Un **flujo de trabajo en análisis de datos** es un proceso sistemático y estructurado que guía la manipulación, exploración y análisis de datos desde su recolección hasta la obtención de resultados finales y su comunicación. Es una hoja de ruta que asegura que cada paso se realice de manera ordenada, eficiente y reproducible, facilitando la comprensión y utilización de los datos.

Hadley Wickham [@wickham2023] ha propuesto un método de flujo de trabajo que se ha convertido en estándar en la ciencia de datos (hay versión en español: [@wickham2023es])

![](01-imagenes/data-science-explore.svg)

Este flujo de trabajo abarca diversas actividades como la importación de datos, su limpieza y transformación, el análisis exploratorio, y el modelado, culminando en la interpretación y presentación de los resultados. Todo esto se hace siguiendo metodologías específicas para asegurar la calidad y precisión del análisis.

La importancia de seguir un flujo de trabajo bien definido radica en la capacidad de replicar estudios, minimizar errores y fomentar la transparencia, permitiendo que cualquier persona pueda entender y validar las decisiones tomadas durante el análisis. Además, mejora la eficiencia al estandarizar procedimientos y facilita la colaboración entre diferentes analistas o equipos de trabajo.

## Etapas en un flujo de trabajo estructurado.

### Recolección de datos

La primera etapa es la **recolección de datos**. Esto implica obtener datos desde diversas fuentes como archivos CSV, bases de datos, APIs, etc. La recolección de datos es fundamental porque la calidad del análisis depende de la calidad de los datos recolectados.

### Inspección de los datos

Una vez recolectados, se procede a **inspeccionar los datos** para entender su estructura y contenido. Esto incluye examinar los tipos de datos, la presencia de valores faltantes, duplicados y la distribución de las variables.

### Limpieza de los datos

La **limpieza de datos** es crucial para asegurar que la información sea precisa y esté en el formato adecuado. Esta etapa incluye:

- Manejo de valores faltantes.
- Eliminación de duplicados.
- Corrección de inconsistencias.
- Transformación de datos a un formato adecuado para el análisis.

### Transformación de los datos

Transformar los datos a un formato **ordenado o arreglado** (_tidy_) es esencial. Según Wickham, los datos **arreglados** tienen una estructura clara: cada variable es una columna, cada observación es una fila, y cada valor tiene su propia celda. Este formato facilita el análisis y la visualización de datos.

### Análisis exploratorio de datos (EDA)

El **Análisis Exploratorio de Datos (EDA)** busca entender los patrones y relaciones en los datos mediante estadísticas descriptivas y visualizaciones. Durante esta etapa se realizan:

- Estadísticas básicas (media, mediana, desviación estándar).
- Gráficos y diagramas para visualizar la distribución de los datos y las relaciones entre variables.

### Modelado de datos

Dependiendo del objetivo del análisis, se pueden aplicar diversos modelos estadísticos para extraer información y hacer predicciones. Esto puede incluir:

- Modelos de regresión.
- Análisis de clasificación.
- Modelos de series temporales, entre otros.

### Comunicación de resultados

Finalmente, es fundamental **comunicar los resultados** de manera clara y efectiva. Esto se hace a través de:

- Tablas y resúmenes interpretativos.
- Gráficos y visualizaciones.
- Informes y presentaciones que expliquen los hallazgos y sus implicaciones.

Siguiendo estos pasos, puedes manejar y analizar datos de manera organizada y reproducible, facilitando la colaboración y la toma de decisiones informadas. Este flujo de trabajo asegura que los datos se traten de manera sistemática, desde su recolección hasta la comunicación de los resultados.

## Razones para seguir un flujo de trabajo

1. **Reproducibilidad**:
   Un flujo de trabajo organizado permite que los análisis sean reproducibles. Otros pueden seguir los mismos pasos para obtener resultados similares, lo que es crucial en la investigación y en la toma de decisiones basadas en datos.

2. **Consistencia**:
   Ayuda a asegurar que los pasos se realizan de manera consistente cada vez que se ejecuta el análisis, reduciendo la posibilidad de errores humanos.

3. **Transparencia**:
   Proporciona un registro claro de los pasos tomados durante el análisis, facilitando la revisión y validación de los resultados.

4. **Eficiencia**:
   Mejora la eficiencia al estandarizar el proceso, permitiendo a los analistas concentrarse en el análisis y la interpretación de los datos en lugar de tareas repetitivas.

5. **Colaboración**:
   Facilita la colaboración entre equipos, ya que los flujos de trabajo bien documentados permiten que otros comprendan fácilmente los métodos y pasos utilizados.

6. **Adaptabilidad**:
   Permite adaptar y ajustar el análisis de manera más fácil cuando se presentan nuevos datos o cuando cambian los objetivos del análisis.

## Problemas de no seguir un flujo de trabajo estructurado

1. **Errores y Sesgos**:
   La falta de un enfoque estructurado puede resultar en errores y sesgos inadvertidos en el análisis, lo que puede llevar a conclusiones incorrectas.

2. **Dificultad para Replicar Resultados**:
   Sin un flujo de trabajo claro, replicar resultados se vuelve complicado, lo que puede afectar la credibilidad del análisis y la capacidad de validación por otros.

3. **Falta de Documentación**:
   La ausencia de una documentación adecuada dificulta entender los pasos y las decisiones tomadas durante el análisis, lo que puede ser un obstáculo en auditorías y revisiones.

4. **Ineficiencia**:
   Sin una estructura clara, los analistas pueden gastar tiempo valioso realizando tareas repetitivas y resolviendo problemas que podrían haberse evitado con un enfoque más organizado.

5. **Problemas de Colaboración**:
   La colaboración se vuelve más difícil si los miembros del equipo no pueden seguir o entender los pasos tomados por otros, lo que puede llevar a malentendidos y duplicación de esfuerzos.

6. **Dificultad para Adaptarse a Cambios**:
   Sin un flujo de trabajo definido, adaptar el análisis a nuevos datos o cambios en los objetivos puede ser más complejo y propenso a errores.


En resumen, seguir un flujo de trabajo estructurado es esencial para garantizar la precisión, eficiencia, y reproducibilidad del análisis de datos, evitando problemas que puedan comprometer la integridad y utilidad de los resultados. .

## Un ejemplo: revisando los datos existentes.

Cuando nos incorporamos a un equipo de trabajo existente, lo más seguro es que ya se disponga de un sistema de archivo de los datos, de acuerdo con los métodos habituales del equipo. En muchos casos, el diseño de la captura de datos sigue aproximadamente el modelo manual en papel; se introducen los datos en la hoja de cálculo y una vez completados, se imprime el documento para su archivo.

El error más común que se suele cometer es, precisamente, tratar la hoja de cálculo como un bloc de notas, es decir, hacer anotaciones de forma libre, colocar los datos y el resultado de los análisis al lado y en cualquier parte de la hoja, y apoyarnos en el contexto para interpretar lo que hemos guardado. Pero para que el ordenador sea capaz de analizar nuestros datos de manera eficiente, debemos estructurarlos de tal forma que el programa use la información tal como nosotros queremos.

Es común utilizar una hoja para guardar múltiples tablas de datos, tal como vemos en la [@fig-excel_mess]. Esta estructura, sin embargo, resulta enormemente confusa para su análisis, o lo imposibilita completamente.

![Hoja Excel desordenada: ¡No hagas esto!](01-imagenes/excel_mess.png){#fig-excel_mess}

En otros casos, los datos se guardan en hojas de cálculo que se componen de diferentes pestañas para cada semana, cada mes o cada año, como vemos en la @fig-2021-09-1_excel_1. Sin embargo, esta forma de almacenar los datos tampoco es la óptima para su análisis.

![Hoja Excel con una estructura no ordenada](01-imagenes/2021-09-1_excel_1.png){#fig-2021-09-1_excel_1}

Si las diferentes tablas presentan situaciones diferentes, o datos que no están relacionados, podemos utilizar diferentes pestañas. Pero si los datos están vinculados, por ejemplo, se corresponden con las mismas medidas, hechas en fechas diferentes (meses, años), la respuesta es que las pestañas no son la forma correcta de almacenarlos datos; lo mejor es añadir una variable que nos permita diferenciar los datos por fecha; nuestro programa de análisis nos permitirá *filtrar* los datos según la fecha que deseemos, y todos estarán en una única tabla, facilitando la coherencia y el análisis posterior.

Hay muchas formas de almacenar la información en una hoja de cálculo, pero sólo la estructura de **datos ordenados o arreglados** facilita la utilización de los datos tanto por la hoja de cálculo como por otros programas de análisis.

## Los datos ordenados o arreglados (*tidy data*)

De la misma manera que la gramática permite ordenar y estructurar un escrito de acuerdo a reglas comunes, hay reglas para que el almacenamiento de los datos sea lo más homogéneo posible y se reduzcan los errores al mínimo.

Las reglas principales al almacenar nuestros datos en una hoja de cálculo son tres:

-   columnas=variables,

-   filas=observaciones,

-   celdas=valores.

Cada variable debe tener su propia columna, cada observación debe tener su propia fila, y cada valor debe tener su propia celda o casilla .

Estas tres reglas básicas son las que hacen que nuestro conjunto de datos sea **ordenado (o arreglado)**[@wickham2023] (hay edición *online* en español: [@wickham2023es]:

La @fig-tidy muestra estas reglas de forma visual.

![Sigue estas tres reglas para tener un conjunto de datos ordenado: las variables están en columnas, las observaciones están en filas, y los valores están en celdas. Fuente de la imagen: @wickham2023](01-imagenes/tidy-1.png){#fig-tidy}

Estas tres reglas están interrelacionadas porque es imposible satisfacer sólo dos de tres.

En una hoja de cálculo, una tabla de datos arreglada tendría este aspecto:

![Hoja Excel con estructura rectangular de datos ordenados](01-imagenes/2023-11-17.png){#fig-excel-camembert}

Datos rectangulares: formato tabla


## Qué es un fichero plano y un fichero CSV

Se suele llamar **fichero plano** a un fichero de datos de texto sin ningún tipo de formato, donde los datos están separados por espacios o tabulaciones. Muchos equipos automáticos, como balanzas de laboratorio o básculas de proceso, producen ficheros planos de texto, que se pueden importar a Excel o R. Un [fichero CSV](https://es.wikipedia.org/wiki/Valores_separados_por_comas) es un fichero plano en el que los valores están separados por un carácter especial, llamado **separador**. Este separador puede ser una coma `,` (cuando los decimales se separan mediante un punto, como en EEUU) o un punto y coma `;` (cuando los decimales se separan mediante una coma, como en España)

::: {#fig-ficheros-texto layout="[[1,1], [1]]"}

![Fichero plano separado por espacios](01-imagenes/fichero_txt.png)

![Fichero CSV separado por comas (USA, GB)](01-imagenes/fichero-csv-comas.png)

![Fichero CSV separado por puntos y comas (Europa, España)](01-imagenes/fichero-csv-europeo.png)

Tres tipos de ficheros planos de texto.
:::

En un fichero plano o en un fichero CSV, la primera fila puede contener los nombres de las columnas. En algunos casos, los elementos de texto pueden estar entre comillas. En estos casos, los programas de importación se ocupan de la conversión de formatos.

La importación de un fichero CSV en Excel en español es directa si se ha generado con puntos y comas como separador y comas para los decimales; si no es así, nos aparecerá como un fichero plano de texto sin formato, y tendremos que realizar una conversión.

### Cómo exportar un fichero CSV desde Excel a R

Una vez que tenemos nuestros datos en Excel, hay dos formas en las que podemos poner los datos a disposición de R para su análisis: exportarlos a un archivo de texto con **formato CSV**, o leer directamente los datos de Excel desde R utilizando la función `read_excel()` de la librería `tidyverse`. En ambos casos, el resultado en R es un **dataframe** o **cuadro de datos**, que es una estructura equivalente a la de nuestra tabla de datos en Excel.

En el ejemplo, utilizaremos un fichero CSV y lo leeremos utilizando una función de base R.

#### Paso 1: Guardar el Fichero CSV desde Excel

1. Abre tu fichero en Excel.
2. Selecciona **Archivo** > **Guardar como**.
3. Elige el formato **CSV (delimitado por comas) (\*.csv)**.
4. Guarda el archivo.

#### Paso 2: Importar el CSV en R usando Base R


::: { .callout-tip collapse="true" .content-visible when-format="html" icon="false" }
## Explicación del código R

**Línea 1: Lectura del archivo CSV**

```r
datos <- read.csv2("ruta/al/archivo.csv", header = TRUE, sep = ";", dec = ",")
```

1. **`datos <-`**: Este símbolo `<-` se utiliza para asignar el resultado de la función a un objeto llamado `data`. En este caso, `data` contendrá los datos leídos desde el archivo CSV.

2. **`read.csv2(...)`**: Esta es una función en R que se utiliza para leer archivos CSV. La versión `read.csv2` se usa comúnmente en Europa y otros lugares donde el separador de campo estándar es el punto y coma (`;`) y el separador decimal es la coma (`,`).

3. **Parámetros de `read.csv2`**:
   - **`"ruta/al/archivo.csv"`**: Especifica la ruta y el nombre del archivo CSV que deseas leer.
   - **`header = TRUE`**: Indica que el archivo CSV tiene una fila de encabezado que contiene los nombres de las columnas.
   - **`sep = ";"`**: Especifica que el separador de campos en el archivo CSV es el punto y coma (`;`).
   - **`dec = ","`**: Indica que el separador decimal en el archivo CSV es la coma (`,`).

**Línea 2: Mostrar las primeras filas del dataframe**

```r
head(data)
```

- **`head(datos)`**: Esta función muestra las primeras 6 filas del dataframe `data` (puedes cambiar el número de filas que muestra añadiendo un número como argumento: `head(data, 10)` mostrará las primeras 10 filas). Esta es una forma útil de inspeccionar rápidamente los datos que has cargado en R.

**Resumen**

Este código lee un archivo CSV con datos delimitados por punto y coma y con comas como separadores decimales, y almacena los datos en un dataframe llamado `datos`. Luego, utiliza la función `head()` para mostrar las primeras 6 filas del dataframe, permitiéndote inspeccionar los datos rápidamente.

:::


```{r}
datos <- read.csv2("excel-R/aula1.csv", header = TRUE, sep = ";", dec = ",", fileEncoding = "latin1")

head(datos)
```

## Uso de los filtros de Excel para revisar y editar los datos

::: {.content-visible .callout-tip icon="false"}

## Para resolver

Poner aquí distintos ejemplos de nombres de variables para ver si son válidos o no. Describir medidas y preguntar cómo llamaríamos a esa variable (por ejemplo, temperatura de la leche que acabamos de descargar de una cisterna)
:::

